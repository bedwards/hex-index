<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Learning Weekly: Issue 416 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Deep Learning Weekly: Issue 416</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/deeplearningweekly/index.html" class="publication">
            Deep Learning Weekly
          </a>
          <span class="separator">&middot;</span><time>Aug 6, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">6 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>This week in deep learning, we bring you <a href="https://openai.com/index/introducing-gpt-oss/">OpenAI's gpt-oss</a>, <a href="https://www.comet.com/site/blog/pretraining/?utm_source=substack&amp;utm_medium=email&amp;utm_campaign=dlw&amp;utm_content=pretraining/">Pretraining: Breaking Down the Modern LLM Training Pipeline</a>, and <a href="https://arxiv.org/html/2507.14447">a paper on Routine: A Structural Planning Framework for LLM Agent System in Enterprise</a>.</p><p>You may also enjoy <a href="https://www.anthropic.com/news/claude-opus-4-1">Anthropic's Claude Opus 4.1</a>, <a href="https://www.full-stack-alignment.ai/paper">Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value</a>, <a href="https://icml.cc/virtual/2025/poster/43499">a paper on Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI</a>, and more!</p><p>As always, happy reading and hacking. If you have something you think should be in next week's issue, find us on Twitter: <a href="https://twitter.com/dl_weekly">@dl_weekly</a>.</p><p>Until next week!</p><div><hr></div><h2><strong>Industry</strong></h2><p><strong><a href="https://openai.com/index/introducing-gpt-oss/">Introducing gpt-oss | OpenAI</a></strong></p><p>OpenAI just released gpt-oss-120b and gpt-oss-20bâ€”two state-of-the-art open-weight language models that deliver strong real-world performance at low cost.</p><p><strong><a href="https://www.anthropic.com/news/claude-opus-4-1">Claude Opus 4.1 \ Anthropic</a></strong></p><p>Anthropic released Claude Opus 4.1, an upgrade to Claude Opus 4 on agentic tasks, real-world coding, and reasoning.</p><p><strong><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">Genie 3: A new frontier for world models</a></strong></p><p>The DeepMind team announced Genie 3, a general purpose world model that can generate an unprecedented diversity of interactive environments.</p><p><strong><a href="https://cohere.com/blog/command-a-vision">Introducing Command A Vision: Multimodal AI Built for Business</a></strong></p><p>The Cohere team introduced Command A Vision, a new state-of-the-art generative model that brings enterprises leading performance across multimodal vision tasks while maintaining strong text capabilities.</p><h2><strong>MLOps &amp; LLMOps</strong></h2><p><strong><a href="https://www.comet.com/site/blog/pretraining/?utm_source=substack&amp;utm_medium=email&amp;utm_campaign=dlw&amp;utm_content=pretraining/">Pretraining: Breaking Down the Modern LLM Training Pipeline</a></strong></p><p>The line between pretraining and fine-tuning in LLMs is increasingly blurred, making it harder to define what "training" means today. This article explores how evolving methods, inconsistent terminology, and opaque pipelines complicate understanding model behavior, emphasizing the critical role of pretraining and data curation in scaling LLMs responsibly.</p><p><strong><a href="https://aws.amazon.com/blogs/machine-learning/ai-judging-ai-scaling-unstructured-text-analysis-with-amazon-nova/">AI judging AI: Scaling unstructured text analysis with Amazon Nova</a></strong></p><p>A practical blog post about deploying LLM jury systems on Amazon Bedrock to scale unstructured text analysis.</p><p><strong><a href="https://cloud.google.com/blog/topics/developers-practitioners/remember-this-agent-state-and-memory-with-adk">Remember this: Agent state and memory with ADK</a></strong></p><p>A Google Cloud blog post illustrating how to implement short-term and long-term memory for AI agents using the Agent Development Kit (ADK) and Vertex AI Memory Bank.</p><h2><strong>Learning</strong></h2><p><strong><a href="https://www.full-stack-alignment.ai/paper">Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value</a></strong></p><p>A research paper that proposes full-stack alignment and thick models of value as an alternative to current human-AI value alignment approaches.</p><p><strong><a href="https://weaviate.io/blog/fine-tune-embedding-model">Why, When and How to Fine-Tune a Custom Embedding Model</a></strong></p><p>A comprehensive technical article detailing the why, when, and how of fine-tuning custom text embedding models to improve retrieval performance in RAG systems.</p><p><strong><a href="https://netflixtechblog.com/fm-intent-predicting-user-session-intent-with-hierarchical-multi-task-learning-94c75e18f4b8">FM-Intent: Predicting </a></strong>...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.deeplearningweekly.com/p/deep-learning-weekly-issue-416" class="read-button" target="_blank" rel="noopener">
          Read full article on Deep Learning Weekly &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>