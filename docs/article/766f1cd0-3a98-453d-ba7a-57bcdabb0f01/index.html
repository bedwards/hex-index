<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI scaling myths - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>AI scaling myths</h1>
        <div class="article-meta">
          <span class="author">By Arvind Narayanan</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/aisnakeoil/index.html" class="publication">
            AI Snake Oil
          </a>
          <span class="separator">&middot;</span><time>Jun 27, 2024</time>
          <span class="separator">&middot;</span>
          <span class="read-time">12 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>So far, bigger and bigger language models have proven more and more capable. But does the past predict the future?</p><p>One popular view is that we should expect the trends that have held so far to continue for many more orders of magnitude, and that it will potentially get us to artificial general intelligence, or AGI. </p><p>This view rests on a series of myths and misconceptions. The seeming predictability of scaling is a misunderstanding of what research has shown. Besides, there are signs that LLM developers are already at the limit of high-quality training data. And the industry is seeing strong <em>downward</em> pressure on model size. While we can't predict exactly how far AI will advance through scaling, we think there’s virtually no chance that scaling alone will lead to AGI.&nbsp;</p><h4><strong>Scaling “laws” are often misunderstood</strong></h4><p>Research on <a href="https://arxiv.org/abs/2001.08361">scaling laws</a> shows that as we increase model size, training compute, and dataset size, language models get “better”. The improvement is truly striking in its predictability, and holds across many orders of magnitude. This is the main reason why many people believe that scaling will continue for the foreseeable future, with regular releases of larger, more powerful models from leading AI companies.</p><p>But this is a complete misinterpretation of scaling laws. What exactly is a “better” model? Scaling laws only quantify the decrease in perplexity, that is, improvement in how well models can predict the next word in a sequence. Of course, perplexity is more or less irrelevant to end users — what matters is “<a href="https://arxiv.org/abs/2206.07682">emergent abilities</a>”, that is, models’ tendency to acquire new capabilities as size increases.</p><p>Emergence is not governed by any law-like behavior. It is true that so far, increases in scale have brought new capabilities. But there is no empirical regularity that gives us confidence that this will continue indefinitely.<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a></p><p>Why might emergence not continue indefinitely? This gets at one of the core debates about LLM capabilities — are they capable of extrapolation or do they only learn tasks represented in the training data? The evidence is incomplete and there is a wide range of reasonable ways to interpret it. But we lean toward the skeptical view. On benchmarks designed to test the efficiency of acquiring skills to solve unseen tasks, LLMs tend to perform <a href="https://arcprize.org/arc">poorly</a>.&nbsp;</p><p>If LLMs can't do much beyond what's seen in training, at some point, having more data no longer ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.normaltech.ai/p/ai-scaling-myths" class="read-button" target="_blank" rel="noopener">
          Read full article on AI Snake Oil &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>