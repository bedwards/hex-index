<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>SerDes Matters! - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>SerDes Matters!</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/chipstrat/index.html" class="publication">
            Chipstrat
          </a>
          <span class="separator">&middot;</span><time>Oct 22, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">7 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>We <a href="https://www.chipstrat.com/p/right-sized-ai-infrastructure-marvell">recently discussed</a> that it makes sense to right-size an AI cluster to fit a particular family of workloads. Specifically, the user requirements of workloads can result in similarly shaped infrastructure requirements:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!z6AE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!z6AE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 424w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 848w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 1272w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!z6AE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png" width="1456" height="1338" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1338,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!z6AE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 424w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 848w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 1272w, https://substackcdn.com/image/fetch/$s_!z6AE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5249ec5b-19eb-43d1-a44c-fb54df07e46b_1784x1640.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">The left represents a family of fast, interactive workloads that could run with small/medium models and could run on “good enough” hardware, which could be old Nvidia chips or cost-optimized offerings from others. The right represents workloads like video models or deep research that demand state-of-the-art computational workhorses with a ton of fast memory.</figcaption></figure></div><p>We already see this “right-sizing” of AI infra with disaggregating LLM inference across different Nvidia hardware; prefill on <a href="https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference">Rubin CPX</a> and decode on traditional Rubin.</p><p>This line of thinking can lead to XPUs; if a particular hyperscaler is quite convinced they’ll need to run a particular type of workload at scale indefinitely, why not trade-off some of the flexibility of GPUs for a more finely tuned AI accelerator? </p><p><strong>For example, won’t OpenAI continue to run fast-thinking (GPT-4o style) LLMs indefinitely?</strong> </p><p>For workloads that might continue to evolve significantly, stick with GPU-based clusters. But for stable workloads at the massive scale of hyperscalers, a custom AI cluster with XPUs can make sense. <em>It’s been three years since ChatGPT launched already…</em></p><p>But customizing the datacenter to a particular workloads is more than just picking a particular already-baked chip. Just like Nvidia systems are GPUs + networking + software (+ memory, + CPUs, + storage), XPU-based systems also include networking, memory, and more. Marvell calls these additional components <strong><a href="https://www.chipstrat.com/i/175545231/xpu-attach">XPU attach</a></strong>.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!L6jX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!L6jX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 424w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 848w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 1272w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!L6jX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png" width="1456" height="802" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:802,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!L6jX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 424w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 848w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 1272w, https://substackcdn.com/image/fetch/$s_!L6jX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f3ff8cd-298c-4a81-b089-7c91a54fd00a_2048x1128.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>And with a custom accelerator partner you can <strong>turn all sorts of knobs deep within the XPU and XPU attach </strong>to tune the resulting AI datacenter to meet your workload’s needs.</p><blockquote><p>Marvell’s Sandeep Bharathi: Now, Matt talked about XPU and XPU attach. What is important to see in an XPU attach, there may be certain IPs that are not necessary, for example, CPU. But what it means that <strong>for each of these, the power and performance per watt requirements are different, which means you need to optimize different SerDes or different Die-to-Dies (D2Ds) for each one of these to meet the needs of the workloads</strong>. </p><p>So customization to achieve the highest performance per watt is a Marvell specialty.</p></blockquote><p>So IP building blocks like SerDes or D2D can be optimized to help the system get the needed system-level performance per Watt.</p><p><strong>But….. what the heck is </strong></p></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://www.chipstrat.com/p/serdes-matters" class="read-button" target="_blank" rel="noopener">
          Read full article on Chipstrat &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>