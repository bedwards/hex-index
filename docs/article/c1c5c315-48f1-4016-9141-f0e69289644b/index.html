<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>First Look at Reasoning From Scratch: Chapter 1 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>First Look at Reasoning From Scratch: Chapter 1</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Mar 29, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">2 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>Hi everyone,</p><p>As you know, I've been writing a lot lately about the latest research on reasoning in LLMs. Before my next research-focused blog post, I wanted to offer something special to my paid subscribers as a thank-you for your ongoing support.</p><p>So, I've started writing a new book on how reasoning works in LLMs, and here I'm sharing the first Chapter 1 with you. This ~15-page chapter is an introduction reasoning in the context of LLMs and provides an overview of methods like inference-time scaling and reinforcement learning.</p><p>Thanks for your support! I hope you enjoy the chapter, and stay tuned for my next blog post on reasoning research!</p><p>Happy reading,<br><em>Sebastian</em></p><h1><strong>Chapter 1: Introduction</strong></h1><p>Welcome to the next stage of large language models (LLMs): <em>reasoning</em>. LLMs have transformed how we process and generate text, but their success has been largely driven by statistical pattern recognition. However, new advances in reasoning methodologies now enable LLMs to tackle more complex tasks, such as solving logical puzzles or multi-step arithmetic. Understanding these methodologies is the central focus of this book.</p><p>In this introductory chapter, you will learn:</p><ul><li><p>What "reasoning" means specifically in the context of LLMs.</p></li><li><p>How reasoning differs fundamentally from pattern matching.</p></li><li><p>The conventional pre-training and post-training stages of LLMs.</p></li><li><p>Key approaches to improving reasoning abilities in LLMs.</p></li><li><p>Why building reasoning models from scratch can improve our understanding of their strengths, limitations, and practical trade-offs.</p></li></ul><p>After building foundational concepts in this chapter, the following chapters shift toward practical, hands-on coding examples to directly implement reasoning techniques for LLMs.</p><h2><strong>1.1 What Does "Reasoning" Mean for Large Language Models?</strong></h2>
      <p>
          <a href="https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch">
              Read more
          </a>
      </p>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>