<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Why Increasing Batch Size Doesn’t Always Speed Up Training - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Why Increasing Batch Size Doesn’t Always Speed Up Training</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Oct 6, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">4 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!g8q1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!g8q1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!g8q1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png" width="670" height="446.82005494505495" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/899a242a-8444-4115-8918-e05a1350607d_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:670,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!g8q1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!g8q1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F899a242a-8444-4115-8918-e05a1350607d_1536x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">Image generated with ChatGPT</figcaption></figure></div><p>When training a language model, each training step feeds the model a batch of token sequences and uses the resulting gradients to update the parameters. Bigger batches sound appealing: they average gradients over more examples (less noise) and keep the GPU busier by processing multiple sequences in parallel.</p><p>The catch is memory. Large batches quickly exhaust GPU memory, so we often have to simulate them with gradient accumulation: run several small micro-batches (e.g., 4 examples at a time), sum their gradients, then apply a single update. Four micro-batches of 4 gives you an effective batch of 16, without ever fitting 16 examples on the GPU at once.</p><p>Intuitively, if you have enough memory, you might expect that just cranking the micro-batch up to 16 would be almost 4x faster than “4 x 4 with accumulation.” In reality, it often isn’t. Past a point, larger per-device batches can slow training down due to many reasons. The result is counterintuitive but common: higher “utilization” on paper, worse throughput on the clock. This is even more apparent when working with small devices, such as consumer GPUs.</p><p>This article explains why. We’ll walk through the<strong> five main reasons</strong> why big batches slow down training and show <strong>when it actually makes sense to increase micro-batch size versus relying on gradient accumulation</strong>. </p><h1>Reason #1: Padding Blow-Up with Variable Lengths</h1><p>When a micro-batch of size <em>B</em> has sequences with lengths <em>L<sub>1</sub>, L<sub>2</sub>, ..., L<sub>B</sub></em>, let:</p><p><em>L<sub>max</sub> = max(L<sub>1</sub>, ..., L<sub>B</sub>)</em></p><p>Most training stacks pad every sequence in that micro-batch up to <em>L<sub>max</sub></em>.</p><p>We have the following:</p><ul><li><p>Useful tokens (what you care about): <em>sum(L<sub>i</sub>)</em></p></li><li><p>Actual number of tokens (because of padding): <em>B * L<sub>max</sub></em></p></li><li><p>Padding waste (tokens): <em>B * L<sub>max</sub> - sum(L<sub>i</sub>)</em></p></li><li><p>Waste fraction: <em>1 - sum(L<sub>i</sub>) / (B * L<sub>max</sub>)</em></p></li></ul><p>With batch_size = 1, <em>L<sub>max</sub> = L<sub>1</sub></em>, so waste is basically zero. With batch_size = 8, a single long sample forces seven shorter ones to run at its length in every layer.</p><p>Let’s see with two examples.</p><h4>Example A: 4k outlier at batch size 8</h4><p>8 sequences of 8 different lengths (number of tokens) in one micro-batch:<br>512, 520, 530, 560, 600, 640, 650, 4000</p><ul><li><p>Useful tokens: 8012 (summing the length of all the sequences)</p></li><li><p>Computed </p></li></ul>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/why-increasing-the-batch-size-doesnt" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>