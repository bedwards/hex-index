<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI existential risk probabilities are too unreliable to inform policy - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>AI existential risk probabilities are too unreliable to inform policy</h1>
        <div class="article-meta">
          <span class="author">By Arvind Narayanan</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/aisnakeoil/index.html" class="publication">
            AI Snake Oil
          </a>
          <span class="separator">&middot;</span><time>Jul 26, 2024</time>
          <span class="separator">&middot;</span>
          <span class="read-time">31 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/black-swan-theory/index.html">
          <strong>Black swan theory</strong>
          <span class="read-time">1 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (10 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/pascals-wager/index.html">
          <strong>Pascal&#039;s wager</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (27 min read)</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.</p><p>This is the first in a series of essays laying out an evidence-based approach for policymakers concerned about AI x-risk, an approach that stays grounded in reality while acknowledging that there are “unknown unknowns”.&nbsp;</p><p>In this first essay, we look at one type of evidence: probability estimates. The AI safety community relies heavily on forecasting the probability of human extinction due to AI (in a given timeframe) in order to inform decision making and policy. An estimate of 10% over a few decades, for example, would obviously be high enough for the issue to be a top priority for society.&nbsp;</p><p>Our central claim is that AI x-risk forecasts are far too unreliable to be useful for policy, and in fact highly misleading.</p><h3><strong>Look behind the curtain</strong></h3><p>If the two of us predicted an 80% probability of aliens landing on earth in the next ten years, would you take this possibility seriously? Of course not. You would ask to see our evidence. As obvious as this may seem, it seems to have been forgotten in the AI x-risk debate that probabilities carry no authority by themselves. Probabilities are <em>usually</em> derived from some grounded method, so we have a strong cognitive bias to view quantified risk estimates as more valid than qualitative ones. But it is possible for probabilities to be nothing more than guesses. Keep this in mind throughout this essay (and more broadly in the AI x-risk debate).</p><p>If we predicted odds for the Kentucky Derby, we don’t have to give you a reason — you can take it or leave it. But if a policymaker takes actions based on probabilities put forth by a forecaster, they had better be able to explain those probabilities to the public (and that explanation must in turn come from the forecaster). <a href="https://plato.stanford.edu/entries/justification-public/">Justification</a> is essential to legitimacy of government and the exercise of power. A core principle of liberal democracy is that the state should not limit people's freedom based on controversial beliefs that reasonable people can reject.&nbsp;</p><p>Explanation is especially important ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.normaltech.ai/p/ai-existential-risk-probabilities" class="read-button" target="_blank" rel="noopener">
          Read full article on AI Snake Oil &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>