<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLM Research Papers: The 2025 List (January to June) - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>LLM Research Papers: The 2025 List (January to June)</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Jul 1, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">6 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article heavily discusses reinforcement learning methods for training reasoning models, particularly &#039;reinforcement learning with verifiable rewards.&#039; RLHF is the foundational technique that enabled modern LLM alignment and reasoning capabilities, making it essential context for understanding the DeepSeek-R1 and similar papers mentioned.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/dual-process-theory/index.html">
          <strong>Dual process theory</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">The article references &#039;System 2 Reasoning&#039; in the paper title &#039;Towards System 2 Reasoning in LLMs.&#039; This refers to Kahneman&#039;s dual process theory distinguishing fast intuitive thinking (System 1) from slow deliberate reasoning (System 2), which is the psychological foundation for chain-of-thought and reasoning model research.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/markov-decision-process/index.html">
          <strong>Markov decision process</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">MDPs are the mathematical framework underlying all reinforcement learning approaches discussed throughout the article. Understanding MDPsâ€”states, actions, rewards, and policiesâ€”provides the theoretical foundation for comprehending how reasoning models are trained via RL methods like those in the cited papers.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>As some of you know, I keep a running list of research papers I (want to) read and reference.</p><p>About six months ago, I shared <a href="https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list">my 2024 list</a>, which many readers found useful. So, I was thinking about doing this again. However, this time, I am incorporating that one piece of feedback kept coming up: <em>"Can you organize the papers by topic instead of date?"</em></p><p>The categories I came up with are:</p><ol><li><p>Reasoning Models</p><p>- 1a. Training Reasoning Models</p><p>- 1b. Inference-Time Reasoning Strategies</p><p>- 1c. Evaluating LLMs and/or Understanding Reasoning</p></li><li><p>Other Reinforcement Learning Methods for LLMs</p></li><li><p>Other Inference-Time Scaling Methods</p></li><li><p>Efficient Training &amp; Architectures</p></li><li><p>Diffusion-Based Language Models</p></li><li><p>Multimodal &amp; Vision-Language Models</p></li><li><p>Data &amp; Pre-training Datasets</p></li></ol><p>Also, as LLM research continues to be shared at a rapid pace, I have decided to break the list into bi-yearly updates. This way, the list stays digestible, timely, and hopefully useful for anyone looking for solid summer reading material.</p><p>Please note that this is just a curated list for now. In future articles, I plan to revisit and discuss some of the more interesting or impactful papers in larger topic-specific write-ups. Stay tuned!</p><div><hr></div><h4>Announcement:</h4><p>It's summer! And that means internship season, tech interviews, and lots of learning.<br>To support those brushing up on intermediate to advanced machine learning and AI topics, <strong>I have made all 30 chapters of my Machine Learning Q and AI book freely available for the summer:</strong><br><br>ðŸ”— <a href="https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents">https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents</a><br><br>Whether you are just curious and want to learn something new or prepping for interviews, hopefully this comes in handy. <br><br>Happy reading, and best of luck if you are interviewing!</p><div><hr></div><h2><strong>1. Reasoning Models</strong></h2><p>This year, my list is very reasoning model-heavy. So, I decided to subdivide it into 3 categories: Training, inference-time scaling, and more general understanding/evaluation.</p><h3><strong>1a. Training Reasoning Models</strong></h3><p>This subsection focuses on training strategies specifically designed to improve reasoning abilities in LLMs. As you may see, much of the recent progress has centered around reinforcement learning (with verifiable rewards), which I covered in more detail in a previous article.</p><div class="digest-post-embed" data-attrs="{&quot;nodeId&quot;:&quot;be74af4a-f842-4ff3-adef-ac0c7491b455&quot;,&quot;caption&quot;:&quot;A lot has happened this month, especially with the releases of new flagship models like GPT-4.5 and Llama 4. But you might have noticed that reactions to these releases were relatively muted. Why? One reason could be that GPT-4.5 and Llama 4 remain conventional models, which means they were trained without explicit reinforcement learning for reasoning.&quot;,&quot;cta&quot;:&quot;Read full story&quot;,&quot;showBylines&quot;:true,&quot;size&quot;:&quot;sm&quot;,&quot;isEditorNode&quot;:true,&quot;title&quot;:&quot;The State of Reinforcement Learning for LLM Reasoning&quot;,&quot;publishedBylines&quot;:[{&quot;id&quot;:27393275,&quot;name&quot;:&quot;Sebastian Raschka, PhD&quot;,&quot;bio&quot;:&quot;I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI &amp; LLM research focusing on code-driven implementations. I am also the author of \&quot;Build a Large Language Model From Scratch\&quot; (amzn.to/4fqvn0D).&quot;,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg&quot;,&quot;is_guest&quot;:false,&quot;bestseller_tier&quot;:100}],&quot;post_date&quot;:&quot;2025-04-19T11:02:44.098Z&quot;,&quot;cover_image&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png&quot;,&quot;cover_image_alt&quot;:null,&quot;canonical_url&quot;:&quot;https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training&quot;,&quot;section_name&quot;:null,&quot;video_upload_id&quot;:null,&quot;id&quot;:161572341,&quot;type&quot;:&quot;newsletter&quot;,&quot;reaction_count&quot;:386,&quot;comment_count&quot;:30,&quot;publication_id&quot;:null,&quot;publication_name&quot;:&quot;Ahead of AI&quot;,&quot;publication_logo_url&quot;:&quot;https://substackcdn.com/image/fetch/$s_!AEiF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png&quot;,&quot;belowTheFold&quot;:true}"></div><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!A5c7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!A5c7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 424w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 848w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1272w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png" width="941" height="477" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:941,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97188,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166943621?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!A5c7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 424w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 848w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1272w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">Annotated figure from Reinforcement Pre-Training, <a href="https://arxiv.org/abs/2506.08007">https://arxiv.org/abs/2506.08007</a></figcaption></figure></div><ul><li><p>8 Jan, Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought, <a href="https://arxiv.org/abs/2501.04682">https://arxiv.org/abs/2501.04682</a></p></li><li><p>13 Jan, The Lessons of Developing Process Reward Models in Mathematical Reasoning, <a href="https://arxiv.org/abs/2501.07301">https://arxiv.org/abs/2501.07301</a></p></li><li><p>16 Jan, Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models, <a href="https://arxiv.org/abs/2501.09686">https://arxiv.org/abs/2501.09686</a></p></li><li><p>20 Jan, Reasoning Language Models: A Blueprint, </p></li></ul>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>