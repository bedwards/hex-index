<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Understand, align, cooperate: AI welfare and AI safety are allies - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Understand, align, cooperate: AI welfare and AI safety are allies</h1>
        <div class="article-meta">
          <span class="author">By Robert Long</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/experiencemachines/index.html" class="publication">
            
          </a>
          <span class="separator">&middot;</span><time>Apr 1, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">13 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>In conversations about AI safety and AI welfare, people sometimes frame them as inherently opposing goals: either we protect humans (AI safety), or we protect AI systems (AI welfare). This framing implies a tragic, necessary choice: <em>whose side are you on? </em>Or it suggests, at least, “<em>shouldn’t we solve safety first, and then worry about AI welfare?”</em> But the truth is that this is a false choice: fortunately, many of the best interventions for protecting AI systems will also protect humans, and vice versa.</p><p>To be sure, there are genuine tensions between AI safety and AI welfare.<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a> But if we only focus on these tensions, we risk creating a self-fulfilling prophecy: by obsessing over potential tradeoffs, we may overlook opportunities for mutually beneficial solutions. As a result, fewer such opportunities will exist.</p><p>Fortunately, it doesn’t have to be this way. There is clear common cause between the fields. Specifically, we can better protect both humans and potential AI moral patients if we work to:</p><ul><li><p><strong>Understand</strong> AI systems better.</p></li><li><p><strong>Align</strong> their goals more effectively with our own.</p></li><li><p><strong>Cooperate</strong> with them when possible.</p></li></ul><p>We have a <em>lot</em> more work to do on all of these, and getting our act together will be good for everyone.</p><ol><li><p><strong>Understand: </strong>If we can’t understand what models think or how they work, we can't know whether they're plotting against us (a safety risk), and we can't know whether they're suffering (a welfare risk).</p></li><li><p><strong>Align: </strong>If we can’t align AI systems so that they want to do what we ask them to do, they pose a danger to us (a safety risk) and potentially (if they are moral patients) are having their desires frustrated (a welfare risk).</p></li><li><p><strong>Cooperate: </strong>If we have no way to cooperate with AI systems that are not fully aligned, such AI systems may rightly believe that they have no choice but to hide, escape, or fight. That’s dangerous for everyone involved—it’s more likely that AIs fight us (a safety risk), and more likely that we have to resort to harsher methods of controlling them (a welfare risk).</p></li></ol><p>Note: I am claiming that these are welfare <em>risks</em>. I’m not claiming that by, e.g., controlling AI systems, we are definitely harming them. At no point in this article am I claiming that current AI systems are definitely conscious or being harmed. But as AI continues to advance, the risk that we are actually ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://experiencemachines.substack.com/p/understand-align-cooperate-ai-welfare" class="read-button" target="_blank" rel="noopener">
          Read full article on  &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>