<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How LLMs Learn from the Internet: The Training Process - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>How LLMs Learn from the Internet: The Training Process</h1>
        <div class="article-meta">
          <span class="author">By Alex Xu</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/bytebytego/index.html" class="publication">
            ByteByteGo Newsletter
          </a>
          <span class="separator">&middot;</span><time>Dec 1, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">16 min read</span>
        </div>
      </header>

      <div class="article-excerpt">
        <p></p>
        <div class="excerpt-fade"></div>
      </div>

      <div class="read-full-article">
        <a href="https://blog.bytebytego.com/p/how-llms-learn-from-the-internet" class="read-button" target="_blank" rel="noopener">
          Read full article on ByteByteGo Newsletter &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/byte-pair-encoding/index.html">
          <strong>Byte-pair encoding</strong>
          <span class="read-time">7 min read</span>
        </a>
        <p class="topic-summary">The article specifically mentions Byte Pair Encoding as the tokenization method LLMs use, but doesn&#039;t explain how this compression algorithm works. Understanding BPE&#039;s origins in data compression and its adaptation for NLP would give readers deeper insight into why LLMs break words into subword units.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/backpropagation/index.html">
          <strong>Backpropagation</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article discusses how parameters are &#039;tuned during training&#039; and mentions gradient descent conceptually, but doesn&#039;t explain the fundamental algorithm that makes neural network learning possible. Backpropagation is the mathematical process that enables LLMs to adjust their billions of parameters.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/perceptron/index.html">
          <strong>Perceptron</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">The article describes LLMs as &#039;extraordinarily sophisticated pattern recognition systems&#039; with billions of parameters acting as weights. The perceptron, invented by Frank Rosenblatt in 1958, was the first trainable neural network and establishes the historical foundation for understanding how weighted connections learn patternsâ€”the basic building block that modern LLMs scale up massively.</p>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>