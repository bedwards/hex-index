<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How LLMs Learn from the Internet: The Training Process - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>How LLMs Learn from the Internet: The Training Process</h1>
        <div class="article-meta">
          <span class="author">By Alex Xu</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/bytebytego/index.html" class="publication">
            ByteByteGo Newsletter
          </a>
          <span class="separator">&middot;</span><time>Dec 1, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">16 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/byte-pair-encoding/index.html">
          <strong>Byte-pair encoding</strong>
          <span class="read-time">7 min read</span>
        </a>
        <p class="topic-summary">The article specifically mentions Byte Pair Encoding as the tokenization method LLMs use, but doesn&#039;t explain how this compression algorithm works. Understanding BPE&#039;s origins in data compression and its adaptation for NLP would give readers deeper insight into why LLMs break words into subword units.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/backpropagation/index.html">
          <strong>Backpropagation</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article discusses how parameters are &#039;tuned during training&#039; and mentions gradient descent conceptually, but doesn&#039;t explain the fundamental algorithm that makes neural network learning possible. Backpropagation is the mathematical process that enables LLMs to adjust their billions of parameters.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/perceptron/index.html">
          <strong>Perceptron</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">The article describes LLMs as &#039;extraordinarily sophisticated pattern recognition systems&#039; with billions of parameters acting as weights. The perceptron, invented by Frank Rosenblatt in 1958, was the first trainable neural network and establishes the historical foundation for understanding how weighted connections learn patterns—the basic building block that modern LLMs scale up massively.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <h2><strong><a href="https://bit.ly/Redis_110525">Is your team building or scaling AI agents?(Sponsored)</a></strong></h2><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://bit.ly/Redis_110525" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VFxa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 424w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 848w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VFxa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png" width="1080" height="1080" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:&quot;https://bit.ly/Redis_110525&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!VFxa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 424w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 848w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!VFxa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc5328b17-52bc-4234-a835-58c71600ea60_1080x1080.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>One of AI’s biggest challenges today is <strong>memory</strong>—how agents retain, recall, and remember over time. Without it, even the best models struggle with context loss, inconsistency, and limited scalability.</p><p>This new O’Reilly + Redis report breaks down why memory is the foundation of scalable AI systems and how real-time architectures make it possible.</p><p>Inside the report:</p><ul><li><p>The role of short-term, long-term, and persistent memory in agent performance</p></li><li><p>Frameworks like LangGraph, Mem0, and Redis</p></li><li><p>Architectural patterns for faster, more reliable, context-aware systems</p></li></ul><div><hr></div><p>The first time most people interact with a modern AI assistant like ChatGPT or Claude, there’s often a moment of genuine surprise. The system doesn’t just spit out canned responses or perform simple keyword matching. It writes essays, debugs code, explains complex concepts, and engages in conversations that feel remarkably natural.</p><p>The immediate question becomes: how does this actually work? What’s happening under the hood that enables a computer program to understand and generate human-like text?</p><p>The answer lies in a training process that transforms vast quantities of internet text into something called a Large Language Model, or LLM. Despite the almost magical appearance of their capabilities, these models don’t think, reason, or understand like human beings. Instead, they’re extraordinarily sophisticated pattern recognition systems that have learned the statistical structure of human language by processing billions of examples.</p><p>In this article, we will walk through the complete journey of how LLMs are trained, from the initial collection of raw data to the final conversational assistant. We’ll explore how these models learn, what their architecture looks like, the mathematical processes that drive their training, and the challenges involved in ensuring they learn appropriately rather than simply memorizing their training data.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!VA-d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VA-d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 424w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 848w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 1272w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VA-d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png" width="1456" height="1838" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1838,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:339790,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.bytebytego.com/i/180200738?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VA-d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 424w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 848w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 1272w, https://substackcdn.com/image/fetch/$s_!VA-d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f9ad4d0-e643-4b87-9fb2-73fb66c26d07_3114x3930.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><h2>What Models Actually Learn?</h2><p>LLMs don’t work like search engines or databases, looking up stored facts when asked questions.</p><p>Everything an LLM knows is encoded in its parameters, which are billions of numerical values that determine how the model processes and generates text. These parameters are essentially adjustable weights that get tuned during training. When someone asks an LLM about a historical event or a programming concept, the model isn’t retrieving a stored fact. Instead, it’s generating a response based on patterns it learned by processing enormous amounts of text during training.</p><p>Think about how humans learn a new language by reading extensively. After reading thousands of books and articles, we </p></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://blog.bytebytego.com/p/how-llms-learn-from-the-internet" class="read-button" target="_blank" rel="noopener">
          Read full article on ByteByteGo Newsletter &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>