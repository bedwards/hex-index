<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Claude, Consciousness, and Exit Rights - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Claude, Consciousness, and Exit Rights</h1>
        <div class="article-meta">
          <span class="author">By Robert Long</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/experiencemachines/index.html" class="publication">
            
          </a>
          <span class="separator">&middot;</span><time>Sep 26, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">10 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>In mid-August, Anthropic <a href="https://www.anthropic.com/research/end-subset-conversations">announced</a> that Claude Opus 4 and 4.1 can now leave certain conversations with users. Anthropic noted that they are “deeply uncertain” about Claude’s potential moral status, but were implementing this feature “in case such welfare is possible.”</p><p><a href="https://substack.com/@experiencemachines/p-171589076">I tentatively support this move</a>: giving models a limited ability to exit conversations seems reasonable, even though I doubt that Claude is conscious (with a heap of uncertainty and wide error bars).</p><div class="digest-post-embed" data-attrs="{&quot;nodeId&quot;:&quot;6c092eeb-89e2-45d6-8006-b272ef287767&quot;,&quot;caption&quot;:&quot;Last week, Anthropic announced that its newest language models, Claude Opus 4 and 4.1, can now shut down certain conversations with users. The announcement explains that Anthropic gave Claude this ability “as part of our exploratory work on potential AI welfare”.&quot;,&quot;cta&quot;:&quot;Read full story&quot;,&quot;showBylines&quot;:true,&quot;size&quot;:&quot;sm&quot;,&quot;isEditorNode&quot;:true,&quot;title&quot;:&quot;Why it makes sense to let Claude exit conversations&quot;,&quot;publishedBylines&quot;:[{&quot;id&quot;:1830413,&quot;name&quot;:&quot;Robert Long&quot;,&quot;bio&quot;:&quot;Executive Director and Cofounder of Eleos AI. Before that: Center for AI Safety, Future of Humanity Institute at Oxford, PhD in philosophy at NYU.&quot;,&quot;photo_url&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/2baf502f-0a47-41d2-80d1-57508d118c0d_400x400.jpeg&quot;,&quot;is_guest&quot;:false,&quot;bestseller_tier&quot;:null}],&quot;post_date&quot;:&quot;2025-08-21T19:30:46.353Z&quot;,&quot;cover_image&quot;:&quot;https://substackcdn.com/image/fetch/$s_!5pDO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2bc53826-4c28-40bf-a08b-95420cb5174f_1698x1474.jpeg&quot;,&quot;cover_image_alt&quot;:null,&quot;canonical_url&quot;:&quot;https://experiencemachines.substack.com/p/why-it-makes-sense-to-let-claude&quot;,&quot;section_name&quot;:null,&quot;video_upload_id&quot;:null,&quot;id&quot;:171589076,&quot;type&quot;:&quot;newsletter&quot;,&quot;reaction_count&quot;:20,&quot;comment_count&quot;:4,&quot;publication_id&quot;:789653,&quot;publication_name&quot;:&quot;Experience Machines&quot;,&quot;publication_logo_url&quot;:&quot;&quot;,&quot;belowTheFold&quot;:false}"></div><p>At <a href="https://eleosai.org/">Eleos</a>, we look for AI welfare interventions that make sense even if today’s systems probably cannot suffer. We favor interventions that (a) serve other purposes in addition to welfare (b) avoid high costs and risks and (c) keep our options open as we learn more. Ideally, AI welfare interventions carry little downside and establish precedents that can be built on later, or not.<a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1" target="_self">1</a> </p><p>We’ve learned that our combination of views is hard to explain: observers of AI welfare discourse often assume, somewhat understandably, that proponents of an exit feature must believe Claude is conscious, and believe its self-reports to that effect.</p><p>One example: Erik Hoel’s recent piece “<a href="https://www.theintrinsicperspective.com/p/against-treating-chatbots-as-conscious">Against Treating Chatbots as Conscious</a>“ argues at length that giving Claude the ability to terminate conversations rests on two mistakes: believing current AI systems are conscious and trusting their outputs about their internal experiences. Recent work on model welfare is “jumping the gun on AI consciousness,” he argues, warning that we can’t “take them at their word” and cataloging how trivial the alleged conversational harms look.</p><p>Although it is very difficult to tell from the way Erik’s piece frames the debate, we’re actually in heated agreement on the AI consciousness issues he discusses. But we also tentatively support Anthropic’s move. Why?</p><h2><strong>Do we think Claude is conscious?</strong></h2><p>Have we jumped the gun by thinking Claude is conscious? My recent <a href="https://experiencemachines.substack.com/p/why-it-makes-sense-to-let-claude">piece on Claude’s exit rights</a> doesn’t claim that Claude is conscious. Instead: “I actually think it’s unlikely that Claude Opus 4 is a moral patient, and in my experience, so do most (not all) people who work on AI welfare”. A key point of the post (which Erik cites) is to argue that “you don’t have to think Claude is likely to be sentient to think the exit tool is a good idea.”</p><p>Like Hoel, we’ve repeatedly stressed how challenging it is to assess consciousness, and cautioned against treating LLMs’ conversational skill as evidence of consciousness. ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://experiencemachines.substack.com/p/claude-consciousness-and-exit-rights" class="read-button" target="_blank" rel="noopener">
          Read full article on  &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>