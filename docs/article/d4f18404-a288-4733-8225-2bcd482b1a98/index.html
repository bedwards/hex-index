<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Giving your AI a Job Interview - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Giving your AI a Job Interview</h1>
        <div class="article-meta">
          <span class="author">By Ethan Mollick</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/oneusefulthing/index.html" class="publication">
            One Useful Thing
          </a>
          <span class="separator">&middot;</span><time>Nov 11, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">10 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>Given how much energy, literal and figurative, goes into developing new AIs, we have a surprisingly hard time measuring how “smart” they are, exactly. The most common approach is to treat AI like a human, by giving it tests and reporting how many answers it gets right. There are dozens of such tests, called benchmarks, and they are the primary way of measuring how good AIs get over time. </p><p>There are some problems with this approach.</p><p>First, many benchmarks and their answer keys are public, so some AIs end up incorporating them into their basic training, whether by accident or <a href="https://arxiv.org/abs/2309.08632">so they can score highly on these benchmarks</a>. But even when that doesn’t happen, it turns out that we often don’t know what these tests really measure. For example, the very popular MMLU-Pro benchmark includes questions like “What is the approximate mean cranial capacity of Homo erectus?” and “What place is named in the title of the 1979 live album by rock legends Cheap Trick?” with ten possible answers for each. What does getting this right tell us? I have no idea. And that is leaving aside the fact that tests are often uncalibrated, meaning we don’t know if moving from 84% correct to 85% is as challenging as moving from 40% to 41% correct. And, on top of all that, for many tests, <a href="https://derenrich.medium.com/errors-in-the-mmlu-the-deep-learning-benchmark-is-wrong-surprisingly-often-7258bb045859">the actual top score may be unachievable</a> because there are many errors in the test questions and <a href="https://gail.wharton.upenn.edu/research-and-insights/tech-report-prompt-engineering-is-complicated-and-contingent/">measures are often reported in unusual ways.</a></p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!CUjN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!CUjN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!CUjN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:169080,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.oneusefulthing.org/i/178292321?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!CUjN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 424w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 848w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 1272w, https://substackcdn.com/image/fetch/$s_!CUjN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec731cbe-84e5-4792-915f-96f81b5e4498_1920x1080.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">Every benchmark has flaws, but they are all trending the same way - up and to the right. The AIME is a hard math exam, GPQA tests scientific and legal knowledge, the MMLU is a general knowledge test, SWE-bench and LiveBench test coding, Terminal-Bench tests agentic ability. Data from <a href="https://epoch.ai/benchmarks/search">Epoch AI</a>.</figcaption></figure></div><p>Despite these issues, all of these benchmarks, taken together, <a href="https://x.com/emollick/status/1969986020042010640?s=20">appear to measure some underlying ability facto</a>r. And higher-quality benchmarks like <a href="https://arcprize.org/arc-agi">ARC-AGI</a> and <a href="https://metr.org/">METR Long Tasks</a> show the same upward, even exponential, trend. This matches tests of the real-world impact of AI across industries that suggest that this underlying increase in “smarts” translates to actual ability in everything from medicine to finance.</p><p>So, collectively, benchmarking has real value, but the few robust individual benchmarks focus on math, science, reasoning, and coding. If you want to measure writing ability or sociological analysis or business advice or empathy, you have very </p>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://www.oneusefulthing.org/p/giving-your-ai-a-job-interview" class="read-button" target="_blank" rel="noopener">
          Read full article on One Useful Thing &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>