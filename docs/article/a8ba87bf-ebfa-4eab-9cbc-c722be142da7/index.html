<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Accelerate Models with Quantization: Recipes for NVFP4, GPTQ, AWQ, SmoothQuant, AutoRound, and FP8 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Accelerate Models with Quantization: Recipes for NVFP4, GPTQ, AWQ, SmoothQuant, AutoRound, and FP8</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Nov 24, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">2 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/quantization-signal-processing/index.html">
          <strong>Quantization (signal processing)</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article discusses various quantization methods (INT4, INT8, FP8, FP4) for neural networks. Understanding the mathematical foundations of quantization—how continuous values are mapped to discrete representations and the inherent trade-offs in precision—provides essential context for why these techniques work and their accuracy implications.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/floating-point-arithmetic/index.html">
          <strong>Floating-point arithmetic</strong>
          <span class="read-time">7 min read</span>
        </a>
        <p class="topic-summary">The article references FP8, FP4, and different numeric formats extensively. Understanding IEEE floating-point representation, how precision relates to bit width, and the historical development of floating-point standards illuminates why reduced-precision formats like FP8 can maintain reasonable accuracy while dramatically improving performance.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/cuda/index.html">
          <strong>CUDA</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article mentions &#039;highly optimized CUDA kernels&#039; as key to achieving fast inference with quantized models. Understanding NVIDIA&#039;s parallel computing architecture, its history, and how it enables GPU-accelerated machine learning provides valuable context for why hardware-specific optimizations matter for running these quantized models efficiently.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!L8L5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!L8L5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!L8L5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png" width="1456" height="794" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:794,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5505614,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://kaitchup.substack.com/i/178089395?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!L8L5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 424w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 848w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!L8L5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd0199a7c-2725-440a-bf59-b495bf5723c6_2816x1536.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">Generated with Nano Banana Pro</figcaption></figure></div><p>Running LLMs is easy. Quantizing LLMs is also easy. But running quantized LLMs? That often doesn’t work as expected. This is one of the reasons GGUF is so popular: it’s a format that can be easily run by popular frameworks like Ollama and llama.cpp.</p><p>However, if you want state-of-the-art quantization accuracy and to take advantage of highly optimized CUDA kernels for INT4, FP8, and FP4 models, you often need to get your hands a bit dirty.</p><p>In this article, I explore <strong>six different quantization recipes</strong> that yield models optimized to run very fast with <strong>vLLM</strong>. We’ve already applied most of them in previous articles using different frameworks:</p><ul><li><p><strong>W4A16</strong>: INT4 quantized weights with GPTQ, AWQ, and AutoRound, calibrated/tuned</p></li><li><p><strong>W8A8</strong>: INT8 quantized weights and quantized activations, calibrated with SmoothQuant</p></li><li><p><strong>FP8-Dynamic</strong>: FP8 quantized weights and <em>dynamically</em> quantized activations</p></li><li><p><strong>NVFP4</strong>: FP4 quantized weights and activations, calibrated</p></li></ul><p>All these recipes can be run on a single consumer GPU, but you’ll need a recent one (for FP8 and NVFP4 in particular), such as an RTX 50xx. I used an <strong><a href="https://runpod.io?ref=1ip9lvtj">RTX 5090 (from RunPod)</a></strong> and was able to quantize 8B models. None of these recipes took more than an hour.</p><p>I also provide a <strong>single customizable script</strong> capable of running each of these recipes. You can find it here:</p><p>In the following sections, we’ll test each recipe with Qwen3 4B Instruct and also its Thinking variants to measure the impact on reasoning and long-sequence generation. I report both inference throughput and accuracy on popular benchmarks.</p><p><em>Note: I focused on Qwen3 in this article, but I could quantize Olmo 3 with the same script. You can find my quantized Olmo 3 here (still ongoing):</em></p><ul><li><p><a href="https://huggingface.co/collections/kaitchup/quantized-olmo-3">Quantized Olmo 3</a></p></li></ul><h2>6 Quantization Recipes</h2>
      <p>
          <a href="https://kaitchup.substack.com/p/quantizing-and-running-fast-models">
              Read more
          </a>
      </p></source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/quantizing-and-running-fast-models" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>