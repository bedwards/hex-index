<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Noteworthy AI Research Papers of 2024 (Part Two) - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Noteworthy AI Research Papers of 2024 (Part Two)</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Jan 15, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">28 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>I hope your 2025 is off to a great start! To kick off the year, I've finally been able to complete the draft and second part of this AI Research Highlights of 2024 article. It covers a variety of relevant topics, from mixture-of-experts models to new LLM scaling laws for precision.</p><p>Note that this article is Part Two in this series, focusing on the second half of 2024 from July through December. You can find <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1">Part One, covering January to June here.</a> </p><p>The selection criteria are admittedly subjective, based on what stood out to me this year. I've also aimed for some variety, so it's not all just about LLM model releases.</p><p>I hope you are having a great 2025, and happy reading!</p><h1>7. July: The Llama 3 Herd of Models</h1><p>Readers are probably already well familiar with Meta AI's Llama 3 models and paper, but since these are such important and widely-used models, I want to dedicate the July section to <a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a> (July 2024) paper by Grattafiori and colleagues.</p><p>What's notable about the Llama 3 model family is the increased sophistication of the pre-training and post-training pipelines compared to its Llama 2 predecessor. Note that this is not only true for Llama 3 but other LLMs like <a href="https://arxiv.org/abs/2408.00118">Gemma 2</a>, <a href="https://arxiv.org/abs/2407.10671">Qwen 2</a>, <a href="https://arxiv.org/abs/2407.21075">Apple's Foundation Models</a>, and others, as I described a few months ago in my <a href="https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training">New LLM Pre-training and Post-training Paradigms</a> article.</p><h2><strong>7.1 Llama 3 architecture summary</strong></h2><p>Llama 3 was first released in 8 billion and 70 billion parameter sizes, but the team kept iterating on the model, releasing 3.1, 3.2, and 3.3 versions of Llama. The sizes are summarized below:&nbsp;</p><p><strong>Llama 3</strong> (April 2024)</p><ul><li><p>8B parameters</p></li><li><p>70B parameters&nbsp;</p></li></ul><p><strong>Llama 3.1</strong> (July 2024, discussed in the paper)</p><ul><li><p>8B parameters</p></li><li><p>70B parameters</p></li><li><p>405B parameters&nbsp;</p></li></ul><p><strong>Llama 3.2 </strong>(September 2024)</p><ul><li><p>1B parameters</p></li><li><p>3B parameters</p></li><li><p>11B parameters (vision-enabled)</p></li><li><p>90B parameters (vision-enabled)&nbsp;</p></li></ul><p>L<strong>lama 3.3 </strong>(December 2024)</p><ul><li><p>70B parameters</p></li></ul><p>Overall, the Llama 3 architecture closely resembles that of Llama 2. The key differences lie in its larger vocabulary and the introduction of grouped-query attention for the smaller model variant. A summary of the differences is shown in the figure below.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Od-H!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Od-H!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 424w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 848w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1272w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png" width="1456" height="808" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b7794088-da83-4121-90db-f84daf266970_1600x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:808,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Od-H!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 424w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 848w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1272w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em>Llama 2 vs 3 comparison from the <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama">bonus material of my Build a Large Language from Scratch book</a></em></figcaption></figure></div><p>If you're curious about architectural details, a great way to learn is by implementing the model from scratch </p>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>