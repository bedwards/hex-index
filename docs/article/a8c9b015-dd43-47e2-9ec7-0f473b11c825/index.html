<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Nvidia, AMD, Amkor, Arista @ UBS Tech Conference - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Nvidia, AMD, Amkor, Arista @ UBS Tech Conference</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/chipstrat/index.html" class="publication">
            Chipstrat
          </a>
          <span class="separator">&middot;</span><time>Dec 8, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">11 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/high-bandwidth-memory/index.html">
          <strong>High Bandwidth Memory</strong>
          <span class="read-time">9 min read</span>
        </a>
        <p class="topic-summary">The article discusses HBM capacity and bandwidth limitations in older GPUs as a potential driver of replacement cycles. Understanding HBM&#039;s technical architecture, stacking technology, and evolution across generations would give readers deeper insight into why memory constraints matter for AI workloads.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/tensor-processing-unit/index.html">
          <strong>Tensor Processing Unit</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article references Google TPUs and questions what happens to older versions, while also discussing the GPU vs XPU competition. Understanding TPU architecture, its differences from GPUs, and Google&#039;s design philosophy provides essential context for the hyperscaler competition discussion.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/fabless-manufacturing/index.html">
          <strong>Fabless manufacturing</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article contrasts Nvidia as a &#039;merchant silicon vendor&#039; against hyperscalers designing custom chips. Understanding the fabless model explains why companies like Nvidia, AMD, and now cloud providers make different architectural tradeoffs and how the semiconductor industry structure shapes AI chip competition.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>Thoughts from various conversations at the UBS conference last week:</p><h2>Nvidia</h2><h3>No Replacement Cycles Yet</h3><p>Colette confirmed that there hasn’t been a datacenter GPU replacement cycle yet:</p><blockquote><p><strong>Timothy Arcuri</strong>: <strong>And I get the question a lot about how much of what you’re shipping is replacing existing GPUs versus just additive to the existing base</strong>. And it seems like almost all of what you’re shipping is just additive to the base. We haven’t even begun to replace the existing installed base. Is that correct?</p><p><strong>Colette Kress</strong>: It’s true. <strong>It’s true that most of the installed base still stays there.</strong> And what we are seeing is the advanced new models want to go to the latest generation because a lot of our codesign was working with the researchers of all of these companies to help understand what they’re going to need for their next models. So that’s the important part that they do. They move that model to the newest architecture and stay with the existing. <strong>So yes, to this date, most of what you’re seeing is all brand new builds throughout the U.S. and across the world.</strong></p></blockquote><p>On the one hand this is fairly obvious: GPUs, even older ones, are super useful whether you’re pre-training, post-training, fine-tuning, serving inference, labeling data, simulating autonomy, synthetic data generation, ablation studies, regression testing, etc etc. R&amp;D teams everywhere can absorb essentially unlimited amounts of old GPU compute. Every lab has more experiments it <em>wants</em> to run than budget for new GPUs.</p><p>So why throw out old GPUs that can still crank out tokens, even if the throughput is lower? <em>Especially if they are nearly or fully depreciated!</em></p><p><strong>But it does raise the question: what </strong><em><strong>would </strong></em><strong>cause GPU replacement cycles?</strong></p><h3>Power Budget Reallocation</h3><p>Recall that power is a constraint. Remember how Andy Jassy answered a capacity question on the Amazon earnings call in terms of <em>power </em>and not chips?</p><blockquote><p><strong>Justin Post:</strong> I’ll ask on AWS. <strong>Can you just kind of go through how you’re feeling about your capacity levels and how capacity constrained you are right now?</strong></p><p><strong>Andrew Jassy</strong>: On the capacity side, we brought in quite a bit of capacity, as I mentioned in my opening comments, 3.8 gigawatts of capacity in the last year with another gigawatt plus coming in the fourth quarter and we expect to double our overall capacity by the end of 2027. <strong>So we’re bringing </strong></p>...</blockquote>
      </div>

      <div class="read-full-article">
        <a href="https://www.chipstrat.com/p/nvidia-amd-amkor-arista-ubs-tech" class="read-button" target="_blank" rel="noopener">
          Read full article on Chipstrat &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>