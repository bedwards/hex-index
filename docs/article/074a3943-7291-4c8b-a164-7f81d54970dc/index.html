<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Coding LLMs from the Ground Up: A Complete Course - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Coding LLMs from the Ground Up: A Complete Course</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>May 10, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">5 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>I wrote a lot about reasoning models in recent months (4 articles in a row)! Next to everything "agentic," reasoning is one of the biggest LLM topics of 2025.</p><p>This month, however, I wanted to share more fundamental or "foundational" content with you on how to code LLMs, which is one of the best ways to understand how LLMs work.</p><p>Why? Many people really liked and benefited from the abbreviated LLM workshop I shared last year:</p><div class="digest-post-embed" data-attrs="{&quot;nodeId&quot;:&quot;b505727d-4b3a-4f1b-aef6-5698725bd617&quot;,&quot;caption&quot;:&quot;If you’d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I've prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.&quot;,&quot;cta&quot;:&quot;Read full story&quot;,&quot;showBylines&quot;:true,&quot;size&quot;:&quot;sm&quot;,&quot;isEditorNode&quot;:true,&quot;title&quot;:&quot;Building LLMs from the Ground Up: A 3-hour Coding Workshop&quot;,&quot;publishedBylines&quot;:[{&quot;id&quot;:27393275,&quot;name&quot;:&quot;Sebastian Raschka, PhD&quot;,&quot;bio&quot;:&quot;I'm an LLM research engineer 10+ years of experience in artificial intelligence. My expertise lies in AI &amp; LLM research focusing on code-driven implementations. I am also the author of \&quot;Build a Large Language Model From Scratch\&quot; (amzn.to/4fqvn0D).&quot;,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F61f4c017-506f-4e9b-a24f-76340dad0309_800x800.jpeg&quot;,&quot;is_guest&quot;:false,&quot;bestseller_tier&quot;:100}],&quot;post_date&quot;:&quot;2024-08-31T10:39:35.940Z&quot;,&quot;cover_image&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/367b547c-9d22-4a3d-b466-1d56ccc6b055_1844x1224.png&quot;,&quot;cover_image_alt&quot;:null,&quot;canonical_url&quot;:&quot;https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up&quot;,&quot;section_name&quot;:null,&quot;video_upload_id&quot;:null,&quot;id&quot;:148329414,&quot;type&quot;:&quot;newsletter&quot;,&quot;reaction_count&quot;:401,&quot;comment_count&quot;:15,&quot;publication_id&quot;:null,&quot;publication_name&quot;:&quot;Ahead of AI&quot;,&quot;publication_logo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4dcbe6f-2617-404f-8368-9bc428272016_1280x1280.png&quot;,&quot;belowTheFold&quot;:false}"></div><p>So, I thought this ~5× longer and more detailed content (~15 hours in total) would be even more useful.</p><p>Also, I'm sadly dealing with a bad neck injury and haven't really been able to work on a computer for the past 3 weeks. I am currently trying a conservative treatment before considering the suggested surgical route. This is the worst timing as I just started to get back on track before life threw another curveball.</p><p>So, during my recovery, I thought sharing these videos I recorded in the last couple of months would be a nice in-between content.</p><p>I hope you find this useful, and thanks for your support!</p><p><em>PS: The videos originally started as supplementary content for my <a href="https://amzn.to/4fqvn0D">Build a Large Language Model (From Scratch) book</a>. But it turns out they also work pretty well as standalone content.</em></p><p><strong>Why build from scratch?</strong> </p><p>It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.</p><p>To offer an analogy: if you are into cars and want to understand how they work, following a tutorial that walks you through building one from the ground up is a great way to learn. Of course, we probably wouldn't want to start by building a Formula 1 race car since it would be prohibitively expensive and overly complex for a first project. Instead, it makes more sense to start with something simpler, like a go-kart.</p><p>Building a go-kart still teaches you how the steering works, how the motor functions, and more. You can even take it to the track and practice (and have a lot of fun with it) before stepping into a professional race car (or joining a company or team that is focused on building one). After all, the best race drivers often got their start by building and tinkering with their own go-karts (think Michael Schumacher and Ayrton ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>