<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The State Of LLMs 2025: Progress, Problems, and Predictions - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>The State Of LLMs 2025: Progress, Problems, and Predictions</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Dec 30, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">33 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article extensively discusses RLHF as a foundational technique that brought us ChatGPT, and contrasts it with newer RLVR approaches. Understanding RLHF&#039;s mechanics and history provides essential context for why RLVR represents a significant shift.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/mixture-of-experts/index.html">
          <strong>Mixture of experts</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">DeepSeek V3, which the article highlights as a pivotal cost-efficiency breakthrough, uses a Mixture of Experts (MoE) architecture with 671B parameters. Understanding MoE explains how DeepSeek achieved state-of-the-art performance at dramatically lower training costs.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/proximal-policy-optimization/index.html">
          <strong>Proximal policy optimization</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">The article identifies PPO as the algorithm behind the original ChatGPT&#039;s RLHF training and contrasts it with GRPO. Understanding PPO&#039;s mechanics illuminates why GRPO represents an algorithmic evolution in how reasoning models are trained.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>As 2025 comes to a close, I want to look back at some of the year’s most important developments in large language models, reflect on the limitations and open problems that remain, and share a few thoughts on what might come next.</p><p>As I tend to say every year, 2025 was a very eventful year for LLMs and AI, and this year, there was no sign of progress saturating or slowing down.</p><h1>1. The Year of Reasoning, RLVR, and GRPO</h1><p>There are many interesting topics I want to cover, but let’s start chronologically in January 2025.</p><p>​Scaling still worked, but it didn’t really change how LLMs behaved or felt in practice (the only exception to that was OpenAI’s freshly released o1, which added reasoning traces). So, when DeepSeek released their <a href="https://arxiv.org/abs/2501.12948">R1 paper</a> in January 2025, which showed that reasoning-like behavior can be developed with reinforcement learning, it was a really big deal. (Reasoning, in the context of LLMs, means that the model explains its answer, and this explanation itself often leads to improved answer accuracy.)</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kcvi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kcvi!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 424w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 848w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 1272w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kcvi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png" width="500" height="489.3543956043956" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1425,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kcvi!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 424w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 848w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 1272w, https://substackcdn.com/image/fetch/$s_!Kcvi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89f0ab43-20be-4c4d-a0f7-1a107d380222_1496x1464.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em>Figure 1: A short response and a longer response including intermediate steps that is typically generated by reasoning models.</em></figcaption></figure></div><h2><strong>1.1 The DeepSeek Moment</strong></h2><p>DeepSeek R1 got a lot of attention for various reasons:</p><p>First, DeepSeek R1 was released as an open-weight model that performed really well and was comparable to the best proprietary models (ChatGPT, Gemini, etc.) at the time.</p><p>Second, the DeepSeek R1 paper prompted many people, especially investors and journalists, to revisit the earlier <a href="https://arxiv.org/abs/2412.19437">DeepSeek V3 paper</a> from December 2024. This then led to a revised conclusion that while training state-of-the-art models is still expensive, it may be an order of magnitude cheaper than previously assumed, with estimates closer to 5 million dollars rather than 50 or 500 million.</p><div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!Tc8v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Tc8v!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 424w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 848w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 1272w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Tc8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png" width="1456" height="351" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:351,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Tc8v!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 424w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 848w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 1272w, https://substackcdn.com/image/fetch/$s_!Tc8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ca64a21-f608-4aba-9920-5a8f8216ae27_1600x386.png 1456w" sizes="100vw" loading="lazy"></picture><div></div></div></a><figcaption class="image-caption"><em>Figure 2: Table from the <a href="https://arxiv.org/abs/2412.19437">DeepSeek V3 paper</a> estimating the cost of training the 671B parameter DeepSeek V3 model.</em></figcaption></figure></div><p>​The DeepSeek R1<a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf"> supplementary materials</a> estimate that training the DeepSeek R1 model on top of DeepSeek V3 costs another $294,000, which is again much lower than everyone believed.</p><div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!2b0j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2b0j!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 424w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 848w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 1272w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2b0j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png" width="1358" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/adaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:1358,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2b0j!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 424w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 848w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 1272w, https://substackcdn.com/image/fetch/$s_!2b0j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadaa546e-94d1-4dfe-b05d-f23e43847211_1358x288.png 1456w" sizes="100vw" loading="lazy"></picture><div></div></div></a><figcaption class="image-caption"><em>Figure 3: Table from the DeepSeek R1 paper’s </em><a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-025-09422-z/MediaObjects/41586_2025_9422_MOESM1_ESM.pdf">supplementary materials</a><em> estimating the cost of training the R1 model on top of DeepSeek V3.</em></figcaption></figure></div><p>Of course, there are many caveats to the 5-million-dollar estimate. For instance, it captures only the compute credit cost for the final model run, but it doesn’t factor in the researchers’ salaries and other </p></source></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/state-of-llms-2025" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>