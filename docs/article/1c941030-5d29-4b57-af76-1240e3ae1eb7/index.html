<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Olmo 3 Is Here! - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Olmo 3 Is Here!</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Nov 21, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">6 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/hallucination-artificial-intelligence/index.html">
          <strong>Hallucination (artificial intelligence)</strong>
          <span class="read-time">10 min read</span>
        </a>
        <p class="topic-summary">The AA-Omniscience benchmark discussed in the article specifically measures how often LLMs hallucinate. This Wikipedia article explains the phenomenon, its causes, and why it&#039;s a critical problem in AI deployment for high-stakes domains like law and healthcare.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!c7_f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:245022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://kaitchup.substack.com/i/167992895?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>Hi Everyone,</p><p>In this edition of The Weekly Kaitchup, I discuss:</p><ul><li><p>Olmo 3</p></li><li><p>Eagle 3 Speculators to Easily Speed Up Inference with vLLM</p></li><li><p>AA-Omniscience: How Often LLMs Hallucinate?</p></li></ul><div><hr></div><h3>Black Friday Subscription Discount</h3><p>For Black Friday, I’m offering a <strong>30% discount</strong> on the yearly subscription to <em>The Kaitchup</em>:</p><p>With this subscription, you get instant access to all the AI notebooks (180+) and all the articles and tutorials (200+).</p><div><hr></div><h2>Olmo 3</h2><p>AI2 has released the third generation of their fully open models: Olmo 3, which includes “Thinking” models at 7B and 32B parameters. There’s also an instruct variant of the 7B model. Intermediate checkpoints (pretraining, SFT, DPO) are available as well, all grouped in this collection:</p><ul><li><p><strong><a href="https://huggingface.co/collections/allenai/olmo-3-post-training">Olmo 3 Post-training</a></strong></p></li></ul><p>They’ve also released the <strong>full training dataset</strong> (also in the collection above).</p><p><a href="https://www.datocms-assets.com/64837/1763646865-olmo_3_technical_report-1.pdf">The technical report is about 100 pages</a>. I haven’t gone through it yet, but from a quick look, their post-training recipe seems to be an improved version of <a href="https://thesalt.substack.com/p/tulu-3-the-post-training-recipe">their </a><strong><a href="https://thesalt.substack.com/p/tulu-3-the-post-training-recipe">TULU</a></strong><a href="https://thesalt.substack.com/p/tulu-3-the-post-training-recipe"> pipeline</a>.</p><p>I’ll cover these models in more detail in an upcoming article, focusing on the <strong>quantized versions</strong> I’m currently preparing with LLM Compressor.</p><p>You can already find some of my quantized models here:</p><ul><li><p><a href="https://huggingface.co/collections/kaitchup/quantized-olmo-3">Quantized Olmo 3 (vLLM-compatible)</a></p></li></ul><h3>First impressions</h3><p>The models look very strong. The 32B variant scores slightly below Qwen3-32B on benchmarks, even though Qwen3 is a 7-month-old model. That said, I’d trust Olmo 3’s reported scores much more as a proxy for real-world performance: AI2 has published the entire training data, so we can directly inspect how much the model was optimized toward specific benchmarks. I’ll know more next week, after spending more time with this model.</p><p>The 7B model appears to also perform below Qwen3 8B (again, that’s only according to benchmarks…). The NVFP4 version seems to preserve accuracy in my early experiments.</p><p>I’m running my own evaluations now and will add Olmo 3 to <strong><a href="https://kaitchup.substack.com/p/the-kaitchup-index">The Kaitchup Index</a></strong> next week.</p><div><hr></div><h2>Eagle 3 Speculators to Easily Speed Up Inference with vLLM</h2><p>Speculative decoding speeds up LLMs by using two models together. A small, cheap “speculator” model quickly drafts several next tokens in one go, and then the large “verifier” model checks that whole chunk in a single forward pass. Any tokens the verifier agrees with are accepted, so you effectively get multiple tokens for the price of one expensive step on the big model, without changing its behavior or quality. We saw how it works in </p>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/olmo-3-is-here" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>