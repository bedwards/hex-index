<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Anatomy of the Least Squares Method, Part Four - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>The Anatomy of the Least Squares Method, Part Four</h1>
        <div class="article-meta">
          <span class="author">By Tivadar Danka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/thepalindrome/index.html" class="publication">
            The Palindrome
          </a>
          <span class="separator">&middot;</span><time>Nov 17, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">18 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/rock-music-in-hungary/index.html">
          <strong>Rock music in Hungary</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (9 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/byte-pair-encoding/index.html">
          <strong>Byte-pair encoding</strong>
          <span class="read-time">7 min read</span>
        </a>
        <p class="topic-summary">The article specifically mentions the byte-pair-encoding algorithm as the tokenization method used by GPT models, explaining how it segments language based on statistical co-occurrences. Understanding this compression algorithm would give readers deeper insight into how LLMs process text.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/attention-machine-learning/index.html">
          <strong>Attention (machine learning)</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article focuses heavily on the attention mechanism as &#039;the heart and soul of a language model&#039; and uses regression to analyze attention adjustments. This Wikipedia article would provide the theoretical foundation for understanding how attention calculates word-pair importance.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p><em>Hey! Itâ€™s Tivadar from The Palindrome.</em></p><p><em>The legendary <a href="https://open.substack.com/users/382604135-mike-x-cohen-phd?utm_source=mentions">Mike X Cohen, PhD</a> is back with the final part of our deep dive into the least squares method, the bread and butter of data science and machine learning.</em></p><p><em>Enjoy!</em></p><p><em>Cheers,<br>Tivadar</em></p><div><hr></div><p>By the end of this post series, you will be confident about understanding, applying, and interpreting regression models (general linear models) that are solved using the famous least-squares algorithm. Hereâ€™s a breakdown of the post series:</p><p><strong><a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares">Part 1: Theory and math</a></strong><a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares">.</a> If you havenâ€™t read this post yet, please do so!</p><p><strong><a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares-ab5">Part 2: Explorations in simulations.</a></strong> You learned how to simulate and visualize data and regression results.</p><p><strong><a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares-818">Part 3: real-data examples</a></strong><a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares-818">.</a> Here you learned how to import, inspect, clean, and analyze a real-world dataset using the statsmodels, pandas, and seaborn libraries.</p><p><strong>Part 4 (this post): modeling GPT activations.</strong> Weâ€™ll dissect OpenAIâ€™s LLM GPT2, the precursor to its state-of-the-art ChatGPT. Youâ€™ll learn more about least-squares and also about LLM mechanisms.</p><h3><strong>Following along with code</strong></h3><p>Seeing math come alive in code gives you a deeper understanding and intuition â€” and that warm fuzzy feeling of confidence in your newly harnessed coding and machine-learning skills. <em>You can learn a lot of math with a bit of code</em><strong>.</strong></p><p>Here is the link to the <a href="https://github.com/mikexcohen/Substack/blob/main/DSUnpacked/leastSquares_4.ipynb">online code on my GitHub</a> for this post. I recommend following along with the code as you read this post.</p><div class="native-video-embed" data-component-name="VideoPlaceholder" data-attrs="{&quot;mediaUploadId&quot;:&quot;35dd791c-2613-4e3a-be28-c650af007df9&quot;,&quot;duration&quot;:null}"></div><div><hr></div><p><em>ðŸ“Œ The Palindrome breaks down advanced math and machine learning concepts with visuals that make everything click.<br><br><a href="https://thepalindrome.org/subscribe">Join the premium tier</a> to get access to <a href="https://thepalindrome.org/p/announcing-the-palindrome-lecture">the upcoming live courses</a> on Neural Networks from Scratch and Mathematics of Machine Learning.</em></p><div><hr></div><h2>Import and inspect the GPT2 model</h2><p>A large language model (LLM) is a deep-learning model that is trained to input text and generate predictions about what text should come next. Itâ€™s a form of generative AI because it uses context and learned worldview information to generate new text.</p><p>If you think LLMs are so complicated that they are impossible to understand, then I have bad news for youâ€¦ youâ€™re wrong! LLMs are not so complicated, and you can learn all about them with just a high-school-level math background. If youâ€™d like to use Python to learn how LLMs are designed and how they work, you can check out my <a href="https://mikexcohen.substack.com/p/llm-breakdown-16-tokenization-words">6-part series</a> on using machine-learning to understand LLM mechanisms here on Substack.</p><p>There are two goals of this post: ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares-a3d" class="read-button" target="_blank" rel="noopener">
          Read full article on The Palindrome &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>