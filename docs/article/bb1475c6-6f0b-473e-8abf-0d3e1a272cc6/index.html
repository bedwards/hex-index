<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GPU Networking, Part 4: Year End Wrap - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>GPU Networking, Part 4: Year End Wrap</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/chipstrat/index.html" class="publication">
            Chipstrat
          </a>
          <span class="separator">&middot;</span><time>Dec 19, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">10 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/exascale-computing/index.html">
          <strong>Exascale computing</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article mentions that NVL72 achieves 1.4 exaFLOPS and notes that the world&#039;s largest supercomputer only recently achieved an exaFLOP. Understanding the history and technical challenges of exascale computing provides crucial context for appreciating why Nvidia&#039;s achievement is significant.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/high-bandwidth-memory/index.html">
          <strong>High Bandwidth Memory</strong>
          <span class="read-time">9 min read</span>
        </a>
        <p class="topic-summary">HBM is central to the article&#039;s explanation of why scale-up networking exists - GPU memory capacity can&#039;t keep up with model size growth. Understanding HBM&#039;s architecture, stacking technology, and bandwidth characteristics would help readers grasp the fundamental constraint driving these networking innovations.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/non-uniform-memory-access/index.html">
          <strong>Non-uniform memory access</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">The article&#039;s core concept of making &#039;remote HBM feel as close to local memory as possible&#039; is essentially describing NUMA architecture principles applied to GPU clusters. Understanding NUMA provides the theoretical foundation for why latency matters so much in scale-up networking.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>As 2025 comes to a close, let’s check in on the current state of the AI networking ecosystem. We’ll touch on <strong>Nvidia</strong>, <strong>Broadcom</strong>, <strong>Marvell</strong>, <strong>AMD</strong>, <strong>Arista</strong>, <strong>Cisco</strong>, <strong>Astera Labs, Credo</strong>. <em>Can’t get to everyone, so we’ll hit on more names next year (Ayar Labs, Coherent, Lumentum, etc).</em></p><p>We’ll look at NVLink vs UALink vs ESUN. And I’ll explain the rationale behind UALoE.</p><p>We’ll talk optical, AECs, and copper.</p><p>It’ll be comprehensive. 6000 or so words, with 70% behind the paywall since I spent a really long time on this :)</p><p>I’ll even toss in some related sell-side commentary. </p><p>But first, a quick refresher on scale-up, scale-out, and scale-across. There’s still some confusion out there.</p><h2>Scale Up, Scale Out, Scale Across</h2><p>AI networking is mostly about moving data and coordinating work. The extremely parallel number crunchers (GPUs/XPUs) need to be fed data on time and in the right order.  </p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!WayZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WayZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 424w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 848w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 1272w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WayZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png" width="556" height="348.21085759244687" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:796,&quot;width&quot;:1271,&quot;resizeWidth&quot;:556,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WayZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 424w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 848w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 1272w, https://substackcdn.com/image/fetch/$s_!WayZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c3340b2-8757-47fc-b5a6-c220fc47a24c_1271x796.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em><a href="https://chalkdustmagazine.com/blog/review-number-munchers/">Number Munchers</a>. I loved this game!!! Back in the day when it was installed on the one shared computer in the classroom.</em></figcaption></figure></div><p>Scale-up, scale-out, and scale-across are three architectural approaches to transmitting that information, each with very different design tradeoffs. </p><p>It’s important to understand the nuance of scale-up, so let’s start there.</p><p><em>BTW: I’ll use GPU and XPU interchangeably as a shorthand for AI accelerator.</em></p><h3>Scale Up</h3><p>GPUs have a fixed amount of high-bandwidth memory (HBM) per chip; that capacity is increasing over time, but it can’t keep up with the rapid increase in LLM size. It’s been a long time since frontier models fit into the memory of a single chip:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!HtEB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!HtEB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 424w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 848w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!HtEB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png" width="1456" height="1042" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1042,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!HtEB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 424w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 848w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!HtEB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F168f0ee3-f4e0-463d-b8d1-fcb0f3e5343d_1492x1068.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">I asked Claude to visualize GPT model growth versus the HBM capacity of a single GPU. Note the log y-axis.</figcaption></figure></div><p>Once a model no longer fits in a single GPU’s HBM, the accelerator must fetch portions of the model from elsewhere. As there’s no nearby shared pool of HBM today, “elsewhere” means accessing HBM attached to other accelerators over a network, which is slower and less predictable than local memory.</p><p>But what if fetching memory from “elsewhere” could be made fast enough that the accelerator barely notices? <strong>That’s the motivation behind scale-up networking!</strong></p><p>Scale-up pools memory across multiple GPUs so they behave like one larger logical device with a much bigger effective HBM footprint:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!V0sd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!V0sd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 424w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 848w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 1272w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!V0sd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png" width="1456" height="807" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:807,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!V0sd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 424w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 848w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 1272w, https://substackcdn.com/image/fetch/$s_!V0sd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93ab80cb-cfcb-4b65-bcf8-d32443c3a6a5_2048x1135.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em>Source: Broadcom</em></figcaption></figure></div><p>Pulling this illusion of locality off requires really fast interconnects; the goal is to make remote </p></source></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://www.chipstrat.com/p/gpu-networking-part-4-year-end-wrap" class="read-button" target="_blank" rel="noopener">
          Read full article on Chipstrat &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>