<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Derivatives in One, Two, and a Billion Variables - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Derivatives in One, Two, and a Billion Variables</h1>
        <div class="article-meta">
          <span class="author">By Tivadar Danka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/thepalindrome/index.html" class="publication">
            The Palindrome
          </a>
          <span class="separator">&middot;</span><time>Nov 25, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">2 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/backpropagation/index.html">
          <strong>Backpropagation</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article explicitly discusses backpropagation as &#039;the premier algorithm for computing the derivatives of nodes in a computational graph&#039; - understanding its history and mathematical foundations would deeply enrich the reader&#039;s comprehension of neural network training</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/gradient-descent/index.html">
          <strong>Gradient descent</strong>
          <span class="read-time">10 min read</span>
        </a>
        <p class="topic-summary">Central to the article&#039;s discussion of training neural networks and minimizing loss functions - the Wikipedia article covers the mathematical theory, variants (stochastic, batch, mini-batch), and convergence properties that extend beyond this introductory treatment</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/automatic-differentiation/index.html">
          <strong>Automatic differentiation</strong>
          <span class="read-time">9 min read</span>
        </a>
        <p class="topic-summary">The article is building a tensor library &#039;with automatic differentiation&#039; - this Wikipedia topic explains the distinction between symbolic differentiation, numerical differentiation, and automatic differentiation, plus forward vs reverse mode which directly relates to backpropagation</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p><em>Quick note. <a href="https://www.amazon.com/Mathematics-Machine-Learning-calculus-probability/dp/1837027870/">There’s a -15% Black Friday deal for my Mathematics of Machine Learning book out there.</a> Make sure to grab it!</em></p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!9FIn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!9FIn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 424w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 848w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 1272w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!9FIn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png" width="1456" height="1796" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1796,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1027736,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://thepalindrome.org/i/179531663?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!9FIn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 424w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 848w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 1272w, https://substackcdn.com/image/fetch/$s_!9FIn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a241aa6-f0fb-4188-a0d6-a25bdd1856b0_2250x2775.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><div><hr></div><p>Hi there!</p><p>Welcome to the next lesson of the Neural Networks from Scratch course, where we’ll build a fully functional tensor library (with automatic differentiation and all) in NumPy, while mastering the inner workings of neural networks in the process.</p><p>Check the previous lecture notes here:</p><ol><li><p><a href="https://thepalindrome.org/p/vectors-and-matrices">Vectors and Matrices</a></p></li><li><p><a href="https://thepalindrome.org/p/introduction-to-computational-graphs">Introduction to Computational Graphs</a></p></li><li><p><a href="https://thepalindrome.org/p/computational-graphs-in-python">Computational Graphs in Python</a></p></li></ol><p>In the last session, we finally dove deep into neural networks and implemented our first computational graphs from scratch.</p><p>Laying the groundwork for our custom neural-networks-from-scratch framework, we have implemented the <code>Scalar</code> class, representing computational graphs built by applying mathematical operations and functions. Something like this:</p><pre><code>a = Scalar(1)
x = Scalar(2)
b = Scalar(3)

sigmoid(a * x + b) # &lt;-- this is a computational graph</code></pre><p>In essence, whenever we use a mathematical expression, say, to define a two-layer perceptron, we define a directed acyclic graph as well. This is called <em>the forward pass</em>, that is, the execution of our model.</p><p>To train the model, that is, find the weights that best fit our data, we have to perform parameter optimization with <em>gradient descent</em>. To perform gradient descent, we have to find the <em>gradient</em>. To find the gradient, we need to perform <em>backpropagation</em>, the premier algorithm for computing the derivatives of nodes in a computational graph.</p><p>Let’s start from scratch: what is the derivative?</p><h1><strong>The derivative</strong></h1><p>Mathematically speaking, computational graphs are <em>functions</em>, defined by expressions such as our recurring example of logistic regression, given by the function</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!097Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!097Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 424w, https://substackcdn.com/image/fetch/$s_!097Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 848w, https://substackcdn.com/image/fetch/$s_!097Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 1272w, https://substackcdn.com/image/fetch/$s_!097Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!097Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png" width="1456" height="531" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:531,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:21551,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thepalindrome.org/i/179531663?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!097Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 424w, https://substackcdn.com/image/fetch/$s_!097Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 848w, https://substackcdn.com/image/fetch/$s_!097Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 1272w, https://substackcdn.com/image/fetch/$s_!097Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ae13e9b-551c-4254-86f5-c10ef052d471_1920x700.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>where <em>x</em> represents our measurements, and <em>a, b</em> represent the model parameters. This is where calculus comes into play.</p><p>To train a model, we combine it with a loss function — like the mean-squared error — and minimize the loss: if <em>x₁, x₂, …, xₙ</em> are our training data samples with labels <em>y₁, y₂, …, yₙ</em>, then the mean-squared error is given by the bivariate function</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ulrQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ulrQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 424w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 848w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 1272w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ulrQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png" width="1456" height="531" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:531,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41069,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://thepalindrome.org/i/179531663?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ulrQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 424w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 848w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 1272w, https://substackcdn.com/image/fetch/$s_!ulrQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28a43792-e6a2-4227-95c2-94eb3ad732c2_1920x700.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>which is a function of the model parameters <em>a</em> and <em>b</em>.</p><p>How do we minimize the loss, that is, find the minimum of <em>L(a, b)</em>?</p><p>One idea is to measure the <em>rate of change</em> of <em>L</em> with respect to the variables <em>a</em> and <em>b</em>, then take a small step in the direction that decreases the loss. This is </p></source></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://thepalindrome.org/p/derivatives-in-one-two-and-billion" class="read-button" target="_blank" rel="noopener">
          Read full article on The Palindrome &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>