<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Notes on RNJ-1, K2-V2, Devstral 2, and GLM-4.6V - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Notes on RNJ-1, K2-V2, Devstral 2, and GLM-4.6V</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Dec 12, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">7 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/transformer-deep-learning/index.html">
          <strong>Transformer (deep learning)</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">The article repeatedly references transformer architecture, attention mechanisms, RoPE, and architectural choices. A deep understanding of the original Transformer architecture helps readers appreciate the design decisions discussed for each model.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article discusses how RNJ-1 notably skipped RL/DPO training, references GRPO and RLVR methods, and mentions the 3-effort training approach for K2-V2. Understanding RLHF provides essential context for why these training decisions matter.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!c7_f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:245022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://kaitchup.substack.com/i/167992895?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>Hi everyone,</p><p>In this edition of The Weekly Kaitchup, I’ll review a subset of the models released over the past two weeks. The list is not exhaustive: I’ll only cover models I’ve actually tested, and I’ll exclude Ministral 3, which I briefly discussed last week and for which I’m preparing a dedicated article.</p><h2>RNJ-1</h2><p>Currently trending #1 on the HF hub in the “text generation” category, RNJ-1 was released by <a href="https://www.essential.ai/">Essential AI</a>, led by Ashish Vaswani (first author of <em>Attention Is All You Need</em>, the Transformer paper).</p><ul><li><p><a href="https://huggingface.co/EssentialAI/rnj-1-instruct">EssentialAI/rnj-1-instruct</a></p></li></ul><p>It’s an 8.3B decoder-only dense model with a very conventional design. It’s based on the Gemma architecture, but trained from scratch: 32 transformer blocks, rotary position embeddings, RMSNorm, and a gated MLP. Attention uses grouped-query attention. There are no architectural tricks. It also has a more common vocabulary size of 128k entries (against 262k for Gemma 3).</p><p>Pretraining was done with an 8k context window on a mixed corpus that leans toward code and STEM material. The optimizer is Muon (the new standard, it seems) with: warmup, flat phase, then cosine decay, all at fairly high global batch sizes. After this, the model is pushed through a mid-training phase that extends the context to 32k using a YaRN-style scheme and reinforces behaviour on longer sequences. On top of that, they did a sizeable supervised fine-tuning stage, again biased toward code, math, and structured QA. Again, everything looks very normal.</p><p>Where it’s different is that they stopped there. No fancy RL or DPO.</p><p>Because the SFT layer is not especially “heavy” in the alignment sense, RNJ-1-Instruct behaves like a usable chat model but still exposes the underlying base distribution enough that further fine-tuning is straightforward. </p><p>In practice, it handles typical code and math benchmarks at the level you’d expect for a well-trained 8B model with that data profile. The main reason to care, if you are an implementer, is that you get a dense 8B backbone with 32k context and a fully specified training recipe that you can easily fine-tune.</p><p>They also show this on the model card:</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!FtIK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!FtIK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 424w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 848w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 1272w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!FtIK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png" width="989" height="590" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:989,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Pass at k evals&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="Pass at k evals" title="Pass at k evals" srcset="https://substackcdn.com/image/fetch/$s_!FtIK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 424w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 848w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 1272w, https://substackcdn.com/image/fetch/$s_!FtIK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d5474bb-65d5-4984-bb25-b7898f43dcfb_989x590.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>This “accuracy vs pass@k” shows that the model can find a better answer if given more tries. Good pass@k indicates that the model is a good target for RLVR methods like GRPO.</p><p>However, nothing is surprising here. Most recent models, not trained with RL, have a very similar pass@k behavior. Here are my </p></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/notes-on-rnj-1-k2-v2-devstral-2-and" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>