<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Anatomy of the Least Squares Method, Part One - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>The Anatomy of the Least Squares Method, Part One</h1>
        <div class="article-meta">
          <span class="author">By Tivadar Danka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/thepalindrome/index.html" class="publication">
            The Palindrome
          </a>
          <span class="separator">&middot;</span><time>Oct 13, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">14 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p><em>Hi there! It‚Äôs Tivadar from The Palindrome.</em></p><p><em>Today‚Äôs post is the first in a series by the legendary <span class="mention-wrap" data-attrs="{&quot;name&quot;:&quot;Mike X Cohen, PhD&quot;,&quot;id&quot;:382604135,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3c804d93-69c2-49a9-a797-2216b4bae5ba_1000x1000.jpeg&quot;,&quot;uuid&quot;:&quot;60617d9c-6a04-4717-b79f-a70a8947fe78&quot;}" data-component-name="MentionToDOM"></span>, educator extraordinaire.</em></p><p><em>In case you haven‚Äôt encountered him yet, Mike is an extremely prolific author; <a href="https://www.amazon.com/stores/author/B00EWB0HO2/allbooks">his textbooks</a> and <a href="https://sincxpress.com/">online courses</a> range from <a href="https://www.amazon.com/Analyzing-Neural-Time-Data-Practice-ebook/dp/B08FZLS7X7">time series analysis</a> through <a href="https://www.amazon.com/Modern-Statistics-Intuition-Math-Python-ebook/dp/B0CQV82FRT">statistics</a> to <a href="https://www.amazon.com/Practical-Linear-Algebra-Data-Science-ebook/dp/B0BDCBP4VP">linear algebra</a>, all with a focus on practical implementations as well.</em></p><p><em>He also recently started on Substack, and if you enjoy The Palindrome, you‚Äôll enjoy his publication too. <a href="https://mikexcohen.substack.com/">So, make sure to subscribe!</a></em></p><p><em>The following series explores the least squares method, a foundational tool in mathematics, data science, and machine learning.</em></p><p><em>Have fun!</em></p><p><em>Cheers,<br>Tivadar</em></p><div><hr></div><p>By the end of this post series, you will be confident about understanding, applying, and interpreting the least-squares algorithm for fitting machine learning models to data. ‚ÄúLeast-squares‚Äù is one of the most important techniques in machine learning and statistics. It is fast, one-shot (non-iterative), easy to interpret, and mathematically optimal. Here‚Äôs a breakdown of what you‚Äôll learn:</p><p><strong>Post 1 (what you‚Äôre reading now üôÇ): Theory and math</strong>. You‚Äôll learn what ‚Äúleast-squares‚Äù means, why it works, and how to find the optimal solution. There‚Äôs some linear algebra and calculus in this post, but I‚Äôll explain the main take-home points in case you‚Äôre not so familiar with the math bits.</p><p><strong>Post 2: Explorations in simulations.</strong> You‚Äôll learn how to simulate data to supercharge your intuition for least-squares, and how to visualize the results. You‚Äôll also learn about residuals and overfitting.</p><p><strong>Post 3: real-data examples.</strong> There‚Äôs no real substitute for real data. And that‚Äôs what you‚Äôll experience in this post. I‚Äôll also teach you how to use the Python <code>statsmodels</code> library.</p><p><strong>Post 4: modeling GPT activations.</strong> This post will be fun and fascinating. We‚Äôll dissect OpenAI‚Äôs LLM GPT2, the precursor to its state-of-the-art ChatGPT. You‚Äôll learn more about least-squares and also about LLM mechanisms.</p><h3>Following along with code</h3><p>I‚Äôm a huge fan of learning math through coding. <em>You can learn a lot of math with a bit of code</em><strong>.</strong></p><p>That‚Äôs why I have Python notebook files that accompany my posts. The essential code bits are pasted directly into this post, but the complete code files, including all the code for visualization and additional explorations, <a href="https://github.com/mikexcohen/Substack/blob/main/DSUnpacked/leastSquares_1.ipynb">are here on my GitHub</a>.</p><p>If you‚Äôre more interested in the theory/concepts, then it‚Äôs completely fine to ignore the code and just read the post. But if you want a deeper level of ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://thepalindrome.org/p/the-anatomy-of-the-least-squares" class="read-button" target="_blank" rel="noopener">
          Read full article on The Palindrome &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>