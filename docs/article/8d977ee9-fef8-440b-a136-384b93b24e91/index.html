<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Against &quot;If Anyone Builds It Everyone Dies&quot;  - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Against &quot;If Anyone Builds It Everyone Dies&quot; </h1>
        <div class="article-meta">
          <span class="author">By Bentham&#039;s Bulldog</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/benthams/index.html" class="publication">
            
          </a>
          <span class="separator">&middot;</span><time>Jan 20, 2026</time>
          <span class="separator">&middot;</span>
          <span class="read-time">41 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <h1>1 Introduction </h1><p>Unlike most books, the thesis of <em>If Anyone Builds It Everyone Dies</em> is the title (a parallel case is that the thesis of <em>What We Owe The Future</em> is “What?? We owe the future?).  IABIED, by Yudkowsky and Soares (Y&amp;S), argues that if anyone builds AI, everyone everywhere, will die.  And this isn’t, like, a metaphor for it causing mass unemployment or making people sad—no, they think that everyone everywhere on Earth will stop breathing.  (I’m thinking of writing a rebuttal book called “If Anyone Builds It, Low Odds Anyone Dies, But Probably The World Will Face A Range of Serious Challenges That Merit Serious Global Cooperation,” but somehow, my guess is editors would like that title less).  </p><p>The core argument of the book is this: as things get really smart, they get lots of new options which make early attempts to control them pretty limited.  Evolution tried to get us to have a bunch of kids.  Yet as we got smarter, we got more unmoored from that core directive.  </p><p>The best way to maximize inclusive genetic fitness would be to give your sperm to sperm banks and sleep around all the time without protection, but most people don’t do that.  Instead people spend their time hanging out—but mostly not sleeping with—friends, scrolling on social media, and going to college.  Some of us are such degenerate reprobates that we try to improve shrimp welfare!  Evolution spent 4 billion years trying to get us to reproduce all the time, and we proceeded to ignore that directive, preferring to spend time watching nine-second TikTok videos.  </p><p>Evolution didn’t aim for any of these things.  They were all unpredictable side-effects. The best way to achieve evolution’s aims was to give us weird sorts of drives and desires.  However, once we got smart, we figured out other ways to achieve those drives and  desires.  IABIED argues that something similar will happen with AI.  We’ll train the AI to have sort of random aims picked up from our wildly imperfect optimization method.  </p><p>Then the AI will get super smart, realize that a better way of achieving those aims is to do something else.  Specifically, for most aims, the best way to achieve them wouldn’t involve keeping pesky humans around, who can stop them.  So the AI will come up with some clever scheme by which it can kill or disempower us, implement it so ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://benthams.substack.com/p/against-if-anyone-builds-it-everyone" class="read-button" target="_blank" rel="noopener">
          Read full article on  &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>