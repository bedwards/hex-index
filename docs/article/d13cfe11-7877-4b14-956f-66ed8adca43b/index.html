<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Best GPUs Under $1,500 for AI: Should You Upgrade? - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Best GPUs Under $1,500 for AI: Should You Upgrade?</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Nov 17, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">2 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/graphics-processing-unit/index.html">
          <strong>Graphics processing unit</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">While readers know GPUs exist, the Wikipedia article covers the fascinating history from early video display controllers through NVIDIA&#039;s invention of the GPU term in 1999, the architectural evolution that made them suitable for parallel computing beyond graphics, and the technical reasons why GPU memory bandwidth became critical for AI workloads</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/cuda/index.html">
          <strong>CUDA</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article benchmarks NVIDIA GPUs and mentions &#039;kernel implementations&#039; without explaining why NVIDIA dominates AI computing. The CUDA Wikipedia article explains the parallel computing platform&#039;s history, how it transformed GPUs into general-purpose processors, and why software ecosystem lock-in has maintained NVIDIA&#039;s AI hardware monopoly despite AMD alternatives</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!gM-Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!gM-Z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!gM-Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png" width="674" height="449.4876373626374" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:674,&quot;bytes&quot;:2513770,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://kaitchup.substack.com/i/177027970?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!gM-Z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 424w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 848w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 1272w, https://substackcdn.com/image/fetch/$s_!gM-Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2daa3408-7d4d-44ac-9551-5716fc0b9197_1536x1024.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">Image generated with ChatGPT</figcaption></figure></div><p>We often see inference throughput and fine-tuning stats for consumer GPUs, but they mostly focus on the high end (RTX 4090/5090). What about more affordable cards: are they simply too slow, or too memory-constrained to run and fine-tune LLMs?</p><p>To find out, I benchmarked GPUs across the last three NVIDIA RTX generations: 3080 Ti, 3090, 4070 Ti, 4080 Super, 4090, 5080, and 5090. With the exception of the xx90 cards, these GPUs offer only 12–16 GB of VRAM.</p><p>Using vLLM, I measured throughput when the model fully fits in GPU memory and when part of it must be offloaded to system RAM. For fine-tuning, I evaluated both LoRA and QLoRA on 1.7B and 8B LLMs.</p><p>Benchmark code and logs:</p><p>I used GPUs from <a href="https://runpod.io?ref=1ip9lvtj">RunPod (referral link)</a> and also report cost-efficiency based on their pricing.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!8h_i!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8h_i!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 424w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 848w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 1272w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8h_i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png" width="1200" height="742" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:742,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8h_i!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 424w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 848w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 1272w, https://substackcdn.com/image/fetch/$s_!8h_i!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87756e4f-6c87-4564-b6d2-b1ec1d92b93d_1200x742.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><h2>Running LLMs without High-End GPUs</h2><p>To benchmark GPUs for inference throughput, use the same stack you plan to deploy. It sounds obvious, but many popular (often marketing-driven) benchmarks don’t resemble real inference frameworks, so <strong>their numbers are speeds you’ll never hit in your use case</strong>. If you run Ollama, benchmark with Ollama and GGUF models. If you use vanilla Hugging Face Transformers, benchmark with Transformers directly.</p><p>Different libraries ship different kernel implementations, each optimized to varying degrees for specific GPU generations.</p>
      <p>
          <a href="https://kaitchup.substack.com/p/best-gpus-under-1500-for-ai-should">
              Read more
          </a>
      </p></source></source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/best-gpus-under-1500-for-ai-should" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>