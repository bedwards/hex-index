<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The State of Reinforcement Learning for LLM Reasoning - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>The State of Reinforcement Learning for LLM Reasoning</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Apr 19, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">38 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>A lot has happened this month, especially with the releases of new flagship models like GPT-4.5 and Llama 4. But you might have noticed that reactions to these releases were relatively muted. Why? One reason could be that GPT-4.5 and Llama 4 remain conventional models, which means they were trained without explicit reinforcement learning for reasoning.</p><p>Meanwhile, competitors such as xAI and Anthropic have added more reasoning capabilities and features into their models. For instance, both the xAI Grok and Anthropic Claude interfaces now include a "thinking" (or "extended thinking") button for certain models that explicitly toggles reasoning capabilities.</p><p>In any case, the muted response to GPT-4.5 and Llama 4 (non-reasoning) models suggests we are approaching the limits of what scaling model size and data alone can achieve.</p><p>However, OpenAI's recent release of the o3 reasoning model demonstrates there is still considerable room for improvement when investing compute strategically, specifically via reinforcement learning methods tailored for reasoning tasks. (According to OpenAI staff during the recent livestream, o3 used 10× more training compute compared to o1.)</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!DWHh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!DWHh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 424w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 848w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1272w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png" width="693" height="432.1730769230769" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:908,&quot;width&quot;:1456,&quot;resizeWidth&quot;:693,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!DWHh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 424w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 848w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1272w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em>Source: OpenAI livestream (https://openai.com/live/) on April 16, 2025</em></figcaption></figure></div><p>While reasoning alone isn't a silver bullet, it reliably improves model accuracy and problem-solving capabilities on challenging tasks (so far). And I expect reasoning-focused post-training to become standard practice in future LLM pipelines.</p><p>So, in this article, let's explore the latest developments in reasoning via reinforcement learning.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!2SLQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2SLQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 424w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 848w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png" width="713" height="497.532967032967" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:1456,&quot;resizeWidth&quot;:713,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2SLQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 424w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 848w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><em>This article focuses on reinforcement learning training methods used to develop and improve reasoning models</em></figcaption></figure></div><p>Because it is a relatively long article, I am providing a Table of Contents overview below. To navigate the table of contents, please use the slider on the left-hand side in the web view.</p><ul><li><p>Understanding reasoning models</p></li><li><p>RLHF basics: where it all started</p></li><li><p>A brief introduction to PPO: RL's workhorse algorithm</p></li><li><p>RL algorithms: from PPO to GRPO</p></li><li><p>RL reward modeling: from RLHF to RLVR</p></li><li><p>How the DeepSeek-R1 reasoning models were trained</p></li><li><p>Lessons from recent RL papers on training reasoning models</p></li><li><p>Noteworthy research papers on training reasoning models</p></li></ul><p><strong>Tip:</strong> If you are already familiar with reasoning basics, RL, PPO, and GRPO, please feel free to directly jump ahead to the “Lessons from recent RL papers on training reasoning models” section, which contains summaries of interesting insights from recent reasoning research papers.</p><h2><strong>Understanding reasoning models</strong></h2><p>The big elephant in the room is, of course, the definition of reasoning. In short, reasoning is about inference and training techniques </p></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>