<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A pragmatic guide to LLM evals for devs - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>A pragmatic guide to LLM evals for devs</h1>
        <div class="article-meta">
          <span class="author">By Gergely Orosz</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/pragmaticengineer/index.html" class="publication">
            The Pragmatic Engineer
          </a>
          <span class="separator">&middot;</span><time>Dec 2, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">14 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/grounded-theory/index.html">
          <strong>Grounded theory</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (30 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/likert-scale/index.html">
          <strong>Likert scale</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (13 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/test-driven-development/index.html">
          <strong>Test-driven development</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">The article discusses how traditional TDD falls short for LLM applications due to non-deterministic outputs. Understanding the formal methodology of TDD - its red-green-refactor cycle, origins in extreme programming, and assumptions about deterministic correctness - provides essential context for why LLM evals represent a paradigm shift in software quality assurance.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>One word that keeps cropping up when I talk with software engineers who build large language model (LLM)-based solutions is “<strong>evals</strong>”. They use evaluations to verify that LLM solutions work well enough because LLMs are non-deterministic, meaning there’s no guarantee they’ll provide the same answer to the same question twice. This makes it more complicated to verify that things work according to spec than it does with other software, for which automated tests are available.</p><p>Evals feel like they are becoming a core part of the AI engineering toolset. And because they are also becoming part of CI/CD pipelines, we, software engineers, should understand them better — especially because we might need to use them sooner rather than later! So, what do good evals look like, and how should this non-deterministic-testing space be approached?</p><p>For directions, I turned to an expert on the topic, <a href="https://hamel.dev/">Hamel Husain</a>. He’s worked as a Machine Learning engineer at companies including Airbnb and GitHub, and teaches the online course <a href="https://maven.com/parlance-labs/evals?promoCode=gergley-49">AI Evals For Engineers &amp; PMs</a> — the upcoming cohort <a href="https://maven.com/parlance-labs/evals?promoCode=gergley-49">starts in January</a>. Hamel is currently writing a book, <a href="https://www.oreilly.com/library/view/evals-for-ai/9798341660717/">Evals for AI Engineers</a>, to be published by O’Reilly next year.</p><p>In today’s issue, we cover:</p><ol><li><p><strong>Vibe-check development trap. </strong>An agent appears to work well, but as soon as it is modified, it can’t be established that it’s working correctly.</p></li><li><p><strong>Core workflow: error analysis</strong>. Error analysis has been a key part of machine learning for decades and is useful for building LLM applications.</p></li><li><p><strong>Building evals: the right tools for the job. </strong>Use code-based evals for deterministic failures, and an LLM-as-judge for subjective cases.</p></li><li><p><strong>Building an LLM-as-judge. </strong>Avoid your LLM judge memorizing answers by partitioning your data and measuring how well the judge generalizes to unfamiliar data.</p></li><li><p><strong>Align the judge, keep trust. </strong>The LLM judge’s expertise needs to be validated against human expertise. Consider metrics like True Positive Rate (TPR) and True Negative Rate (TNR).</p></li><li><p><strong>Evals in practice: from CI/CD to production monitoring. </strong>Use evals in the CI/CD pipeline, but use production data to continuously validate that they work as expected, too.</p></li><li><p><strong>Flywheel of improvement. </strong>Analyze → measure → Improve → automate → start again</p></li></ol><p>With that, it’s over to Hamel:</p><div><hr></div><h2>1. Vibe-check development trap</h2><p>Organizations are embedding LLMs into applications from customer service to content creation. Yet, unlike traditional software, LLM pipelines don’t produce deterministic outputs; their responses are often subjective ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://newsletter.pragmaticengineer.com/p/evals" class="read-button" target="_blank" rel="noopener">
          Read full article on The Pragmatic Engineer &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>