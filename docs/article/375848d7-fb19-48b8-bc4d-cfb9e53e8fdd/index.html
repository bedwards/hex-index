<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Eagle 3 Speculators: When To Use Them? - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Eagle 3 Speculators: When To Use Them?</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Dec 9, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">2 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/autoregressive-model/index.html">
          <strong>Autoregressive model</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">Explains why LLMs generate tokens sequentially and why this creates a bottleneck that speculative decoding attempts to solve - essential background for understanding the inference optimization problem</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/knowledge-distillation/index.html">
          <strong>Knowledge distillation</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The technique of training smaller models to mimic larger ones is directly relevant to how draft models like EAGLE-3 are trained to predict what the target model would generate</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>EAGLE-3 is a family of draft models (“speculators”) for speculative decoding. As with other speculative setups, a small model proposes several future tokens and a larger target model verifies them in a single pass. When the draft model’s guesses are accurate and cheap enough, this reduces the total number of heavy forward passes, and therefore the overall cost of inference, while keeping the target model’s output unchanged.</p><p>The EAGLE-3 speculators are designed to raise the acceptance rate of drafted tokens and make better use of each verification pass. They do this through architectural changes (multi-layer feature fusion) and a training setup that more closely matches how speculative decoding is actually run at inference time. The aim is to shift more of the work onto a small, fast model and let the large model act mainly as a validator.</p><p>In this article, I will look at EAGLE-3 in practice using <a href="https://huggingface.co/collections/RedHatAI/speculator-models">the released speculators with vLLM</a>. I will experiment with Qwen3 32B on an A100 80 GB GPU and focus on end-to-end behavior: throughput, acceptance length, and wall-clock latency. In particular, I will compare high-concurrency continuous batching, where the GPU is already saturated, with batch size 1, where speculative decoding has more opportunity to lower the effective cost per generated token.</p><p>The following notebook shows how to run and evaluate EAGLE-3 speculators:</p><h2><strong>EAGLE-3: High-Accuracy Draft Models for Fast Speculative Decoding</strong></h2>
      <p>
          <a href="https://kaitchup.substack.com/p/eagle-3-speculators-when-to-use-them">
              Read more
          </a>
      </p>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/eagle-3-speculators-when-to-use-them" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>