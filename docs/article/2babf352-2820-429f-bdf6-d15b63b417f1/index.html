<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Learning Weekly: Issue 431 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Deep Learning Weekly: Issue 431</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/deeplearningweekly/index.html" class="publication">
            Deep Learning Weekly
          </a>
          <span class="separator">&middot;</span><time>Nov 19, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">6 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">MiroThinker uses reinforcement learning for interaction scaling with environment feedback - RLHF is the foundational technique that enabled modern AI agents to learn from interactions, directly relevant to understanding how these research agents improve</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>This week in deep learning, we bring you <a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo">Gemini 3</a>, <a href="https://www.comet.com/site/blog/ai-agents/?utm_source=substack&amp;utm_medium=email&amp;utm_campaign=dlw&amp;utm_content=ai-agents/">The Definitive Guide to Agentic AI</a>, and <a href="https://arxiv.org/abs/2511.10647">a paper on Depth Anything 3: Recovering the Visual Space from Any Views</a>.</p><p>You may also enjoy <a href="https://openai.com/index/gpt-5-1/">GPT-5.1</a>, <a href="https://www.anthropic.com/engineering/code-execution-with-mcp">Code execution with MCP: building more efficient AI agents</a>, <a href="https://arxiv.org/abs/2511.11793">a paper on MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</a>, and more!</p><p>As always, happy reading and hacking. If you have something you think should be in next weekâ€™s issue, find us on Twitter: <a href="https://twitter.com/dl_weekly">@dl_weekly</a>.</p><p>Until next week!</p><div><hr></div><h2><strong>Industry</strong></h2><p><strong><a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo">A new era of intelligence with Gemini 3</a></strong></p><p>Google releases Gemini 3 Pro with breakthrough reasoning scores, PhD-level performance on benchmarks, and enhanced multimodal and agentic coding capabilities.</p><p><strong><a href="https://x.ai/news/grok-4-1">Grok 4.1 | xAI</a></strong></p><p>xAI introduced Grok 4.1, which brings significant improvements to the real-world usability of Grok.</p><p><strong><a href="https://openai.com/index/gpt-5-1/">GPT-5.1: A smarter, more conversational ChatGPT</a></strong></p><p>OpenAI releases GPT-5.1 with adaptive reasoning, improved conversational style, and enhanced customization options for ChatGPT users.</p><h2><strong>MLOps &amp; LLMOps</strong>.</h2><p><strong><a href="https://www.comet.com/site/blog/ai-agents/?utm_source=substack&amp;utm_medium=email&amp;utm_campaign=dlw&amp;utm_content=ai-agents/">The Definitive Guide to Agentic AI: What AI Agents Actually Are and How to Build Them for Production</a></strong></p><p>Discover the core principles behind truly agentic AI systems, how to build them for production, and the reasons they often fail at scale.</p><p><strong><a href="https://qdrant.tech/blog/qdrant-1.16.x/">Qdrant 1.16 - Tiered Multitenancy &amp; Disk-Efficient Vector Search</a></strong></p><p>A technical update announcing Qdrant 1.16, which introduces Tiered Multitenancy, the ACORN search algorithm, and Inline Storage for disk-efficient, high-performance vector search.</p><p><strong><a href="https://developer.nvidia.com/blog/building-an-interactive-ai-agent-for-lightning-fast-machine-learning-tasks/">Building an Interactive AI Agent for Lightning-Fast Machine Learning Tasks</a></strong></p><p>A technical blog post about building a data science agent using Nemotron Nano-9B-v2 and CUDA-X libraries, delivering massive 3x to 43x speedups for ML experimentation.</p><p><strong><a href="https://www.anthropic.com/engineering/code-execution-with-mcp">Code execution with MCP: building more efficient AI agents \ Anthropic</a></strong></p><p>An article detailing how adopting code execution with the Model Context Protocol (MCP) reduces token consumption and increases efficiency for AI agents managing hundreds of tools.</p><p><strong><a href="https://opensearch.org/blog/introducing-real-time-streaming-for-ai-models-and-agents-in-opensearch/">Real-time streaming for AI models and agents in OpenSearch</a></strong></p><p>A blog post launching experimental real-time streaming capabilities in OpenSearch 3.3 via the Predict Stream and Execute Stream Agent APIs.</p><h2><strong>Learning</strong></h2><p><strong><a href="https://www.philschmid.de/gemini-3-prompt-practices">Gemini 3 Prompting: Best Practices for General Usage</a></strong></p><p>An instructional guide providing best practices for prompting Gemini 3 Pro, focusing on core principles like precise instructions, structured XML/Markdown tagging, and more.</p><p><strong><a href="https://pair.withgoogle.com/explorables/sae/">Mapping LLMs with Sparse Autoencoders</a></strong></p><p>An explainer describing Sparse Autoencoders (SAEs) as a technique to map LLM activations into monosemantic, interpretable features, allowing researchers to ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.deeplearningweekly.com/p/deep-learning-weekly-issue-431" class="read-button" target="_blank" rel="noopener">
          Read full article on Deep Learning Weekly &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>