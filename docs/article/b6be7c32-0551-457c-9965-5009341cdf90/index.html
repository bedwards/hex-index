<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Quantizing Olmo 3: Most Efficient and Accurate Formats - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Quantizing Olmo 3: Most Efficient and Accurate Formats</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Dec 2, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">1 min read</span>
        </div>
      </header>

      <div class="article-excerpt">
        <p></p>
        <div class="excerpt-fade"></div>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/quantizing-olmo-3-most-efficient" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/quantization-signal-processing/index.html">
          <strong>Quantization (signal processing)</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article focuses on quantizing neural network weights to different bit formats (INT8, FP8, NVF4). Understanding the mathematical foundations of quantization—how continuous values are mapped to discrete levels and the tradeoffs involved—provides essential context for why these techniques reduce model size while potentially sacrificing accuracy.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article mentions Olmo 3&#039;s training pipeline includes DPO (Direct Preference Optimization) and RL stages. RLHF is the foundational technique that these methods build upon, explaining how language models are aligned to human preferences through reward modeling and policy optimization.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/floating-point-arithmetic/index.html">
          <strong>Floating-point arithmetic</strong>
          <span class="read-time">7 min read</span>
        </a>
        <p class="topic-summary">The quantization formats discussed (fp8-dynamic, nvfp4, w4a16) all involve different floating-point representations. Understanding how floating-point numbers work—mantissa, exponent, precision tradeoffs—helps readers grasp why FP8 and FP4 formats can approximate full-precision weights with significant memory savings.</p>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>