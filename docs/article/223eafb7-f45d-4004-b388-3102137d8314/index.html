<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Proving (literally) that ChatGPT isn&#039;t conscious - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Proving (literally) that ChatGPT isn&#039;t conscious</h1>
        <div class="article-meta">
          <span class="author">By Erik Hoel</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/theintrinsicperspective/index.html" class="publication">
            
          </a>
          <span class="separator">&middot;</span><time>Jan 15, 2026</time>
          <span class="separator">&middot;</span>
          <span class="read-time">11 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/integrated-information-theory/index.html">
          <strong>Integrated information theory</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (15 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/universal-approximation-theorem/index.html">
          <strong>Universal approximation theorem</strong>
          <span class="read-time">1 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (11 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/kolmogorov-complexity/index.html">
          <strong>Kolmogorov complexity</strong>
          <span class="read-time">10 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (31 min read)</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZZ9d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg" width="1456" height="472" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:472,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 424w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 848w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!ZZ9d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5f9e8e24-7b87-4324-9868-d459a9eb0630_2534x822.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>Imagine we could prove that there is nothing it is like to be ChatGPT. Or any other Large Language Model (LLM). That they have no experiences associated with the text they produce. That they do not actually feel happiness, or curiosity, or discomfort, or anything else. Their shifting claims about consciousness are remnants from the training set, or guesses about what you’d like to hear, or the acting out of a persona.</p><p>You may already believe this, but a proof would mean that a lot of people who think otherwise, including some major corporations, have been playing make believe. Just as a child easily grants consciousness to a doll, humans are predisposed to grant consciousness easily, and so we have been fooled by “<a href="https://mustafa-suleyman.ai/seemingly-conscious-ai-is-coming">seemingly conscious AI</a>.”</p><p>However, without a proof, the current state of LLM consciousness discourse is closer to “Well, that’s just like, your opinion, man.”</p><p>This is because there is no scientific consensus around exactly how consciousness works (although, at least, those in the field do mostly share a <a href="https://www.theintrinsicperspective.com/p/consciousness-is-a-great-mystery">common definition</a> of what we seek to understand). There are <a href="https://closertotruth.com/news/what-350-different-theories-of-consciousness-reveal-about-reality/">currently hundreds</a> of scientific theories of consciousness trying to explain how the brain (or other systems, like AIs) generates subjective and private states of experience. I got my PhD in neuroscience helping develop one such theory of consciousness: <a href="https://en.wikipedia.org/wiki/Integrated_information_theory">Integrated Information Theory</a>, working under Giulio Tononi, its creator. And I’ve studied consciousness all my life. But which theory out of these hundreds is correct? Who knows! Honestly? Probably none of them.</p><p>So, how would it be possible to rule out LLM consciousness altogether?</p><p>In a <a href="https://web3.arxiv.org/pdf/2512.12802">new paper</a>, now up on arXiv, <strong>I prove that </strong><em><strong>no</strong></em><strong> non-trivial theory of consciousness </strong><em><strong>could</strong></em><strong> exist that grants consciousness to LLMs.</strong> </p><p>Essentially, meta-theoretic reasoning allows us to make statements about all possible theories of consciousness, and so lets us jump to the end of the debate: the conclusion of LLM non-consciousness.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!nKNU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!nKNU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 424w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 848w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 1272w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!nKNU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png" width="1456" height="1639" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1639,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:647044,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.theintrinsicperspective.com/i/183813754?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!nKNU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 424w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 848w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 1272w, https://substackcdn.com/image/fetch/$s_!nKNU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd66d2b97-e59f-4926-8b5a-7bdd6a750f6b_1476x1662.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p><a href="https://web3.arxiv.org/pdf/2512.12802">You can now read the paper on arXiv.</a></p><p>What is uniquely powerful about this proof is that it requires you to believe nothing specific about consciousness other than a scientific theory of consciousness should be <em>falsifiable</em> and <em>non-trivial</em>. If you believe those things, you should deny LLM consciousness. </p><p>Before the details, I think it is helpful to say what this proof is not.</p><ul><li><p>It is not arguing about probabilities. LLMs are not conscious.</p></li><li><p>It is not applying some theory </p></li></ul></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://www.theintrinsicperspective.com/p/proving-literally-that-chatgpt-isnt" class="read-button" target="_blank" rel="noopener">
          Read full article on  &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>