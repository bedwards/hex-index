<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Import AI 418: 100b distributed training run; decentralized robots; AI myths - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Import AI 418: 100b distributed training run; decentralized robots; AI myths</h1>
        <div class="article-meta">
          <span class="author">By Jack Clark</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/importai/index.html" class="publication">
            Import AI
          </a>
          <span class="separator">&middot;</span><time>Jun 30, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">16 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>Welcome to Import AI, a newsletter about AI research. Import AI runs on lattes, ramen, and feedback from readers. If you’d like to support this, please subscribe.</p><p><strong>Better video models with radial attention:<br></strong><em>…Efficiency improvements for internet-generated media…<br></em>Researchers with MIT, NVIDIA, Princeton, UC Berkeley, Stanford and startup First Intelligence have built and released Radial Attention, an attention mechanism that can be used for training and sampling from video generation models.<br>   "Unlike image generation, video synthesis involves an additional temporal dimension, dramatically increasing the number of tokens to process. As self attention scales quadratically with sequence length, training and inference on long videos become prohibitively expensive, limiting model practicality and scalability," they write. "The key insight of Radial Attention is that attention scores between tokens decay with increasing spatial and temporal distance. This motivates us to allocate computation based on the inherent spatiotemporal correlations in video data".<br><br><strong>Good performance on real world models</strong>: The results are convincing: the authors show that they're able to get a 2.78X training speedup and 2.35X inference speedup on Hunyuan Video, a good video generation model <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo">from Tencent</a>.<br>   They also demonstrate similarly good performance (1.78X training, 1.63X inference) on the <a href="https://github.com/genmoai/mochi">Mochi 1 video model</a>.<br>   "At default video lengths, Radial Attention achieves up to a 1.9× speedup while maintaining video quality. For videos up to 4× longer, Radial Attention preserves video fidelity and delivers up to 4.4× and 3.7× speedups in training and inference, respectively, with minimal LoRA fine-tuning," they write.<br><br><strong>Why this matters - making it cheaper to do AI entertainment</strong>: The internet has become a vast engine for the consumption of video content - see social media shorts, YouTube, the streaming services, etc. Technologies like Radial Attention will help lower the cost of training and sampling from AI video models, which will make it cheaper to produce synthetic video content. Where the internet before was the place that we stored videos that were gathered from the world, it will now increasingly become a machine where people use internet-mediated services to generate videos, then internet-mediated services to propagate them as well.<br>   <strong>Read more:</strong> <a href="https://arxiv.org/abs/2506.19852">Radial Attention: O(nlogn) Sparse Attention with Energy Decay for Long Video Generation (arXiv)</a>.<br>   <strong>Get the code </strong>for <a href="https://github.com/mit-han-lab/radial-attention">Radial Attention here (MIT-han-lab, GitHub)</a>.<br><br>***<br><br><strong>Pete Buttigieg thinks AI is a big deal:<br></strong>Fellow Substacker and former presidential candidate Pete Buttigieg has written a post about how he ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://importai.substack.com/p/import-ai-418-100b-distributed-training" class="read-button" target="_blank" rel="noopener">
          Read full article on Import AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>