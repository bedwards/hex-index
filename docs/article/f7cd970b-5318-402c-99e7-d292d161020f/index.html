<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chinese AI in 2025, Wrapped - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Chinese AI in 2025, Wrapped</h1>
        <div class="article-meta">
          <span class="author">By Jordan Schneider</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/chinatalk/index.html" class="publication">
            ChinaTalk
          </a>
          <span class="separator">&middot;</span><time>Dec 11, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">13 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/mixture-of-experts/index.html">
          <strong>Mixture of experts</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">The article repeatedly references DeepSeek&#039;s Mixture-of-Experts (MoE) architecture as a key innovation enabling cost-efficient training. Understanding this machine learning technique would help readers grasp why this architectural choice was significant for Chinese AI development under compute constraints.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/export-control/index.html">
          <strong>Export control</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">The US-China chip war and export restrictions on Nvidia H20s/H200s are central to the article&#039;s narrative about Chinese AI development. Understanding the history and mechanisms of export controls provides essential context for the ongoing technology rivalry.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>A year for the history books for the Chinese AI beat. We began the year astonished by DeepSeek’s frontier model, and are ending in December with Chinese open models like Qwen <a href="https://www.thewirechina.com/2025/11/09/cheap-and-open-source-chinese-ai-models-are-taking-off/">powering</a> Silicon Valley’s startup gold rush.</p><p>It’s a good time to stop and reflect on Chinese AI milestones throughout 2025. What really mattered, and what turned out to be nothingburgers? </p><p>This piece recaps:</p><ul><li><p>The biggest model drops of the year</p></li><li><p>China’s evolving AGI discussion among Alibaba leadership and the Politburo</p></li><li><p>The biggest swings in the US-China chip war</p></li><li><p>Beijing’s answer to America’s AI Action plan and the MFA’s </p></li><li><p>Robots</p></li></ul><h1>Models</h1><h3>The DeepSeek Moment</h3><p><em>Liang Wenfeng lit the fire</em></p><p>DeepSeek-R1 came out on January 20, thwarting everyone’s Chinese New Year plans. The cost-efficient LLM, which uses a Mixture-of-Experts (MoE) architecture, caused many in Silicon Valley to re-evaluate their bets on scaling — and on unfettered American dominance in frontier models. DeepSeek is powered by domestically trained Chinese engineering talent, an apparent belief in AGI, and no-strings-attached hedge fund money (it is owned by High-Flyer 幻方量化, a Hangzhou-based quantitative trading firm). There were initial concerns that such a recipe could not be replicated by more capital-constrained Chinese tech startups, but Kimi <a href="https://www.chinatalk.media/p/kimi-k2-the-open-source-way">proved that wrong</a> with K2 in July; <a href="https://podcasts.apple.com/us/podcast/the-z-ai-playbook/id1289062927?i=1000737778106">Z.ai</a>, Qwen, and MiniMax followed.</p><p>We translated Chinese tech media 36Kr’s <a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas">interview</a> with DeepSeek CEO Liang Wenfeng back in November 2024, and spent much of January 2025 on the DeepSeek beat (see Jordan’s conversations on DeepSeek with Miles Brundage <a href="https://podcasts.apple.com/us/podcast/emergency-pod-deepseek-r1-and-the-future-of/id1289062927?i=1000685366992">here</a> and with Kevin Xu of <em>Interconnected</em> <a href="https://podcasts.apple.com/us/podcast/deepseek-what-it-means-and-what-happens-next/id1289062927?i=1000687276601">here</a>). Over at the newsletter, we covered how China <a href="https://www.chinatalk.media/p/deepseek-the-view-from-china">reacted</a> to DeepSeek’s rise, its <a href="https://www.chinatalk.media/p/deepseeks-secret-to-success">secret sauce</a>, and <a href="https://www.chinatalk.media/p/deepseek-a-tragedy-foretold">concerns</a> around open-source as a strategy.</p><p>DeepSeek continues to be a big deal. For one, it paved the way for an open-source race dominated by Chinese models. Nearly every notable model released by Chinese companies in 2025 has been open source. In public blog posts, social media discussions, and private conversations, Chinese engineers and tech executives repeatedly attribute their open-source orientation to the example set by DeepSeek.</p><div class="apple-podcast-container" data-component-name="ApplePodcastToDom"><iframe class="apple-podcast " data-attrs="{&quot;url&quot;:&quot;https://embed.podcasts.apple.com/us/podcast/emergency-pod-deepseek-r1-and-the-future-of/id1289062927?i=1000685366992&quot;,&quot;isEpisode&quot;:true,&quot;imageUrl&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/podcast-episode_1000685366992.jpg&quot;,&quot;title&quot;:&quot;EMERGENCY POD: DeepSeek R1 and the Future of AI Competition with Miles Brundage&quot;,&quot;podcastTitle&quot;:&quot;ChinaTalk&quot;,&quot;podcastByline&quot;:&quot;&quot;,&quot;duration&quot;:1953000,&quot;numEpisodes&quot;:&quot;&quot;,&quot;targetUrl&quot;:&quot;https://podcasts.apple.com/us/podcast/emergency-pod-deepseek-r1-and-the-future-of/id1289062927?i=1000685366992&amp;uo=4&quot;,&quot;releaseDate&quot;:&quot;2025-01-24T22:20:00Z&quot;}" src="https://embed.podcasts.apple.com/us/podcast/emergency-pod-deepseek-r1-and-the-future-of/id1289062927?i=1000685366992" frameborder="0" allow="autoplay *; encrypted-media *;" allowfullscreen="true"></iframe></div><div class="apple-podcast-container" data-component-name="ApplePodcastToDom"><iframe class="apple-podcast " data-attrs="{&quot;url&quot;:&quot;https://embed.podcasts.apple.com/us/podcast/deepseek-what-it-means-and-what-happens-next/id1289062927?i=1000687276601&quot;,&quot;isEpisode&quot;:true,&quot;imageUrl&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/podcast-episode_1000687276601.jpg&quot;,&quot;title&quot;:&quot;DeepSeek: What It Means and What Happens Next&quot;,&quot;podcastTitle&quot;:&quot;ChinaTalk&quot;,&quot;podcastByline&quot;:&quot;&quot;,&quot;duration&quot;:4314000,&quot;numEpisodes&quot;:&quot;&quot;,&quot;targetUrl&quot;:&quot;https://podcasts.apple.com/us/podcast/deepseek-what-it-means-and-what-happens-next/id1289062927?i=1000687276601&amp;uo=4&quot;,&quot;releaseDate&quot;:&quot;2025-01-30T21:58:00Z&quot;}" src="https://embed.podcasts.apple.com/us/podcast/deepseek-what-it-means-and-what-happens-next/id1289062927?i=1000687276601" frameborder="0" allow="autoplay *; encrypted-media *;" allowfullscreen="true"></iframe></div><p>On the technical end, despite some <a href="https://www.theregister.com/2025/09/19/deepseek_cost_train/">remaining mystery</a> surrounding the exact cost of training R1, DeepSeek’s viability was a shot in the arm for Chinese labs working under compute constraints. Going into 2026, with restrictions on H200s <a href="https://www.reuters.com/world/china/us-open-up-exports-nvidia-h200-chips-china-semafor-reports-2025-12-08/">loosened</a> and reporting that DeepSeek is still training on smuggled Nvidia, easier access to TSMC-fabbed Nvidia chips may be just what DeepSeek needs ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.chinatalk.media/p/china-ai-in-2025-wrapped" class="read-button" target="_blank" rel="noopener">
          Read full article on ChinaTalk &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>