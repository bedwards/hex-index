<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How d-Matrix&#039;s In-Memory Compute Tackles AI Inference Economics - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>How d-Matrix&#039;s In-Memory Compute Tackles AI Inference Economics</h1>
        <div class="article-meta">
          <span class="author">By Vikram Sekar</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/viksnewsletter/index.html" class="publication">
            Vik&#039;s Newsletter
          </a>
          <span class="separator">&middot;</span><time>Dec 9, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">13 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/in-memory-processing/index.html">
          <strong>In-memory processing</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article&#039;s central focus is on d-Matrix&#039;s in-memory compute approach for AI inference. Understanding the broader technical foundations of in-memory computing - how it differs from traditional von Neumann architectures and why moving computation closer to data reduces latency - provides essential context for evaluating d-Matrix&#039;s claims.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/memristor/index.html">
          <strong>Memristor</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">The article mentions memristors as one approach to implementing analog in-memory compute weights. Memristors are a fascinating fourth fundamental circuit element theorized by Leon Chua in 1971 and first physically realized in 2008, with significant implications for neuromorphic computing that readers may not know deeply.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/ohms-law/index.html">
          <strong>Ohm&#039;s law</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article explains how analog in-memory compute leverages Ohm&#039;s law (I=GV) for multiplication operations. While readers may remember the basic formula, the deeper history of Georg Ohm&#039;s discovery, the physics behind electrical resistance, and its foundational role in electronics provides enriching context for understanding why this natural property enables efficient computation.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p><em>Each week, I help investors and professionals stay up-to-date on the semiconductor industry. If you’re new, <a href="https://www.viksnewsletter.com/p/new-start-here">start here</a>. See <a href="https://www.viksnewsletter.com/p/new-start-here?r=222kot&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false">here</a> for all the benefits of upgrading your subscription tier!</em></p><p><em>Paid subscribers will get have access to a video discussion of this essay, key highlights, and a google drive link to this article to parse with LLMs.</em></p><div><hr></div><p><em><strong>Disclaimer</strong>: This article is entirely my own opinion. I have not been paid by d-Matrix, nor do I have any access to internal documents. All information is publicly available (references cited). I do not hold any investment position in d-Matrix, and this is not investment advice. Do your own research. This article does not reflect the views of any past, present, or future employers, nor does it directly or indirectly imply any competitors are better or worse. This is my attempt at trying to understand how core technology works and where its advantages lie. I do not endorse any products.</em></p><p><em><strong>Disclosure</strong>: I requested that d-Matrix review the article to ensure that I do not misunderstand/misrepresent their technology. I’m grateful to them for pointing out errors in my conceptual understanding. All editorial decisions are entirely mine.</em></p><div><hr></div><p>Recently d-Matrix, a Bay Area AI inference chip startup, <a href="https://www.d-matrix.ai/announcements/d-matrix-raises-275-million-to-power-the-age-of-ai-inference/">announced its Series C funding of $275M</a> which brings its total funding up to $450M.</p><p>d-Matrix claims to have the “<strong>world’s highest performing, most efficient data center inference platform for hyperscale, enterprise, and sovereign customers</strong>,” and a “full-stack inference platform that combines breakthrough compute-memory integration, high-speed networking, and inference-optimized software to deliver <strong>10× faster performance, 3× lower cost, and 3–5× better energy efficiency than GPU-based systems</strong>.”</p><p>Their main compute engine is called Corsair, and is based on a different approach to inference called <strong>in-memory compute</strong>. In this post, we will look at this technology in detail, how it provides all those benefits, and where it is useful.</p><p><strong>For free subscribers:</strong></p><ul><li><p>Analog in-memory computing</p></li><li><p>d-Matrix’s digital in-memory compute solution</p></li><li><p>Four chiplets and LPDDR5</p></li><li><p>Scaling up to rack-level solutions</p></li><li><p>References</p></li></ul><p><strong>For paid subscribers:</strong></p><ul><li><p>A real-world use-case for d-Matrix DIMC hardware</p></li><li><p>Designing Hardware for Latency, Throughput, and TCO</p></li><li><p>The PCIe Advantage</p></li><li><p>Possible Uses of Small Inference Models running d-Matrix Hardware</p></li></ul><div><hr></div><h3>Analog In-memory Compute (AIMC)</h3><p>The process of AI training and inference involves a lot of matrix multiplications, followed by additions, which come from vector multiplications. If you need a deeper understanding of what those operations are ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.viksnewsletter.com/p/d-matrix-in-memory-compute" class="read-button" target="_blank" rel="noopener">
          Read full article on Vik&#039;s Newsletter &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>