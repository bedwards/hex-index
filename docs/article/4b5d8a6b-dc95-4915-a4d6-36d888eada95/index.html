<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>New LLM Pre-training and Post-training Paradigms - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>New LLM Pre-training and Post-training Paradigms</h1>
        <div class="article-meta">
          <span class="author">By Sebastian Raschka</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/sebastianraschka/index.html" class="publication">
            Ahead of AI
          </a>
          <span class="separator">&middot;</span><time>Aug 17, 2024</time>
          <span class="separator">&middot;</span>
          <span class="read-time">23 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>The development of large language models (LLMs) has come a long way, from the early GPT models to the sophisticated open-weight LLMs we have today. Initially, the LLM training process focused solely on pre-training, but it has since expanded to include both pre-training and post-training. Post-training typically encompasses supervised instruction fine-tuning and alignment, which was popularized by ChatGPT.</p><p>Training methodologies have evolved since ChatGPT was first released. In this article, I review the latest advancements in both pre-training and post-training methodologies, particularly those made in recent months.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!H0ge!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!H0ge!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 424w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 848w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 1272w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!H0ge!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png" width="1456" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!H0ge!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 424w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 848w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 1272w, https://substackcdn.com/image/fetch/$s_!H0ge!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4d4ee2d-c249-4b5f-8dae-a82cd648e990_1600x549.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption">An overview of the LLM development and training pipeline, with a focus on new pre-training and post-training methodologies discussed in this article</figcaption></figure></div><p>There are hundreds of LLM papers each month proposing new techniques and approaches. However, one of the best ways to see what actually works well in practice is to look at the pre-training and post-training pipelines of the most recent state-of-the-art models. Luckily, four major new LLMs have been released in the last months, accompanied by relatively detailed technical reports.</p><p>In this article, I focus on the pre-training and post-training pipelines of the following models:</p><ol><li><p>Alibaba's Qwen 2</p></li><li><p>Apple Intelligence Foundation Language Models</p></li><li><p>Google's Gemma 2</p></li><li><p>Meta AI's Llama 3.1</p></li></ol><p>These models are presented in order based on the publication dates of their respective technical papers on arXiv.org, which also happens to align with their alphabetical order.</p><p><strong>This article is a passion project that I created in my free time and over the weekends. If you find it valuable and would like to support my work, please consider purchasing a copy of my books and recommending them to your colleagues. Your review on Amazon would also be greatly appreciated!</strong></p><div class="captioned-image-container"><figure><a class="image-link image2" target="_blank" href="https://substackcdn.com/image/fetch/$s_!6HyS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6HyS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 424w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 848w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 1272w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6HyS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png" width="514" height="229.46428571428572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:650,&quot;width&quot;:1456,&quot;resizeWidth&quot;:514,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6HyS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 424w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 848w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 1272w, https://substackcdn.com/image/fetch/$s_!6HyS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14cc04f8-e484-4d81-8613-45ace975d38a_1600x714.png 1456w" sizes="100vw"></picture><div></div></div></a><figcaption class="image-caption"><a href="http://mng.bz/M96o">Build a Large Language Model (from Scratch)</a>, <a href="https://nostarch.com/machine-learning-and-ai-beyond-basics">Machine Learning Q and AI</a>, and <a href="https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/">Machine Learning with PyTorch and Scikit-Learn</a></figcaption></figure></div><ul><li><p><a href="http://mng.bz/M96o">Build a Large Language Model (from Scratch)</a> is a highly focused book dedicated to coding LLMs from the ground up in PyTorch, covering everything from pre-training to post-trainingâ€”arguably the best way to truly understand LLMs.</p></li><li><p><a href="https://nostarch.com/machine-learning-and-ai-beyond-basics">Machine Learning Q and AI</a> is a great book for those who are already familiar with the basics; it dives into intermediate and advanced concepts covering deep neural networks, vision transformers, multi-GPU training paradigms, LLMs, and many more.</p></li><li><p><a href="https://www.amazon.com/Machine-Learning-PyTorch-Scikit-Learn-scikit-learn-ebook-dp-B09NW48MR1/dp/B09NW48MR1/">Machine Learning with PyTorch and Scikit-Learn</a> is a comprehensive guide to machine learning, deep learning, and AI, offering a well-balanced mix of theory and practical code. It's the </p></li></ul></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training" class="read-button" target="_blank" rel="noopener">
          Read full article on Ahead of AI &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>