<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Nvidia&#039;s Climb to $4T and the Peaks Ahead - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Nvidia&#039;s Climb to $4T and the Peaks Ahead</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/chipstrat/index.html" class="publication">
            Chipstrat
          </a>
          <span class="separator">&middot;</span><time>Jul 22, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">13 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p>Nvidia is now worth $4 trillion. <em>Peak?</em> Nope. Just a checkpoint. This company has a habit of turning summits into basecamps.</p><p>To see why $4T is a milestone rather than the peak, we’ll retrace Nvidia’s rise from dominating the training era to powering the macro shift to inference. Then we’ll look ahead. Agentic AI, autonomy, and robotics are each shaping up to become a trillion-dollar market in their own right.</p><p>We’ll also explore why constrained supply has quietly been a strength and why Nvidia’s long-term growth will hinge on access to China, the world’s most strategically important market for physical AI. </p><p><em>If you’re tired of Nvidia being caught in geopolitical crossfire… its future is even more entangled with China.</em></p><h2>The First Trillion </h2><p>Nvidia’s first trillion-dollar milestone was the payoff of being <strong>the first and best platform for large-scale AI training</strong>. </p><p>Chips alone weren’t enough. Nvidia had to control the full system to make clusters scale. It already had the parallel compute (GPUs) and the software (CUDA). The missing piece was high-bandwidth interconnect.</p><p>That came in 2019 with the Mellanox acquisition. From the <a href="https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion">press release</a>:</p><blockquote><p>Datacenters in the future will be architected as giant compute engines with tens of thousands of compute nodes, designed holistically with their interconnects for optimal performance. </p></blockquote><p>That vision came true faster than expected. </p><p>As transformer-based LLMs took off, training needs exploded. Only two companies had the AI stack to meet that scale: Nvidia and <em>Google</em>. </p><p>Yes, Google. </p><p>Over the past decade, Google built its own accelerated computing platform, including high-bandwidth interconnects. As <a href="https://cloud.google.com/transform/ai-specialized-chips-tpu-history-gen-ai">Google Cloud’s blog</a> recounts,</p><blockquote><p>“In late 2014, when TPU v1 was being fabbed, we realized training capability was the limiting factor... So we built an interconnected machine with 256 TPU chips connected with a very high-bandwidth, custom interconnect to really get a lot of horsepower behind training models”</p></blockquote><p>But Google doesn’t sell silicon. TPUs were kept internal for years and only became available through Google Cloud in 2018. GCP remains a small slice (~10%) of Google’s ad-driven business. And while TPUs delivered strong performance on internal workloads, they lacked a robust external-facing software ecosystem. Combined with Google’s track record of shelving side projects, TPUs were never a serious option for the broader market. </p><p>Nvidia, by contrast, exists to sell silicon.</p><p><strong>Nvidia was the first, best, and only option for labs training at the frontier.</strong></p><p>OpenAI, Meta, xAI, and others all built ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.chipstrat.com/p/nvidias-climb-to-4t-and-the-peaks" class="read-button" target="_blank" rel="noopener">
          Read full article on Chipstrat &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>