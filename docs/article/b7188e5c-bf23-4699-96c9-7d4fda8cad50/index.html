<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How Grab Built a Vision LLM to Scan Images - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>How Grab Built a Vision LLM to Scan Images</h1>
        <div class="article-meta">
          <span class="author">By Alex Xu</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/bytebytego/index.html" class="publication">
            ByteByteGo Newsletter
          </a>
          <span class="separator">&middot;</span><time>Feb 3, 2026</time>
          <span class="separator">&middot;</span>
          <span class="read-time">12 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <h2><a href="https://bit.ly/Datadog_020326">Kubernetes Quick-Start Guide (Sponsored)</a></h2><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://bit.ly/Datadog_020326" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!yUVu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 424w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 848w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 1272w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!yUVu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png" width="1200" height="628" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:628,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:165481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://bit.ly/Datadog_020326&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.bytebytego.com/i/186361847?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!yUVu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 424w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 848w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 1272w, https://substackcdn.com/image/fetch/$s_!yUVu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F216cc96e-e6f9-4566-b7f8-222f9c8d73f1_1200x628.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>Cut through the noise with this engineer-friendly guide to Kubernetes observability. Save this reference for fast-track access to essential kubectl commands and critical metrics, from disk I/O and network latency to real-time cluster events. Perfect for scaling, debugging, and tuning your workloads without sifting through endless docs.</p><div><hr></div><p>Digital services require accurate extraction of information from user-submitted documents such as identification cards, driverâ€™s licenses, and vehicle registration certificates. This process is essential for electronic know-your-customer (eKYC) verification. However, the diversity of languages and document formats across the region makes this task particularly challenging.</p><p>Grab Engineering Team faced significant obstacles with traditional Optical Character Recognition (OCR) systems, which struggled to handle the variety of document templates. While powerful proprietary Large Language Models (LLMs) were available, they often failed to adequately understand Southeast Asian languages, produced errors and hallucinations, and suffered from high latency. Open-source Vision LLMs offered better efficiency but lacked the accuracy required for production deployment.</p><p>This situation prompted Grab to fine-tune existing models and eventually build a lightweight, specialized Vision LLM from the ground up. In this article, we will look at the complete architecture, the technical decisions made, and the results achieved.</p><div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!87Ba!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!87Ba!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 424w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 848w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 1272w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!87Ba!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png" width="1456" height="864" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:283043,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://blog.bytebytego.com/i/186361847?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!87Ba!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 424w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 848w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 1272w, https://substackcdn.com/image/fetch/$s_!87Ba!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad069df7-f987-451c-ba35-0e8487cdbd02_3208x1904.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a><figcaption class="image-caption"><strong>Source: </strong><a href="https://engineering.grab.com/custom-vision-llm-at-grab">Grab Engineering Blog</a></figcaption></figure></div><p><em>Disclaimer: This post is based on publicly shared details from the Grab Engineering Team. Please comment if you notice any inaccuracies.</em></p><h2>Understanding Vision LLMs</h2><p>Before diving into the solution, it helps to understand what a Vision LLM is and how it differs from traditional text-based language models.</p><p>A standard LLM processes text inputs and generates text outputs. A Vision LLM extends this capability by enabling the model to understand and process images. The architecture consists of three essential components working together:</p><ul><li><p>The first component is the image encoder. This module processes an image and converts it into a numerical format that computers can work with. Think of it as translating visual information into a structured representation of numbers and vectors.</p></li><li><p>The second component is the vision-language projector. This acts as a bridge between the image encoder and the language model. It transforms the numerical representation of the image into a format that the language model can interpret and use alongside text inputs.</p></li><li><p>The third component is the language model itself. This is the familiar text-processing model that takes both the transformed image information and any text instructions to generate a final text output. In the case of document processing, </p></li></ul></source>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://blog.bytebytego.com/p/how-grab-built-a-vision-llm-to-scan" class="read-button" target="_blank" rel="noopener">
          Read full article on ByteByteGo Newsletter &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>