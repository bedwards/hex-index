<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The New AI Consciousness Paper - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>The New AI Consciousness Paper</h1>
        <div class="article-meta">
          <span class="author">By Scott Alexander</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/astralcodexten/index.html" class="publication">
            Astral Codex Ten
          </a>
          <span class="separator">&middot;</span><time>Nov 19, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">19 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/aphantasia/index.html">
          <strong>Aphantasia</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (10 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/philosophical-zombie/index.html">
          <strong>Philosophical zombie</strong>
          <span class="read-time">16 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (18 min read)</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/integrated-information-theory/index.html">
          <strong>Integrated information theory</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">Linked in the article (15 min read)</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <h3><strong>I.</strong></h3><p>Most discourse on AI is low-quality. Most discourse on consciousness is super-abysmal-double-low quality. Multiply these - or maybe raise one to the exponent of the other, or something - and you get the quality of discourse on AI consciousness. It’s not great.</p><p>Out-of-the-box AIs mimic human text, and humans <a href="https://www.lesswrong.com/posts/Fy2b55mLtghd4fQpx/the-zombie-preacher-of-somerset">almost</a> always describe themselves as conscious. So if you ask an AI whether it is conscious, it will often say yes. But because companies know this will happen, and don’t want to give their customers existential crises, they hard-code in a command for the AIs to answer that they <em>aren’t</em> conscious. Any response the AIs give will be determined by these two conflicting biases, and therefore not really believable. <a href="https://arxiv.org/abs/2510.24797">A recent paper</a> expands on this method by subjecting AIs to a mechanistic interpretability <a href="https://www.astralcodexten.com/p/the-road-to-honest-ai">“lie detector” test</a>; it finds that AIs which say they’re conscious think they’re telling the truth, and AIs which say they’re not conscious think they’re lying. But it’s hard to be sure this isn’t just the copying-human-text thing. Can we do better? Unclear; the more common outcome for people who dip their toes in this space is to do <a href="https://x.com/kenklippenstein/status/1990200570112847923">much, much worse</a>.</p><p>But a rare bright spot has appeared: a seminal paper published earlier this month in <em>Trends In Cognitive Science</em>, <strong><a href="https://www.sciencedirect.com/science/article/pii/S1364661325002864">Identifying Indicators Of Consciousness In AI Systems</a></strong>. Authors include Turing-Award-winning AI researcher Yoshua Bengio, leading philosopher of consciousness David Chalmers, and even a few members of our conspiracy. If any AI consciousness research can rise to the level of merely awful, surely we will find it here.</p><p>One might divide theories of consciousness into three bins:</p><ul><li><p><em>Physical</em>: whether or not a system is conscious depends on its substance or structure. </p></li><li><p><em>Supernatural:</em> whether or not a system is conscious depends on something outside the realm of science, perhaps coming directly from God.</p></li><li><p><em>Computational: </em>whether or not a system is conscious depends on how it does cognitive work.</p></li></ul><p>The current paper announces it will restrict itself to computational theories. Why? Basically the <a href="https://en.wikipedia.org/wiki/Streetlight_effect">streetlight effect</a>: everything else ends up trivial or unresearchable. If consciousness depends on something about cells (what might this be?), then AI doesn’t have it. If consciousness comes from God, then God only knows whether AIs have it. But if consciousness depends on which algorithms get used to process data, then this team of top computer scientists might have valuable ...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.astralcodexten.com/p/the-new-ai-consciousness-paper" class="read-button" target="_blank" rel="noopener">
          Read full article on Astral Codex Ten &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>