<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Scaling RL and Self-Verifiable Reasoning: INTELLECT-3 and DeepSeekMath-V2 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Scaling RL and Self-Verifiable Reasoning: INTELLECT-3 and DeepSeekMath-V2</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/kaitchup/index.html" class="publication">
            The Kaitchup
          </a>
          <span class="separator">&middot;</span><time>Nov 28, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">8 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">The article discusses RLVR (Reinforcement Learning with Verifiable Rewards) and RL training pipelines extensively. Understanding RLHF provides crucial context for how modern AI models learn from feedback signals, which is the foundation these newer techniques build upon.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/mixture-of-experts/index.html">
          <strong>Mixture of experts</strong>
          <span class="read-time">12 min read</span>
        </a>
        <p class="topic-summary">INTELLECT-3 is described as a 106B-parameter MoE model with 12B active parameters. Understanding the Mixture of Experts architecture explains why models can have large total parameters but only use a fraction during inference, which is central to the efficiency claims in the article.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/automated-theorem-proving/index.html">
          <strong>Automated theorem proving</strong>
          <span class="read-time">14 min read</span>
        </a>
        <p class="topic-summary">DeepSeekMath-V2 is specifically designed for verifying mathematical proofs. The historical context of automated theorem proving—from early symbolic AI to modern neural approaches—provides valuable background for understanding why self-verifiable mathematical reasoning represents a significant advance.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <div class="captioned-image-container"><figure><a class="image-link image2 is-viewable-img" target="_blank" href="https://substackcdn.com/image/fetch/$s_!c7_f!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" data-component-name="Image2ToDOM"><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png" width="1456" height="815" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:815,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:245022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://kaitchup.substack.com/i/167992895?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/$s_!c7_f!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 424w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 848w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1272w, https://substackcdn.com/image/fetch/$s_!c7_f!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd3f4615-2c66-4a1a-8e7c-7002fe563e0b_1517x849.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><div class="pencraft pc-display-flex pc-gap-8 pc-reset"></div></div></div></a></figure></div><p>Hi Everyone,</p><p>In this edition of The Weekly Kaitchup, I discuss:</p><ul><li><p>INTELLECT-3: A Better GLM-4.5-Air </p></li><li><p>DeepSeekMath-V2: A New Math Model to Verify Mathematical Proof</p></li></ul><div><hr></div><p>I’ll be at NeurIPS in San Diego next week!<br>If you’d like me to attend specific talks or ask questions to certain authors, or if you have particular recommendations on what to see, let me know in the comments. I’ll mainly focus on work around quantization, PEFT, and evaluation, and I’ll share a full report with all the interesting things I learn.</p><p>Also, since San Diego is quite far from my little corner of the French countryside, I’ll probably skip my usual Monday article and publish it on Tuesday/Wednesday instead.</p><div><hr></div><h3>Black Friday Subscription Discount</h3><p>For Black Friday, I’m offering a <strong>30% discount</strong> on the yearly subscription to <em>The Kaitchup</em>:</p><p>With this subscription, you get instant access to all the AI notebooks (180+), articles, and tutorials (200+).</p><div><hr></div><h2>INTELLECT-3: A Better GLM-4.5-Air </h2><p>GLM models are very popular now as they perform well on most tasks. Among open-weight models, I prefer them over recent DeepSeek and Kimi models. </p><p>The GLM-4.5-Air is far smaller, and thus easier to run, than the GLM 4.6, but it is 4 months old! Thanks to prime intellect, we just got a very good update.</p><p>INTELLECT-3 is a 106B-parameter MoE model (12B active) trained <s>end-to-end</s> (not really end-to-end, but they market it like this) with RLVR on top of GLM-4.5-Air. </p><ul><li><p><a href="https://huggingface.co/PrimeIntellect/INTELLECT-3">PrimeIntellect/INTELLECT-3</a></p></li></ul><p>The work is primarily about infrastructure: it exposes a production-style RL stack for long-context, tool-using models, not just a checkpoint. </p><p>The stack covers asynchronous RL, standardized environments, and large-scale sandboxed code execution, all built around open-weight models. The result is a reproducible recipe for scaling RLVR to 512 H200 GPUs with long contexts and agentic behavior. No, this recipe is not for everyone. Using prime-intellect GPU pricing, that’s ~$1,300/hour. They mentioned they used these GPUs for 2 months, so that’s nearly a $1M model, if you want to do the same in the cloud.</p><h3>RLVR stack and system design</h3><p>The core engine is <a href="https://github.com/PrimeIntellect-ai/prime-rl">prime-rl</a>, their asynchronous off-policy RL framework that splits responsibilities across a CPU orchestrator, a trainer, and an inference pool. Training uses FSDP2-based data parallelism and torchtitan-style parallelism for MoE models. And inference uses a fleet of OpenAI-compatible vLLM servers extended to accept hot weight updates. The orchestrator is stateless and cheap: it streams rollouts from inference, forms batches, feeds </p>...</source>
      </div>

      <div class="read-full-article">
        <a href="https://kaitchup.substack.com/p/scaling-rl-and-self-verifiable-reasoning" class="read-button" target="_blank" rel="noopener">
          Read full article on The Kaitchup &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>