<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A safe harbor for AI evaluation and red teaming - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>A safe harbor for AI evaluation and red teaming</h1>
        <div class="article-meta">
          <span class="author">By Arvind Narayanan</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/aisnakeoil/index.html" class="publication">
            AI Snake Oil
          </a>
          <span class="separator">&middot;</span><time>Mar 5, 2024</time>
          <span class="separator">&middot;</span>
          <span class="read-time">9 min read</span>
        </div>
      </header>

      

      <div class="article-excerpt">
        <p><em>This blog post is authored by Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Arvind Narayanan, Percy Liang, and Peter Henderson. The paper has 23 authors and is available <a href="https://sites.mit.edu/ai-safe-harbor/files/2024/03/Safe-Harbor-0e192065dccf6d83.pdf">here</a>.</em></p><p>Today, we are releasing an open letter encouraging AI companies to provide legal and technical protections for <a href="https://krebsonsecurity.com/2022/06/what-counts-as-good-faith-security-research/">good-faith</a> research on their AI models. The letter focuses on the importance of independent evaluations of proprietary generative AI models, particularly those with millions of users. In an accompanying paper, we discuss existing challenges to independent research and how a more equitable, transparent, and accountable researcher ecosystem could be developed.</p><p>The letter has been signed by hundreds of researchers, practitioners, and advocates across disciplines, and is open for signatures.&nbsp;</p><p><strong>Read and sign the open letter <a href="https://sites.mit.edu/ai-safe-harbor/">here</a>. Read the paper <a href="https://sites.mit.edu/ai-safe-harbor/files/2024/03/Safe-Harbor-0e192065dccf6d83.pdf">here</a>.</strong> </p><h4><strong>Independent evaluation of AI is crucial for uncovering vulnerabilities</strong></h4><p><a href="https://openai.com/policies/sharing-publication-policy#research">AI</a> <a href="https://www.frontiermodelforum.org/">companies</a>, <a href="https://arxiv.org/pdf/2306.05949.pdf">academic</a> <a href="https://arxiv.org/pdf/2307.03718.pdf">researchers</a>, and <a href="https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/">civil</a> <a href="https://epic.org/wp-content/uploads/2023/05/EPIC-Generative-AI-White-Paper-May2023.pdf">society</a> agree that generative AI models pose acute risks: independent risk assessment is an essential mechanism for providing accountability. Nevertheless, barriers exist that inhibit the independent evaluation of many AI models.</p><p>Independent researchers often evaluate and “red team” AI models to measure a variety of different risks. In this work, we focus on post-release evaluation of models (or APIs) by external researchers beyond the model developer. This is also referred to as <a href="https://www.ajl.org/bugs">algorithmic audits</a> by <a href="https://sustainabilitydigitalage.org/featured/wp-content/uploads/missing-links-in-ai-governance-unesco-mila.pdf#page=13">third parties</a>. Some companies also conduct red teaming before their models are released both internally and with experts they select.&nbsp;</p><p>While many types of testing are critical, independent evaluation of AI models that are already deployed is <a href="https://arxiv.org/abs/2307.03718">widely</a> <a href="https://www.ajl.org/bugs">regarded</a> as essential for ensuring safety, security, and trust. Independent red-teaming research of AI models has uncovered vulnerabilities related to <a href="https://arxiv.org/abs/2310.02446">low resource languages</a>, <a href="https://llm-tuning-safety.github.io/">bypassing</a> <a href="https://arxiv.org/abs/2310.02949">safety measure</a>, and a <a href="https://arxiv.org/abs/2307.02483">wide</a> <a href="http://www.jailbreakchat.com">range</a> of <a href="https://twitter.com/AIPanic">jailbreaks</a>. These evaluations investigate a broad set of often unanticipated model flaws, related to <a href="https://openai.com/research/practices-for-governing-agentic-ai-systems">misuse</a>, <a href="https://arxiv.org/pdf/2303.11408.pdf">bias</a>, <a href="https://hbr.org/2023/04/generative-ai-has-an-intellectual-property-problem">copyright</a>, and other issues.</p><h4><strong>Terms of service can discourage community-led evaluations</strong></h4><p>Despite the need for independent evaluation, conducting research related to these vulnerabilities is often legally prohibited by the terms of service for popular AI models, including those of OpenAI, Google, Anthropic, Inflection, Meta, and Midjourney.</p><p>While these terms are intended as a deterrent against malicious actors, they also inadvertently restrict AI safety and trustworthiness research—companies forbid the research and may enforce their policies with account suspensions (as an example, see <a href="https://www.anthropic.com/legal/aup">Anthropic’s acceptable use </a>...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.normaltech.ai/p/a-safe-harbor-for-independent-ai" class="read-button" target="_blank" rel="noopener">
          Read full article on AI Snake Oil &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>