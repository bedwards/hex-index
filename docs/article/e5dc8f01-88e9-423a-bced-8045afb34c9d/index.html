<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Deep Learning Weekly: Issue 432 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="article-page">
      <header class="article-header">
        <h1>Deep Learning Weekly: Issue 432</h1>
        <div class="article-meta">
          <span class="author">By Various</span>
          <span class="separator">&middot;</span>
          <a href="../../publication/deeplearningweekly/index.html" class="publication">
            Deep Learning Weekly
          </a>
          <span class="separator">&middot;</span><time>Nov 26, 2025</time>
          <span class="separator">&middot;</span>
          <span class="read-time">5 min read</span>
        </div>
      </header>

      
      <section class="deep-dives">
        <h2>Deep Dives</h2>
        <p class="deep-dives-intro">Explore related topics with these Wikipedia articles, rewritten for enjoyable reading:</p>
        <ul class="deep-dive-list">
          
      <li class="deep-dive-item">
        <a href="../../wikipedia/reinforcement-learning-from-human-feedback/index.html">
          <strong>Reinforcement learning from human feedback</strong>
          <span class="read-time">13 min read</span>
        </a>
        <p class="topic-summary">Multiple papers in this issue discuss reinforcement learning strategies for improving AI model performance, including HunyuanOCR&#039;s use of RL for OCR tasks and the General Agentic Memory paper&#039;s mention of end-to-end optimization through RL. Understanding RLHF provides crucial context for how modern AI systems are trained and refined.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/optical-character-recognition/index.html">
          <strong>Optical character recognition</strong>
          <span class="read-time">11 min read</span>
        </a>
        <p class="topic-summary">The HunyuanOCR technical report is a featured paper discussing a vision-language model dedicated to OCR tasks. While readers may use OCR tools, the Wikipedia article covers the deep history, technical approaches, and evolution from template matching to neural networks that provides valuable context.</p>
      </li>

      <li class="deep-dive-item">
        <a href="../../wikipedia/transformer-deep-learning/index.html">
          <strong>Transformer (deep learning)</strong>
          <span class="read-time">15 min read</span>
        </a>
        <p class="topic-summary">The article references Vision Transformers (ViT) in HunyuanOCR and discusses various large language models. Understanding the foundational transformer architecture—attention mechanisms, encoder-decoder structures, and why it revolutionized NLP and computer vision—provides essential context for nearly every topic covered.</p>
      </li>
        </ul>
      </section>
    

      <div class="article-excerpt">
        <p>This week in deep learning, we bring you <a href="https://www.anthropic.com/news/claude-opus-4-5">Claude Opus 4.5</a>, <a href="https://huggingface.co/blog/continuous_batching">Continuous batching from first principles</a>, and <a href="https://arxiv.org/abs/2511.19575">a paper on HunyuanOCR Technical Report</a>.</p><p>You may also enjoy <a href="https://ai.meta.com/blog/sam-3d/">Introducing SAM 3D: Powerful 3D Reconstruction for Physical World Images</a>, <a href="https://www.anthropic.com/news/disrupting-AI-espionage">Disrupting the first reported AI-orchestrated cyber espionage campaign</a>, <a href="https://arxiv.org/abs/2511.18423">a paper on General Agentic Memory Via Deep Research</a>, and more!</p><p>As always, happy reading and hacking. If you have something you think should be in next week’s issue, find us on Twitter: <a href="https://twitter.com/dl_weekly">@dl_weekly</a>.</p><p>Until next week!</p><div><hr></div><h2><strong>Industry</strong></h2><p><strong><a href="https://www.anthropic.com/news/claude-opus-4-5">Introducing Claude Opus 4.5 \ Anthropic</a></strong></p><p>Anthropic released Claude Opus 4.5, the company’s most intelligent model yet with state-of-the-art performance in coding and agentic tasks.</p><p><strong><a href="https://ai.meta.com/blog/sam-3d/">Introducing SAM 3D: Powerful 3D Reconstruction for Physical World Images</a></strong></p><p>The Meta AI team announced SAM 3D, a release that includes SAM 3D Objects for scene reconstruction and SAM 3D Body for human body estimation.</p><p><strong><a href="https://openai.com/index/expanding-data-residency-access-to-business-customers-worldwide/">Expanding data residency access to business customers worldwide</a></strong></p><p>OpenAI announced that eligible customers using ChatGPT Enterprise, ChatGPT Edu, or the API Platform in many global markets can now choose local data residency.</p><p><strong><a href="https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/">Fara-7B: An Efficient Agentic Model for Computer Use</a></strong></p><p>The Microsoft AI team announced Fara-7B, their first agentic SLM designed specifically for computer use.</p><p><strong><a href="https://allenai.org/blog/olmo3">Olmo 3: Charting a path through the model flow to lead open-source AI</a></strong></p><p>Ai2 releases OLMo 3, a fully open language model family with complete training data and development pipeline transparency.</p><p><strong><a href="https://bfl.ai/blog/flux-2">FLUX.2: Frontier Visual Intelligence</a></strong></p><p>Black Forest Labs releases FLUX.2, a new image generation model with multi-reference support and 4MP resolution editing.</p><h2><strong>MLOps &amp; LLMOps</strong>.</h2><p><strong><a href="https://medium.com/google-cloud/antigravity-and-postgresql-no-gravity-only-vibes-46a7699fd21f">Antigravity and PostgreSQL: No gravity, only vibes | by MCP Toolbox for Databases</a></strong></p><p>An article about using Google Antigravity IDE and Gemini 3 with Model Context Protocol to streamline PostgreSQL database development through natural language interactions.</p><p><strong><a href="https://opensearch.org/blog/introducing-agentic-search-in-opensearch-transforming-data-interaction-through-natural-language/">Introducing agentic search in OpenSearch: Transforming data interaction through natural language</a></strong></p><p>An introduction to agentic search in OpenSearch 3.3, which uses LLM-powered agents and tools to transform data interaction.</p><h2><strong>Learning</strong></h2><p><strong><a href="https://www.anthropic.com/news/disrupting-AI-espionage">Disrupting the first reported AI-orchestrated cyber espionage campaign \ Anthropic</a></strong></p><p>A critical security report detailing the disruption of the first reported large-scale AI-orchestrated cyber espionage campaign, where a state-sponsored actor used Claude Code agents to execute 80-90% of the attack lifecycle.</p><p><strong><a href="https://medium.com/mongodb/reciprocal-rank-fusion-and-relative-score-fusion-classic-hybrid-search-techniques-3bf91008b81d">Reciprocal Rank Fusion and Relative Score Fusion: Classic Hybrid Search Techniques</a></strong></p><p>An article that delves into two classic hybrid search fusion techniques: Reciprocal Rank Fusion (RRF) and Relative Score Fusion (RSF).</p><p><strong><a href="https://huggingface.co/blog/continuous_batching">Continuous </a></strong>...</p>
      </div>

      <div class="read-full-article">
        <a href="https://www.deeplearningweekly.com/p/deep-learning-weekly-issue-432" class="read-button" target="_blank" rel="noopener">
          Read full article on Deep Learning Weekly &rarr;
        </a>
        <p class="copyright-note">
          This excerpt is provided for preview purposes.
          Full article content is available on the original publication.
        </p>
      </div>
    </article>
  
  </main>
</body>
</html>