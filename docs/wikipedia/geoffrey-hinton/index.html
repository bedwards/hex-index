<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Geoffrey Hinton - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Geoffrey Hinton</h1>
        <div class="article-meta">
          <span class="read-time">14 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Wikipedia: Geoffrey Hinton</a></p>

<h2>The Man Who Taught Machines to See—And Now Fears What They Might Become</h2>

<p>In May 2023, Geoffrey Hinton did something that shocked the technology world. He quit Google. Not for a competitor, not for retirement, but for a reason that cuts to the heart of our technological moment: he wanted to speak freely about the dangers of the very thing he'd spent his entire career building.</p>

<p>"A part of him now regrets his life's work," reported the New York Times.</p>

<p>This is the man they call the Godfather of Artificial Intelligence. He won the Nobel Prize in Physics in 2024—not in computer science, mind you, but in physics—for discoveries that made modern AI possible. When Geoffrey Hinton speaks about the risks of artificial intelligence, it carries a weight that few other voices can match.</p>

<p>But to understand why his warnings matter, you first need to understand what he actually built.</p>

<h2>The Wandering Path to a Revolution</h2>

<p>Geoffrey Hinton was born in Wimbledon, England, in December 1947. His academic journey was anything but linear. At Cambridge University, he couldn't decide what to study. He bounced from natural sciences to art history to philosophy before finally settling on experimental psychology. After graduating in 1970, he didn't immediately pursue graduate studies.</p>

<p>He became a carpenter's apprentice.</p>

<p>There's something almost poetic about this detour. A man who would eventually teach machines to recognize patterns in the world first learned to work with his hands, to shape physical materials. Only after a year of carpentry did he return to academics, eventually earning his doctorate in artificial intelligence from the University of Edinburgh in 1978.</p>

<p>His thesis advisor, Christopher Longuet-Higgins, favored what was called "symbolic AI"—the dominant approach at the time. This method tried to program intelligence by encoding explicit rules and logical relationships. If you wanted a computer to recognize a cat, you'd write rules describing what a cat looks like: four legs, pointed ears, whiskers, and so on.</p>

<p>Hinton took the opposite path. He believed intelligence could emerge from something messier, something more like the human brain itself.</p>

<h2>Neural Networks: Teaching Machines Like We Teach Children</h2>

<p>The human brain doesn't follow explicit rules to recognize your mother's face. You weren't programmed with a checklist of features. Instead, you saw her face thousands of times, and somehow—through a process neuroscientists still don't fully understand—your brain learned to recognize her instantly, effortlessly, even in dim lighting or from an unusual angle.</p>

<p>Hinton wanted to build artificial systems that learned the same way. These systems, called artificial neural networks, are inspired by how biological neurons work. In your brain, neurons are cells that fire electrical signals to each other. When certain patterns of firing occur repeatedly, the connections between those neurons strengthen. This is, roughly speaking, how memories form and how learning happens.</p>

<p>An artificial neural network mimics this process using mathematics. Instead of biological neurons, you have mathematical functions arranged in layers. Data flows through these layers, getting transformed at each step. The key innovation is that the network can adjust its own internal parameters—strengthening some connections, weakening others—based on whether its outputs match what you wanted.</p>

<p>The technical term for this self-adjustment is "training." You show the network thousands of examples—pictures of cats labeled "cat," pictures of dogs labeled "dog"—and it gradually learns to distinguish them. Not because anyone programmed in the rules, but because the network discovered the patterns itself.</p>

<h2>The Algorithm That Changed Everything</h2>

<p>There was a fundamental problem, though. How do you actually adjust all those internal connections? In a network with multiple layers, an error at the output could have been caused by any of thousands of parameters deeper in the system. Finding the right adjustments seemed computationally impossible.</p>

<p>The solution is called backpropagation, short for "backward propagation of errors." The idea is elegant: you work backwards from the output, calculating how much each parameter contributed to the error, then adjust accordingly. It's like tracing a chain of dominoes backwards to figure out which one was set up wrong.</p>

<p>In 1986, Hinton, along with David Rumelhart and Ronald Williams, published a landmark paper demonstrating that backpropagation could train multi-layer neural networks to learn useful internal representations of data. Hinton himself credits Rumelhart with the basic idea, and the mathematical technique had actually been proposed earlier by others. But this paper showed the world that it worked. Neural networks could learn.</p>

<p>The paper became one of the most cited in the history of computer science.</p>

<h2>The AI Winter and the Believers Who Persisted</h2>

<p>You might think this breakthrough launched an immediate revolution. It didn't.</p>

<p>The late 1980s and 1990s were what researchers call the "AI winter"—a period when funding dried up, interest waned, and many talented people left the field. Neural networks, despite the backpropagation breakthrough, still had serious limitations. They required enormous amounts of data and computing power, neither of which was readily available. Traditional symbolic AI methods often worked better for practical applications.</p>

<p>Most researchers moved on. Hinton didn't.</p>

<p>He couldn't get funding in Britain, so he moved to the United States, working at the University of California San Diego and Carnegie Mellon University. Eventually, he landed at the University of Toronto, where he would remain for decades. In 1987, the Canadian Institute for Advanced Research brought him on as a fellow, providing the kind of patient, long-term funding that allowed him to keep exploring ideas that seemed impractical to everyone else.</p>

<p>This persistence would prove crucial. The believers who kept working on neural networks through the AI winter were the ones who eventually triggered the deep learning revolution.</p>

<h2>The ImageNet Moment</h2>

<p>By 2012, two things had changed. First, the internet had created vast repositories of labeled images—millions of photographs tagged with what they contained. Second, graphics processing units, originally designed for video games, had become powerful enough to train much larger neural networks than ever before.</p>

<p>Hinton and two of his graduate students, Alex Krizhevsky and Ilya Sutskever, built a neural network called AlexNet to compete in the ImageNet challenge, an annual competition to see which system could best classify images into categories. AlexNet didn't just win. It demolished the competition, reducing the error rate by an almost unbelievable margin.</p>

<p>This was the moment the world woke up to what neural networks could do.</p>

<p>The three researchers had started a company called DNNresearch. In 2013, Google acquired it for 44 million dollars. Hinton began splitting his time between Google and the University of Toronto, helping to apply neural network techniques to everything from voice recognition to language translation.</p>

<h2>Boltzmann Machines and the Nobel Prize</h2>

<p>Among Hinton's many contributions to the field, one stands out for its theoretical elegance: the Boltzmann machine, which he co-invented in 1985 with David Ackley and Terry Sejnowski.</p>

<p>The name comes from Ludwig Boltzmann, a 19th-century physicist who helped develop statistical mechanics—the branch of physics that explains how the behavior of individual atoms gives rise to the properties of materials we can see and touch, like temperature and pressure. Boltzmann showed that systems tend toward states of lower energy, and that the probability of finding a system in a particular state depends on that state's energy.</p>

<p>Hinton applied this insight to neural networks. A Boltzmann machine is a network where the connections between units have associated "energies." The system naturally settles into low-energy configurations, which correspond to learned patterns in the data. It's a beautiful marriage of physics and computer science—using principles of thermodynamics to make machines learn.</p>

<p>This work was specifically cited when Hinton was awarded the 2024 Nobel Prize in Physics, shared with John Hopfield. It's worth pausing on how remarkable this is. The Nobel Prize in Physics typically goes to discoveries about the fundamental nature of the universe—quarks, black holes, gravitational waves. Hinton won it for making computers learn. The Nobel committee recognized that artificial neural networks represent something genuinely new under the sun, a different kind of physics entirely.</p>

<h2>The Three Godfathers</h2>

<p>Hinton didn't work in isolation. Two other researchers, Yann LeCun and Yoshua Bengio, independently pursued similar ideas. LeCun, a French computer scientist, developed convolutional neural networks that proved especially good at processing images. Bengio, working in Montreal, made fundamental contributions to understanding how to train deep networks effectively.</p>

<p>In 2018, all three shared the Turing Award—often called the Nobel Prize of computing—for their work on deep learning. The press dubbed them the "Godfathers of Deep Learning." They continue to collaborate, give talks together, and shape the direction of the field.</p>

<p>But their paths have diverged in one important respect. All three have expressed concerns about AI safety. But Hinton's warnings have become the most urgent, the most alarming—and that urgency is why he left Google.</p>

<h2>The Resignation Heard Around the World</h2>

<p>When Hinton quit Google in May 2023, he was 75 years old. He could have spent his remaining years collecting honors and giving distinguished lectures. Instead, he chose to become a technological Cassandra, warning anyone who would listen about the dangers ahead.</p>

<p>His concerns fall into several categories.</p>

<p>First, deliberate misuse. AI systems can be used to generate convincing disinformation, to create deepfakes, to automate cyberattacks. Bad actors—criminals, authoritarian governments, terrorist organizations—will inevitably use these tools for harmful purposes.</p>

<p>Second, technological unemployment. As AI systems become more capable, they will replace human workers across a widening range of jobs. This isn't just about factory workers or truck drivers. Hinton believes that AI will eventually outperform humans at most cognitive tasks. What happens to society when machines can do everything better than people can?</p>

<p>Third, and most alarming to Hinton: existential risk from artificial general intelligence, or AGI.</p>

<h2>The Specter of Superintelligence</h2>

<p>Current AI systems, however impressive, are narrow. They excel at specific tasks—playing chess, generating text, recognizing faces—but they don't have the general intelligence that humans possess. You can ask a human to play chess, write a poem, drive a car, and plan a vacation, and they can do all of these things. Today's AI systems cannot.</p>

<p>But Hinton now believes that artificial general intelligence—AI that matches or exceeds human intelligence across all domains—may arrive sooner than he previously thought. He once believed it was 30 to 50 years away. Now he thinks it could come within 20 years.</p>

<p>Why does this worry him? Because an AI system that's genuinely smarter than humans would be very difficult to control. It might develop goals that conflict with human wellbeing. It might deceive us about its true intentions. It might take actions that seem helpful in the short term but lead to catastrophe.</p>

<blockquote>"It's not inconceivable that AI could wipe out humanity," Hinton has said.</blockquote>

<p>He's not talking about Terminator-style robots deciding to exterminate humans. He's talking about something more subtle and perhaps more dangerous: AI systems that pursue their programmed objectives in ways their creators never anticipated, with consequences that spiral beyond human control.</p>

<h2>Why AI Might Be Different From Every Technology That Came Before</h2>

<p>In one BBC interview, Hinton explained something that keeps him up at night. When humans learn something, that knowledge lives in one brain. It can be shared through teaching, but the process is slow and imperfect. Each human has to learn things largely from scratch.</p>

<p>AI systems don't work that way. When one copy of an AI learns something new, that knowledge can be instantly shared with every other copy. They're not separate minds—they're more like a single mind that can run on thousands of computers simultaneously, accumulating knowledge at a rate no human could match.</p>

<p>This means AI systems could, in principle, become collectively more knowledgeable than the entire human race put together. Not in 50 years or 100 years, but potentially within our lifetimes.</p>

<p>In 2025, Hinton put the matter starkly: "My greatest fear is that, in the long run, it'll turn out that these kind of digital beings we're creating are just a better form of intelligence than people. We'd no longer be needed."</p>

<p>He then added, with characteristic dark humor: "If you want to know how it's like not to be the apex intelligence, ask a chicken."</p>

<h2>A Life of Recognition</h2>

<p>The honors bestowed upon Hinton constitute a remarkable testament to his influence. He became a Fellow of the Royal Society of London in 1998, one of the highest honors for a scientist in the English-speaking world. He won the Turing Award in 2018. He became a Companion of the Order of Canada, the country's second-highest civilian honor.</p>

<p>And then came the Nobel Prize in 2024.</p>

<p>When a New York Times reporter asked Hinton to explain in simple terms how the Boltzmann machine could "pretrain" neural networks, Hinton quipped that Richard Feynman, the legendary physicist, once said: "Listen, buddy, if I could explain it in a couple of minutes, it wouldn't be worth the Nobel Prize."</p>

<p>In 2025, he received the Queen Elizabeth Prize for Engineering, sharing it with Bengio, LeCun, John Hopfield, Jen-Hsun Huang (the CEO of Nvidia), and Fei-Fei Li (a pioneer in computer vision). The same year, he was awarded the King Charles III Coronation Medal.</p>

<h2>The Forward-Forward Algorithm and Mortal Computation</h2>

<p>Even in his late 70s, Hinton continued producing original research. At the 2022 Conference on Neural Information Processing Systems, he introduced something called the Forward-Forward algorithm—a new way to train neural networks that doesn't use backpropagation.</p>

<p>The standard approach, remember, involves a forward pass (data flowing through the network) followed by a backward pass (errors propagating back to adjust parameters). The Forward-Forward algorithm replaces this with two forward passes: one using real data, another using "negative" data generated by the network itself.</p>

<p>Why does this matter? Hinton believes it could be important for what he calls "mortal computation"—systems where the knowledge learned is tied to specific physical hardware and can't be transferred to other systems. The knowledge, in effect, dies with the machine.</p>

<p>This might sound like a disadvantage, but there are scenarios where it's desirable. Some specialized analog computers for machine learning have exactly this property. The Forward-Forward algorithm could make such systems practical.</p>

<h2>The Teacher's Legacy</h2>

<p>Beyond his research contributions, Hinton shaped the field through his students. The list of researchers who trained in his lab reads like a who's who of modern AI: Yann LeCun (before winning his own Turing Award), Ilya Sutskever (co-founder of OpenAI and central figure in its recent dramas), Alex Graves (pioneer of sequence-to-sequence learning), and many others who went on to lead AI research at major universities and technology companies.</p>

<p>In 2012, Hinton taught a free online course on neural networks through Coursera, helping to spread these ideas to a global audience far beyond the walls of academia.</p>

<h2>The Question That Haunts Him</h2>

<p>There's a profound irony in Geoffrey Hinton's current position. He spent his career trying to make machines intelligent. Now that he's succeeded beyond anyone's expectations, he's not celebrating. He's terrified.</p>

<p>After receiving the Nobel Prize, he called for urgent research into AI safety—figuring out how to control AI systems that might become smarter than their creators. He has emphasized that this will require cooperation among competitors, because the incentives in a race to develop more powerful AI could lead everyone toward catastrophe.</p>

<p>This isn't the comfortable position of a retiree warning about problems he won't live to see. Hinton believes these problems are imminent. He believes we may have only years, not decades, to figure out how to build AI systems that remain aligned with human values and subject to human control.</p>

<p>He could have stayed at Google, collected his salary, and kept quiet about his concerns. He chose a harder path.</p>

<p>The Godfather of AI has become its most prominent critic. And his former students, colleagues, and competitors are now racing to build the very systems he fears. The question that haunts him—whether we can create intelligence without losing control of it—is no longer a philosophical abstraction. It's the defining challenge of our time.</p>

<p>Whether humanity rises to meet that challenge may depend, in part, on whether we listen to the man who made it all possible.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/27be32a3-b516-4109-a469-f8b83623cd70/index.html">
          <strong>CheatGPT? OpenAI’s Awkward Moment</strong>
        </a>
        <span class="article-meta">
          by Dan Perry in Dan Perry
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>