<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Conference on Neural Information Processing Systems - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Conference on Neural Information Processing Systems</h1>
        <div class="article-meta">
          <span class="read-time">11 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems">Wikipedia: Conference on Neural Information Processing Systems</a></p>

<h2>Where the Future Gets Announced</h2>

<p>Every December, thousands of researchers descend on a single city to share their latest work in artificial intelligence. The presentations at this gathering have included the foundational papers behind technologies you now use every day—from the speech recognition in your phone to the image search in your photo library. This is NeurIPS, the Conference on Neural Information Processing Systems, and understanding its history helps explain why artificial intelligence research looks the way it does today.</p>

<p>But here's what makes NeurIPS particularly fascinating: it started as something quite different from what it became.</p>

<h2>Born at a Ski Resort</h2>

<p>The story begins in 1986, not in a university lecture hall or a corporate research lab, but at an invitation-only gathering at Snowbird, a ski resort in Utah. The California Institute of Technology and Bell Laboratories—then one of the world's premier industrial research labs—had been hosting annual meetings on neural networks for computing. These were small affairs, exclusive by design.</p>

<p>But some researchers wanted something more open, more interdisciplinary. They proposed a complementary conference that would bring together people studying biological nervous systems and people building artificial ones. The idea was revolutionary for its time: what if understanding how the brain works could help us build better computers, and what if building artificial neural networks could help us understand the brain?</p>

<p>The first NeurIPS conference happened in 1987. Ed Posner, an information theorist, served as president, while Yaser Abu-Mostafa, a learning theorist, ran the program. This pairing wasn't accidental—it reflected the conference's founding vision of bridging different fields.</p>

<h2>Two Streams Diverge</h2>

<p>Those early NeurIPS meetings covered remarkable intellectual territory. Some researchers presented work on pure engineering problems—how to get machines to recognize patterns, make decisions, or learn from examples. Others used computer models as tools for understanding actual biological nervous systems, treating artificial networks as simplified laboratories for exploring how real neurons might work.</p>

<p>Over time, these two streams drifted apart. The biological researchers increasingly went their own way, developing their own conferences and journals. Meanwhile, NeurIPS became dominated by machine learning, artificial intelligence, and statistics. The word "Neural" in the name became something of a historical curiosity, a fossil from the conference's origins.</p>

<p>And then something unexpected happened.</p>

<h2>The Deep Learning Revolution</h2>

<p>Starting around 2012, a particular approach to building artificial neural networks—one involving many layers of processing, hence "deep" learning—began achieving breakthrough results. Faster computers and vastly larger datasets made it possible to train networks of a size and complexity that earlier researchers could only dream about.</p>

<p>Suddenly, that word "Neural" in the conference name was relevant again.</p>

<p>The achievements announced at NeurIPS in the years that followed read like a checklist of technologies that would reshape entire industries. Speech recognition systems that could finally understand natural conversation. Computer vision systems that could identify objects in photographs with superhuman accuracy. Systems that could generate captions describing what was happening in images. Translation systems that made it possible to read websites in languages you'd never studied.</p>

<p>Perhaps most dramatically, systems that could play the ancient board game Go at world championship level—something that experts had predicted was decades away.</p>

<p>These breakthroughs shared a common thread: they drew inspiration from biology, but in unexpected ways. Convolutional neural networks, the architecture behind most image recognition, were inspired by the hierarchy of visual processing areas in the brain's cortex. Reinforcement learning techniques, which enabled the Go-playing systems, borrowed ideas from how the basal ganglia—a cluster of neurons deep in the brain—seems to learn from rewards and punishments.</p>

<h2>The Awkward Acronym</h2>

<p>For most of its history, the conference was abbreviated as NIPS. This worked fine until the late 2010s, when some attendees and commentators pointed out that the abbreviation had unfortunate associations—both as slang relating to anatomy and as a slur against people of Japanese descent.</p>

<p>In November 2018, the conference board made the decision to change the abbreviation to NeurIPS. It was a small change in some ways—just three extra letters—but it reflected a growing awareness in the field about who felt welcome at these gatherings and who didn't.</p>

<h2>More Than Machine Learning</h2>

<p>While machine learning dominates the conference today, NeurIPS has always drawn researchers from neighboring fields. Cognitive scientists present work on how humans think and reason. Psychologists share insights about learning and perception. Linguists working on statistical approaches to language find a receptive audience. Information theorists—following in the tradition of founding president Ed Posner—continue to contribute fundamental insights about the nature of learning and communication.</p>

<p>Computer vision researchers have made NeurIPS a key venue for their work, which makes sense given how central visual processing has been to the deep learning revolution. And neuroscientists, while less central than in the early days, still attend—sometimes to share biological insights that might inspire new algorithms, sometimes to use machine learning as a tool for analyzing their own data.</p>

<h2>The Traveling Conference</h2>

<p>NeurIPS has been something of a nomad among academic conferences. For its first thirteen years, from 1987 through 2000, it called Denver home. Then it moved north to Vancouver, where it stayed for a decade. Granada, Spain hosted in 2011—a rare European appearance. Lake Tahoe, straddling the California-Nevada border, welcomed attendees in 2012 and 2013.</p>

<p>Montreal and Barcelona took turns in the mid-2010s. Long Beach, California hosted in 2017. Then back to Montreal, back to Vancouver. The pandemic years of 2020 and 2021 forced the conference online, like so many others. New Orleans hosted in 2022 and 2023. Vancouver again in 2024.</p>

<p>Future locations reveal the conference's growing international ambition: San Diego and Mexico City will jointly host in 2025, and Warsaw, Poland is scheduled for 2026—a sign that European researchers, long underrepresented at a historically North American conference, are becoming more central to the field.</p>

<p>One charming tradition lasted until 2013: the workshops that followed the main conference were held at nearby ski resorts, a callback to the Snowbird origins. Eventually, the conference simply grew too large for any ski resort to accommodate.</p>

<h2>How Big Is It?</h2>

<p>To understand NeurIPS's significance, consider that it's one of only three conferences considered the top tier for machine learning research. The other two are the International Conference on Machine Learning, known as ICML, and the International Conference on Learning Representations, called ICLR. Getting a paper accepted at any of these three is a major career milestone for researchers.</p>

<p>The conference now operates on a double-track format, meaning two sessions of talks run simultaneously. Until 2015, it was single-track—everyone attended the same talks. The switch to double-track was a concession to growth; there was simply too much good work to fit into a single sequence of presentations.</p>

<h2>The Randomness Experiment</h2>

<p>In 2014, the conference's program chairs ran a fascinating experiment on their own reviewing process. They took ten percent of all submitted papers and secretly sent them through two completely independent groups of reviewers. This meant the same paper would be evaluated twice, by different people, with neither group knowing the other existed.</p>

<p>The results were sobering. A significant number of papers were accepted by one group of reviewers and rejected by the other. Various researchers analyzed the data and reached a striking conclusion: the acceptance decisions were closer to random chance than most people in the field wanted to admit.</p>

<p>John Langford, a machine learning researcher, put it memorably. A purely random decision—a coin flip—would show about 78 percent arbitrariness. The actual NeurIPS process showed about 60 percent arbitrariness. That's better than pure chance, certainly. But it's much closer to random than to a deterministic, reliable system.</p>

<p>This finding sparked ongoing debates about how to improve peer review in machine learning, debates that continue today. Some researchers argue for more reviewers per paper. Others advocate for open review, where authors and reviewers can see each other's identities. Still others have questioned whether the conference model itself makes sense for a field moving as quickly as machine learning.</p>

<h2>Honoring the Founders</h2>

<p>The conference has established two named lectureships to recognize distinguished researchers. The Posner Lecture honors Ed Posner, the information theorist who served as the first conference president and founded the NeurIPS Foundation. Posner died in 1993, but the foundation he created continues to organize the conference, with Terrence Sejnowski serving as president ever since.</p>

<p>The list of Posner Lecturers reads like a who's who of machine learning and artificial intelligence. Yann LeCun, who would later win the Turing Award for his work on deep learning. Yoshua Bengio, who shared that Turing Award. Reinforcement learning pioneer Rich Sutton. Probabilistic reasoning expert Daphne Koller. The physicist John Hopfield, whose work on associative memory helped launch the neural network revival of the 1980s.</p>

<p>The Breiman Lecture, introduced in 2015, honors Leo Breiman, a statistician who served on the NeurIPS board for over a decade. Breiman was famous for developing random forests, a machine learning technique that remains widely used today. He was also known for a provocative essay arguing that statisticians focused too much on interpretable models and not enough on predictive accuracy—a position that anticipated many later debates about machine learning.</p>

<h2>Building Community</h2>

<p>As the field has grown, various communities within it have organized to support members from underrepresented groups. Black in AI formed in 2017, creating a space for Black researchers in artificial intelligence to connect, collaborate, and support each other's careers. Queer in AI started in 2016 with similar goals for LGBTQ+ researchers.</p>

<p>These groups, known as affinity groups, have become an important part of the NeurIPS ecosystem. They host workshops, organize mentoring programs, and work to make the field more welcoming to people who might otherwise feel isolated in a predominantly white, predominantly male, predominantly straight research community.</p>

<h2>The Papers That Matter</h2>

<p>The conference proceedings have been published continuously since 1987, creating an invaluable archive of how the field has evolved. The first proceedings, published by the American Institute of Physics, were simply titled "Neural Information Processing Systems." Later volumes, published first by Morgan Kaufmann, then by MIT Press, and currently by Curran Associates, go by "Advances in Neural Information Processing Systems."</p>

<p>Looking through these proceedings is like watching artificial intelligence grow up. Early papers grapple with fundamental questions about how machines might learn at all. Papers from the 1990s and 2000s develop the statistical foundations that would later prove essential. And papers from the deep learning era announce breakthrough after breakthrough, each building on what came before.</p>

<p>For researchers in the field, having a paper in NeurIPS proceedings is a mark of quality. For historians of technology, those proceedings are a primary source for understanding one of the most consequential scientific developments of our time.</p>

<h2>Why It Matters</h2>

<p>If you've used voice commands on your phone, gotten recommendations from a streaming service, or watched a car navigate using computer vision, you've benefited from research first presented at NeurIPS. The conference has been central to the development of deep learning, reinforcement learning, generative models, and countless other techniques that now power applications used by billions of people.</p>

<p>But NeurIPS is more than a venue for announcing results. It's where researchers meet collaborators, where graduate students find mentors, where ideas cross-pollinate between subfields. The hallway conversations at NeurIPS have launched entire research programs. The workshops have incubated new subfields that later grew into conferences of their own.</p>

<p>Understanding NeurIPS helps explain why artificial intelligence research looks the way it does. The conference's interdisciplinary origins—bridging biology and engineering, theory and practice—established a culture that values both mathematical rigor and practical impact. Its embrace of neural networks, even during the long years when other approaches dominated, positioned it perfectly to lead the deep learning revolution.</p>

<p>And its ongoing evolution—the name change, the affinity groups, the geographic expansion—reflects a field grappling with its own success, trying to become more inclusive and more global even as it grows faster than anyone anticipated.</p>

<p>Every December, the artificial intelligence research community gathers to share what they've learned. What they present there will shape technology for years to come. NeurIPS, born at a ski resort in Utah, has become one of the most important scientific conferences in the world.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/e1fc54a4-da9f-402d-97ca-e598b598b68a/index.html">
          <strong>Mistral Large 3: Not a Reasoning Model</strong>
        </a>
        <span class="article-meta">
          by Various in The Kaitchup
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>