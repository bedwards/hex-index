<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Replication crisis - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Replication crisis</h1>
        <div class="article-meta">
          <span class="read-time">10 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Replication_crisis">Wikipedia: Replication crisis</a></p>

<p>Here's an uncomfortable truth about science: a disturbingly large number of published findings might be wrong. Not wrong in the sense of being slightly off, or needing minor corrections. Wrong in the sense that when other scientists try to repeat the original experiments, they get completely different results.</p>

<p>This is the replication crisis, and it's been shaking the foundations of scientific research since the early 2010s.</p>

<h2>Why Replication Matters</h2>

<p>The entire edifice of scientific knowledge rests on a simple promise: if you follow the same procedures, you should get the same results. It doesn't matter if you're in Tokyo or Toronto, whether you're a graduate student or a Nobel laureate. The laws of nature don't play favorites. An experiment that works should work anywhere, for anyone.</p>

<p>This is what separates science from anecdote, from superstition, from wishful thinking. As environmental health scientist Stefan Schmidt put it, replication is "the proof that the experiment reflects knowledge that can be separated from the specific circumstances under which it was gained."</p>

<p>When replication fails systematically, that promise starts to crumble.</p>

<h2>The Scope of the Problem</h2>

<p>Psychology and medicine have been ground zero for replication efforts, though not because these fields are uniquely troubled. They simply attracted the most scrutiny first. Researchers have methodically gone back to classic studies—the kind that get cited in textbooks and referenced in popular science articles—and tried to reproduce them from scratch.</p>

<p>The results have been sobering. Study after study has failed to hold up under examination. And while psychology and medicine have received the most attention, evidence suggests the problem extends throughout the natural and social sciences. Chemistry, biology, economics, political science—none appear immune.</p>

<p>This doesn't mean these fields lack rigor. Quite the opposite. The crisis represents science doing exactly what it's supposed to do: self-correcting. The problem is that this correction mechanism has historically been slow, inconsistent, and often ignored.</p>

<h2>Understanding Replication</h2>

<p>Before diving deeper, it's worth clarifying what we mean by replication, because there are actually several kinds.</p>

<p><em>Direct replication</em> means repeating the original procedures as closely as possible. Same equipment, same methods, same everything—just different researchers and different subjects. If you claimed that playing Mozart to plants makes them grow faster, a direct replication would have me set up the exact same experiment in my lab.</p>

<p><em>Systematic replication</em> introduces intentional changes. Maybe I use Beethoven instead of Mozart, or roses instead of ferns. This helps identify which elements of the original finding are essential and which are incidental.</p>

<p><em>Conceptual replication</em> tests the underlying hypothesis using entirely different methods. If your theory is that plants respond to music, I might measure their growth response to vibrations at specific frequencies, removing the confounding variable of the particular musical composition.</p>

<p>There's also a related but distinct concept: <em>reproducibility</em>. This refers to taking the original data and rerunning the analysis to verify the results. You're not collecting new data—you're checking whether the math was done correctly. This is why many researchers now make their raw data publicly available.</p>

<h2>The Statistical Machinery</h2>

<p>To understand how the crisis emerged, you need to understand how scientists decide whether their results are meaningful.</p>

<p>Most research uses what's called null hypothesis testing. The null hypothesis is typically a statement of "no effect"—for example, "this drug doesn't affect recovery rates from the disease." The alternative hypothesis is that there is an effect.</p>

<p>Researchers collect data and then calculate the probability of observing their results if the null hypothesis were true. This probability is the infamous p-value. If you found that patients taking the drug recovered 20% faster, you'd ask: what are the odds of seeing a 20% improvement by pure chance, if the drug actually does nothing?</p>

<p>The conventional threshold is p < 0.05, meaning there's less than a 5% chance of seeing results this extreme by random chance alone. When your p-value falls below this threshold, you declare the results "statistically significant" and reject the null hypothesis.</p>

<p>This seems reasonable enough. A 5% false positive rate sounds pretty good.</p>

<p>But there's a catch.</p>

<h2>The Problem with 5%</h2>

<p>That 5% false positive rate assumes perfect conditions. It assumes researchers are testing genuine hypotheses, running their analyses correctly, and reporting all their results honestly.</p>

<p>In practice, the real false positive rate is much higher.</p>

<p>Consider what happens when a researcher has flexibility in their analysis. Maybe they can measure the outcome at week four or week eight. Maybe they can include or exclude certain subjects based on various criteria. Maybe they can control for different combinations of confounding variables. Each of these choices gives them another path to statistical significance.</p>

<p>This flexibility—sometimes innocent, sometimes not—dramatically inflates the false positive rate. What started as a 5% chance of error can balloon to 50% or higher.</p>

<p>And that's before we even consider publication bias: the tendency for journals to publish positive findings while relegating null results to file drawers. If twenty labs test the same ineffective drug, one will find significant results by chance. That one study gets published. The nineteen failures disappear.</p>

<h2>Effect Sizes and Their Discontents</h2>

<p>Beyond the binary question of "significant or not," scientists also care about effect sizes—how large the observed effect actually is. A drug that improves recovery by 0.1% might be statistically significant with a large enough sample, but it's clinically meaningless.</p>

<p>Effect sizes get defined differently depending on the field and the type of data. One common measure, Cohen's d, essentially asks: how many standard deviations apart are the two groups being compared?</p>

<p>But here's where things get subtle. Effect sizes can't be directly observed. They must be estimated from data using statistical formulas. Different formulas have different properties—some are more efficient, some are less biased, some have smaller variance. This means researchers have choices to make, and those choices can influence results.</p>

<p>More troublingly, an effect size of zero—suggesting no relationship between variables—doesn't guarantee true independence. Two variables might have a complex, non-linear relationship that averages out to zero when measured crudely. Or they might affect different subgroups in opposite directions, canceling each other out in aggregate.</p>

<h2>The Language of Uncertainty</h2>

<p>Let's pause to clarify some terminology that often causes confusion.</p>

<p>A <em>false positive</em>, also called a Type I error, occurs when you conclude there's an effect when there actually isn't one. You reject the null hypothesis incorrectly. The significance level (alpha, typically 0.05) is the probability you're willing to accept for this kind of error.</p>

<p>A <em>false negative</em>, or Type II error, is the opposite: concluding there's no effect when there actually is one. You fail to reject a false null hypothesis. The probability of avoiding this error is called statistical power.</p>

<p>These two error rates trade off against each other. If you want to be very certain you're not finding effects that don't exist (low alpha), you need more evidence to declare significance, which means you'll miss more real effects (lower power). If you want to catch every real effect (high power), you'll also catch more spurious ones (higher alpha).</p>

<p>This tradeoff can be managed by collecting more data, but larger samples cost more time and money. In practice, many studies are underpowered—they have only a coin-flip chance of detecting effects that actually exist.</p>

<h2>The P-Value Distribution</h2>

<p>Here's a fascinating mathematical fact that underlies much of modern statistics: if the null hypothesis is true, p-values are uniformly distributed between zero and one. Every value is equally likely.</p>

<p>Think about what this means. If a drug truly does nothing, about 5% of studies will still find p < 0.05 by chance. About 1% will find p < 0.01. About 0.1% will find p < 0.001.</p>

<p>When the alternative hypothesis is true—when there really is an effect—the p-value distribution shifts. It becomes peaked near zero, with most studies finding very small p-values. The stronger the effect, the more the distribution piles up near zero.</p>

<p>This gives researchers a tool for diagnosing problems. If you look at the p-values across an entire literature and they're clustered suspiciously around 0.04 or 0.05—just barely significant—something's wrong. The distribution should either be uniform (null hypothesis true) or peaked near zero (real effect). A bump at the significance threshold suggests selective reporting or analytical flexibility.</p>

<h2>How Did We Get Here?</h2>

<p>The replication crisis didn't emerge from nowhere. It arose from a perfect storm of institutional pressures, methodological limitations, and human psychology.</p>

<p>Academic careers depend on publications. Publications favor positive, novel findings. This creates enormous pressure to produce significant results. Few researchers consciously commit fraud, but many unconsciously engage in practices that inflate their false positive rates: testing multiple hypotheses, stopping data collection when significance is reached, excluding inconvenient data points, choosing among multiple analytical approaches.</p>

<p>These practices aren't necessarily malicious. They reflect genuine uncertainty about the "right" way to analyze data and genuine motivation to find real effects. But their cumulative impact has been devastating.</p>

<p>Meanwhile, replication attempts have traditionally been unrewarded. Journals don't want to publish "same result as before" papers. Funding agencies don't want to pay for research that merely confirms existing work. Graduate students can't build careers on replication. The incentives all pointed toward novelty, not verification.</p>

<h2>The Birth of Metascience</h2>

<p>The recognition of these problems has given rise to an entirely new discipline: metascience. This is the science of science—using empirical methods to study how research is actually conducted and how it could be improved.</p>

<p>Metascientists have developed tools to detect publication bias, estimated the prevalence of questionable research practices, and quantified how often studies replicate. They've proposed and tested reforms like pre-registration (declaring your analysis plan before seeing the data), registered reports (journals agreeing to publish studies based on methodology, regardless of results), and larger sample sizes.</p>

<p>The phrase "replication crisis" itself was coined in the early 2010s, as these issues moved from specialist concern to widespread awareness. High-profile failures to replicate famous studies made headlines. Researchers began scrutinizing their own fields with new skepticism.</p>

<h2>Looking Forward</h2>

<p>Is this a crisis or a correction? Perhaps both.</p>

<p>The dismaying revelations about false findings represent science's immune system finally kicking in. The problems being exposed aren't new—they've existed for decades. What's new is our awareness of them and our willingness to address them.</p>

<p>Replication failures don't mean science is broken. They mean science is working, albeit more slowly and painfully than we'd like. Unsupported hypotheses are being eliminated. Weak findings are being questioned. Standards are being raised.</p>

<p>The crisis has also revealed how much we took for granted. We assumed published results were reliable without building systems to verify them. We assumed peer review caught methodological problems without checking whether it actually did. We assumed incentives aligned with truth-seeking without examining whether they really do.</p>

<p>Now we know better. And knowing, as they say, is half the battle.</p>

<h2>What This Means for Knowledge</h2>

<p>If you're reading about science in the news, this context matters. A single study, no matter how carefully conducted, is just one piece of evidence. The significance threshold of p < 0.05 doesn't mean the finding is 95% likely to be true—it means something much weaker and more technical about what you'd expect to see if the null hypothesis were true.</p>

<p>The strongest scientific knowledge comes from multiple independent replications, ideally using different methods and conducted by different research groups. When an effect shows up consistently across dozens of studies, in different populations, measured in different ways, you can have real confidence.</p>

<p>Until then, appropriate skepticism is warranted—not cynicism that dismisses all research, but healthy uncertainty that treats findings as provisional pending further evidence. This is, after all, what scientists themselves are supposed to do. The replication crisis is partly a story about what happens when that fundamental scientific humility gets lost.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Replication_crisis" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/d8893b89-d385-4eb6-830a-d5b778de65d4/index.html">
          <strong>The Economy of Knowing</strong>
        </a>
        <span class="article-meta">
          by Stuart Buck in 
        </span>
      </li>

      <li class="related-article-item">
        <a href="../../article/b44ac982-cc52-44df-84cf-04d53ba46c2f/index.html">
          <strong>New Podcast Release: Ellen Langer</strong>
        </a>
        <span class="article-meta">
          by Peter Singer in Bold Reasoning with Peter Singer
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>