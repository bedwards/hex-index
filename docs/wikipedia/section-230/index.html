<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Section 230 - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Section 230</h1>
        <div class="article-meta">
          <span class="read-time">13 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Section_230">Wikipedia: Section 230</a></p>

<h2>Twenty-Six Words That Built the Internet</h2>

<p>In 1996, two congressmen wrote a single sentence that would shape the future of human communication. It reads: "No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider."</p>

<p>That's it. Twenty-six words.</p>

<p>These words became Section 230 of the Communications Act, and they are why YouTube doesn't get sued every time someone uploads a defamatory video, why Twitter isn't held liable for every false tweet, and why Facebook can host billions of posts without employing billions of lawyers. Without this single sentence, the internet as we know it—the chaotic, creative, occasionally toxic, endlessly generative internet—might never have emerged.</p>

<h2>The Lawsuit That Started Everything</h2>

<p>To understand why Section 230 exists, you need to travel back to the early 1990s, when the internet was a strange new frontier and nobody quite knew what the rules should be.</p>

<p>Two companies—CompuServe and Prodigy—were among the first to offer Americans access to online discussion forums. They made different choices about how to run their platforms. CompuServe took a hands-off approach, essentially saying: post whatever you want, we won't police it. Prodigy hired moderators to review content and remove anything inappropriate.</p>

<p>Then both companies got sued.</p>

<p>CompuServe won its case. The court ruled that because the company didn't review or edit content, it was merely a distributor—like a bookstore or a newsstand. Bookstores don't read every book on their shelves, so they're not responsible if one of them contains libel. CompuServe was the same, the court reasoned.</p>

<p>Prodigy lost. Because the company had chosen to moderate content, the court treated it as a publisher—like a newspaper. And publishers are responsible for everything they print. By trying to keep its platform civil, Prodigy had inadvertently made itself liable for every single thing its users posted.</p>

<p>The message was perverse: if you want to avoid lawsuits, don't try to clean up your platform. The more responsibility you take, the more legal exposure you create.</p>

<h2>A Congressman Reads the Newspaper</h2>

<p>Representative Christopher Cox, a Republican from California, read about these two cases and thought the courts had gotten everything backwards. "It struck me that if that rule was going to take hold then the internet would become the Wild West and nobody would have any incentive to keep the internet civil," he later said.</p>

<p>Cox teamed up with Democratic Representative Ron Wyden from Oregon to draft a solution. Their idea was elegant: treat online platforms as neither publishers nor mere distributors, but as something new entirely. Platforms would be immune from liability for content posted by their users, and—crucially—they wouldn't lose that immunity by trying to moderate content.</p>

<p>This was the key innovation. Under the Cox-Wyden approach, a platform could choose to remove hate speech, pornography, or harassment without suddenly becoming responsible for everything else on the site. The law explicitly protected "good faith" efforts to restrict material that platforms considered "obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable."</p>

<p>That last phrase—"otherwise objectionable"—would prove extraordinarily broad. It essentially gave platforms permission to set their own standards.</p>

<h2>A Strange Birth</h2>

<p>The story of how Section 230 became law is genuinely odd. It was tucked inside the Communications Decency Act of 1996, which was primarily designed to criminalize sending "indecent" material to minors online. The larger bill was championed by Senator James Exon, a Democrat from Nebraska who was worried about children stumbling onto pornography.</p>

<p>So here was this strange legislative package: one part trying to restrict speech online, another part trying to protect platforms from liability for that speech. The whole thing passed with near-unanimous support and was signed by President Bill Clinton in February 1996.</p>

<p>Within a year, the Supreme Court struck down the anti-indecency provisions as unconstitutional violations of the First Amendment. But Section 230 survived. The court determined it was "severable"—legal jargon meaning it could stand on its own even though the surrounding legislation had been invalidated.</p>

<p>And so the twenty-six words lived on, orphaned from their original context, ready to shape a digital revolution their authors could barely have imagined.</p>

<h2>How the Law Actually Works</h2>

<p>Section 230 has two main components, both hiding under the somewhat misleading heading "Good Samaritan."</p>

<p>The first part says that providers of "interactive computer services" won't be treated as publishers of content created by others. If a user posts something defamatory on your platform, you can't be sued for defamation. The user can be sued, but you're protected.</p>

<p>The second part says that platforms can remove content they find objectionable without losing their immunity. This is the "good faith" moderation protection that was designed to fix the Prodigy problem.</p>

<p>Courts have developed a three-part test to determine when Section 230 applies. First, the defendant must be a provider or user of an interactive computer service. Second, the lawsuit must be trying to treat the defendant as a publisher of the content in question. Third, the content must have been provided by someone else—the platform can't claim immunity for content it created itself.</p>

<p>The immunity isn't absolute. Section 230 doesn't protect platforms from federal criminal prosecution. It doesn't shield them from intellectual property claims. And since 2018, it doesn't protect platforms that facilitate sex trafficking.</p>

<h2>The Foundation of Modern Tech</h2>

<p>It's difficult to overstate how much Section 230 enabled.</p>

<p>Consider what happens every minute on the internet. Users upload 500 hours of video to YouTube. They send 500 million tweets per day. They post 95 million photos and videos to Instagram. They write reviews, forum posts, comments, and messages in quantities that no human team could possibly review in real time.</p>

<p>Without Section 230, every one of these platforms would face a choice: either review every piece of content before it goes live—an impossible task at scale—or accept crushing legal liability for the inevitable defamatory, libelous, or otherwise illegal content that slips through.</p>

<p>Search engines exist because of Section 230. If Google could be sued for every problematic webpage it links to, it would either have to manually verify every site on the internet or shut down. Review sites like Yelp and TripAdvisor depend on the law. So do online marketplaces, social networks, comment sections, and dating apps.</p>

<p>A 2017 study by the economic consulting firm NERA estimated that Section 230 and its companion law, the Digital Millennium Copyright Act, together supported about 425,000 American jobs and generated $44 billion in annual revenue.</p>

<h2>The Good Samaritan Paradox</h2>

<p>Here's the deep irony at the heart of Section 230: it was designed to encourage platforms to clean up their act, but critics now argue it allows them to avoid accountability for leaving content up.</p>

<p>The law's authors believed that if platforms knew they could moderate without incurring liability, they would moderate more aggressively. They imagined a future where online spaces would become more civil because platforms would have every incentive to remove the worst content.</p>

<p>What actually happened was more complicated. Platforms did use their moderation powers, but inconsistently and often opaquely. They developed elaborate content policies but enforced them unevenly. They automated much of the process with algorithms that made mistakes. And they discovered that engagement—the metric that drove advertising revenue—was often highest for content that provoked outrage.</p>

<p>The law gave platforms both a shield and a choice. The shield protected them from liability. The choice was what to do with that protection.</p>

<h2>From Left and Right, Complaints</h2>

<p>By the 2020s, Section 230 had become one of the few issues that united American politicians across party lines—not in support, but in criticism. Both Democrats and Republicans wanted to change it, though for opposite reasons.</p>

<p>Conservatives argued that platforms were using their moderation powers to silence right-wing voices. When Twitter and Facebook banned Donald Trump after the January 6th Capitol riot, it confirmed for many Republicans that Big Tech was engaged in political censorship. They wanted Section 230 reformed to prevent platforms from removing legal speech based on viewpoint.</p>

<p>Liberals had the opposite complaint. They argued that platforms weren't moderating enough—that Section 230 gave tech companies immunity to host hate speech, disinformation, and harassment with impunity. Legal scholar Mary Anne Franks pointed out that the law's protections effectively subsidized the spread of bigotry, with marginalized groups bearing the heaviest costs.</p>

<p>Both sides were, in a sense, correct. Section 230 did give platforms enormous discretion. They could remove almost anything they wanted, or remove almost nothing. The law's only requirement was that whatever moderation they did perform be in "good faith."</p>

<h2>The Seigenthaler Incident</h2>

<p>One of the more famous demonstrations of Section 230's reach involved an unlikely target: Wikipedia.</p>

<p>John Seigenthaler was a distinguished American journalist who had worked as an aide to Robert Kennedy in the 1960s. In 2005, he discovered that his Wikipedia biography contained a bizarre and defamatory claim—that he had been "thought to have been directly involved in the Kennedy assassinations."</p>

<p>The false information had sat on Wikipedia for 132 days before anyone noticed. When Seigenthaler investigated, he found he had no legal recourse against Wikipedia itself. Section 230 protected the platform from liability for content posted by its users.</p>

<p>Seigenthaler wrote a scathing op-ed in USA Today: "We live in a universe of new media with phenomenal opportunities for worldwide communications and research—but populated by volunteer vandals with poison-pen intellects. Congress has enabled them and protects them."</p>

<p>The incident became a landmark example of both Wikipedia's vulnerability to vandalism and the scope of Section 230's protections.</p>

<h2>The Zeran Case Sets the Template</h2>

<p>The legal interpretation of Section 230 was largely set by a 1997 case called Zeran v. America Online.</p>

<p>The facts were disturbing. Shortly after the 1995 Oklahoma City bombing, someone posted advertisements on AOL using a Seattle man named Kenneth Zeran's phone number. The ads offered t-shirts celebrating the bombing with slogans like "Visit Oklahoma... It's a BLAST!!!" Zeran was flooded with threatening phone calls. He contacted AOL repeatedly, begging them to remove the posts. They eventually did, but slowly, and new posts kept appearing.</p>

<p>Zeran sued AOL for negligence in failing to remove the defamatory content quickly. The Fourth Circuit Court of Appeals ruled against him, holding that Section 230 provided complete immunity. The court's reasoning would shape internet law for decades: Congress had made a deliberate choice to protect platforms from liability because "the specter of tort liability in an area of such prolific speech would have an obviously chilling effect."</p>

<p>The court noted that it would be "impossible for service providers to screen each of their millions of postings for possible problems." Rather than create incentives for platforms to remain ignorant of bad content (as the Prodigy case had done), Section 230 allowed them to address problems without fear that doing so would make things worse.</p>

<h2>The FOSTA-SESTA Exception</h2>

<p>For more than two decades, Section 230 remained largely unchanged. Then came the first major carve-out.</p>

<p>In 2018, Congress passed FOSTA-SESTA—the Allow States and Victims to Fight Online Sex Trafficking Act. The law removed Section 230 protection for platforms that facilitated sex trafficking. Websites could now be held liable if they knowingly assisted, supported, or facilitated sex trafficking.</p>

<p>The law was a response to Backpage.com, a classified advertising site that prosecutors alleged had become a hub for sex trafficking. Under Section 230, victims had struggled to hold the site accountable.</p>

<p>FOSTA-SESTA was controversial even among those who wanted to fight trafficking. Critics argued that by making platforms liable for user content related to sex work, the law would push the sex trade underground where it would be harder to police. Sex worker advocacy groups warned that the law would endanger their members by eliminating online spaces where they could screen clients and share safety information.</p>

<p>Whatever its merits, FOSTA-SESTA established an important precedent: Section 230's protections could be narrowed. The wall wasn't impenetrable.</p>

<h2>What If Section 230 Disappeared?</h2>

<p>Legal scholars and tech policy experts have gamed out what might happen if Section 230 were repealed or significantly weakened.</p>

<p>One possibility is that platforms would become much more restrictive. Facing potential liability for any user content, they might preemptively remove anything even slightly risky. Political speech, controversial opinions, criticism of powerful people—all might be suppressed simply because platforms couldn't afford the legal exposure of hosting them.</p>

<p>Another possibility is that platforms would moderate less, not more. If any moderation decision could be second-guessed in court, platforms might decide it's safer to do nothing at all. They could argue they're mere conduits—the way CompuServe did in 1991—and hope the courts buy it.</p>

<p>A third possibility is that the business models of modern tech companies would become unworkable. User-generated content—the foundation of social media, review sites, and collaborative platforms—might simply be too risky to host. The internet could revert to something more like broadcast television, with professional content gatekeepers deciding what gets published.</p>

<p>Some argue this wouldn't be entirely bad. If platforms had to think harder about what they hosted, they might make better choices. The incentives that currently reward engagement over accuracy might shift toward rewarding responsibility.</p>

<h2>Global Implications</h2>

<p>Section 230 is American law, but its effects ripple worldwide.</p>

<p>Most major internet platforms are headquartered in the United States. The legal environment that shaped Facebook, Google, Twitter, and YouTube was an American legal environment, and Section 230 was at its center. When these platforms expanded globally, they brought their American DNA with them.</p>

<p>Other countries have made different choices. The European Union has pursued a more regulatory approach, requiring platforms to remove certain types of content and holding them responsible when they fail. Germany's NetzDG law imposes fines on platforms that don't quickly remove hate speech. The United Kingdom's Online Safety Bill creates new obligations for platform accountability.</p>

<p>These international developments are putting pressure on the American model. If platforms have to follow stricter rules elsewhere, they may adopt those rules globally—or create a fragmented internet where different users see different things depending on where they live.</p>

<h2>The Enduring Question</h2>

<p>At its core, Section 230 represents a bet that Congress made in 1996: that the benefits of an open, user-generated internet would outweigh the harms, and that platforms should have the freedom to develop their own approaches to content moderation.</p>

<p>For nearly three decades, that bet has held. The internet became the most powerful tool for communication and commerce in human history. It democratized publishing, enabled new forms of community, and created entirely new industries.</p>

<p>It also became a vector for harassment, disinformation, hate speech, and radicalization on a scale that the law's authors never anticipated.</p>

<p>Whether Section 230 should change—and how—remains one of the most contested questions in technology policy. The platforms say they need immunity to function at scale. Critics say that immunity has enabled irresponsibility. Conservatives say platforms censor too much. Liberals say they don't censor enough.</p>

<p>The twenty-six words that built the internet may need to be rewritten for the world that internet helped create. But there's little agreement on what the new words should say.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Section_230" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/e0d8d384-367b-4468-976a-b8639bf20b1c/index.html">
          <strong>11 predictions for 2026</strong>
        </a>
        <span class="article-meta">
          by Casey Newton in Platformer
        </span>
      </li>

      <li class="related-article-item">
        <a href="../../article/1dcc86e2-2eec-4e48-bef6-e83990a8dc95/index.html">
          <strong>The World Needs Europe to Rein In Social Media Before It Breaks Us All</strong>
        </a>
        <span class="article-meta">
          by Dan Perry in Dan Perry
        </span>
      </li>

      <li class="related-article-item">
        <a href="../../article/47577efa-ff5a-4b80-b694-8ca7b7002e75/index.html">
          <strong>EXCLUSIVE: Obama-Linked Stanford Center Held Secret Meeting With Foreign Governments To Plot Global Internet Censorship </strong>
        </a>
        <span class="article-meta">
          by Michael Shellenberger in Public
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>