<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Dartmouth workshop - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Dartmouth workshop</h1>
        <div class="article-meta">
          <span class="read-time">10 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">Wikipedia: Dartmouth workshop</a></p>

<p>In the summer of 1956, a small group of mathematicians and scientists gathered on a college campus in rural New Hampshire to discuss whether machines could think. They had no budget for anything beyond conversation. No laboratory equipment, no prototype computers to tinker with, no grand demonstrations planned. Just a top-floor room in the math department, some folding chairs, and about eight weeks of unstructured time.</p>

<p>From this modest beginning emerged an entirely new field of science.</p>

<h2>The Naming of Things</h2>

<p>Before the summer of 1956, researchers interested in thinking machines couldn't even agree on what to call their work. Some used "cybernetics," a term coined by the mathematician Norbert Wiener that emphasized feedback loops and self-regulating systems—the way a thermostat adjusts temperature or how a person reaches for a glass of water. Others preferred "automata theory," which focused on abstract mathematical models of computation. Still others spoke of "complex information processing."</p>

<p>The variety of names pointed to something deeper: nobody quite knew what this field was supposed to be.</p>

<p>John McCarthy was a young assistant professor of mathematics at Dartmouth College, just a few years past his doctorate from Princeton. He wanted to organize a summer workshop to clarify these scattered ideas, but first he needed a name—something neutral that wouldn't tie the new field to any existing camp.</p>

<p>He chose "artificial intelligence."</p>

<p>The term was deliberately diplomatic. It avoided the baggage of cybernetics, which would have meant either accepting Wiener as the field's founding authority or constantly arguing with him. It sidestepped automata theory's narrow focus on mathematical abstraction. And it captured something ambitious: not just machines that processed information, but machines that genuinely exhibited intelligence.</p>

<h2>The Pitch</h2>

<p>McCarthy needed money. In early 1955, he approached the Rockefeller Foundation, one of the great philanthropic institutions of the twentieth century, known for funding ambitious scientific endeavors. He proposed a summer seminar at Dartmouth for about ten participants.</p>

<p>That June, McCarthy traveled to New York with Claude Shannon, the legendary Bell Labs mathematician who had essentially invented information theory—the mathematical framework underlying all digital communication. Together they met with Robert Morison, the Foundation's Director of Biological and Medical Research.</p>

<p>Morison was intrigued but uncertain. This was, after all, a visionary project. Could machines really be made to think? Would a summer of conversation produce anything concrete?</p>

<p>By September 1955, McCarthy had assembled a formal proposal with three co-authors: Shannon, Marvin Minsky (then a young researcher at Harvard), and Nathaniel Rochester (an IBM engineer who had helped design the company's first commercial scientific computer). The proposal opened with a statement that remains remarkable for its confidence:</p>

<blockquote>
<p>We propose that a 2-month, 10-man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.</p>
</blockquote>

<p>Read that again. Every aspect of learning. Any feature of intelligence. Can in principle be precisely described. Can be made to simulate it.</p>

<p>This was not a modest claim about building better calculators. It was a declaration that the human mind itself could be understood, formalized, and reproduced in a machine.</p>

<h2>The Arrival</h2>

<p>Around June 18, 1956, the earliest participants began arriving in Hanover. The town sits along the Connecticut River in the Upper Valley of New Hampshire, home to Dartmouth College since 1769. Summer in Hanover is brief and green, the campus quiet between academic terms.</p>

<p>Ray Solomonoff may have been the first to arrive, possibly accompanied by a mathematician named Tom Etter. McCarthy was already there, having secured an apartment for the summer. Most participants would stay at the Hanover Inn, the traditional hotel adjacent to campus, though Solomonoff and Minsky found rooms in professors' apartments.</p>

<p>The group had reserved the entire top floor of the mathematics department for their discussions. Most weekdays, they would gather in the main classroom. Sometimes someone would lead a focused discussion of their own ideas. More often, the conversation wandered freely across topics.</p>

<p>This was not a directed research project. There was no agenda, no deliverables, no final report to produce. It was, as one later account described it, "an extended brainstorming session."</p>

<h2>Who Came</h2>

<p>The original plan called for eleven attendees over two months. Reality was messier. People came and went. Some stayed for days, others for weeks. On any given day, between three and eight people might show up for the session.</p>

<p>Ray Solomonoff kept careful notes throughout the summer, and from these we know who actually attended. The core group included:</p>

<ul>
<li>John McCarthy, the organizer, who would go on to coin the term "artificial intelligence" and invent the programming language LISP</li>
<li>Marvin Minsky, who would become perhaps the most famous AI researcher of the twentieth century</li>
<li>Claude Shannon, already a legend for his work on information theory</li>
<li>Nathaniel Rochester, the IBM engineer</li>
<li>Ray Solomonoff, who would later develop the theory of algorithmic probability</li>
</ul>

<p>Only three people—McCarthy, Minsky, and Solomonoff—stayed for the entire eight weeks.</p>

<p>Others drifted through. Oliver Selfridge, a pioneer in pattern recognition. Julian Bigelow, who had helped build one of the first electronic computers at Princeton. W. Ross Ashby, a British psychiatrist and cyberneticist. Arthur Samuel, an IBM researcher who had built a checkers-playing program that could learn from experience.</p>

<p>And tucked into Solomonoff's list is a name that jumps out: John Nash. Yes, that John Nash—the mathematician who would later win the Nobel Prize in Economics for his work on game theory, the subject of the book and film "A Beautiful Mind." He visited briefly, though his contributions to the discussions went unrecorded.</p>

<p>Herbert Simon and Allen Newell came from the RAND Corporation and Carnegie Mellon. They brought something remarkable: a working program called the Logic Theorist, which could prove mathematical theorems. It was perhaps the first true artificial intelligence program, and they demonstrated it at Dartmouth. The machine had proved thirty-eight of the first fifty-two theorems in Bertrand Russell and Alfred North Whitehead's Principia Mathematica—and for one theorem, it found a proof more elegant than the one the human mathematicians had published.</p>

<h2>What They Discussed</h2>

<p>The proposal had laid out an ambitious research agenda: natural language processing, neural networks, the theory of computation, abstraction, creativity. These remain central concerns of artificial intelligence research nearly seventy years later.</p>

<p>But the summer itself was less structured than the proposal suggested. Conversations ranged widely. Some directions that emerged would shape the field for decades:</p>

<p><strong>Symbolic methods.</strong> The idea that intelligence could be modeled through the manipulation of symbols according to formal rules. This approach would dominate AI research for thirty years, leading to expert systems, theorem provers, and elaborate knowledge representations.</p>

<p><strong>Limited domains.</strong> Rather than trying to build general intelligence all at once, focus on narrow problems where progress could be demonstrated. This practical orientation would eventually produce everything from chess programs to medical diagnosis systems.</p>

<p><strong>Deduction versus induction.</strong> Should thinking machines work by applying logical rules (like a mathematician proving theorems) or by learning patterns from examples (like a child learning language)? This tension would persist throughout the history of AI, with the pendulum swinging between rule-based and learning-based approaches.</p>

<p>Arthur Samuel, the checkers researcher, later summarized the experience: "It was very interesting, very stimulating, very exciting."</p>

<h2>The Constitutional Convention of AI</h2>

<p>The Dartmouth Workshop has been called "the Constitutional Convention of AI"—a reference to the 1787 gathering in Philadelphia where American founders drafted the Constitution. The analogy captures something important.</p>

<p>The Philadelphia convention didn't create American democracy from nothing. The colonies had been governing themselves for over a century. But the convention gave the scattered experiments a coherent form, a shared vocabulary, and institutional legitimacy.</p>

<p>Similarly, thinking-machine research existed before Dartmouth. Alan Turing had written about machine intelligence in 1950. Shannon and others had been building learning machines at Bell Labs. Researchers at various universities were pursuing related problems in isolation.</p>

<p>What Dartmouth provided was crystallization. It gave the field a name: artificial intelligence. It gathered the founders in one place, letting them discover common ground and establish personal connections that would persist for decades. And perhaps most importantly, it gave the field confidence. The proposal's bold claim—that every aspect of intelligence could in principle be described precisely enough for a machine to simulate—became a shared article of faith.</p>

<h2>What Didn't Happen</h2>

<p>The proposal's optimism was, in retrospect, wildly premature. "We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer."</p>

<p>A summer.</p>

<p>The founders imagined that machine intelligence was a problem like breaking an enemy's code or building an atomic bomb—difficult, certainly, requiring brilliant people and sustained effort, but fundamentally solvable once the right minds were assembled and given resources.</p>

<p>It wasn't.</p>

<p>Nearly seventy years later, we still don't have machines that can "use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves" in the way the proposal envisioned. The problems turned out to be vastly harder than anyone anticipated. Each apparent solution revealed new layers of difficulty beneath.</p>

<p>The history of AI since Dartmouth has been marked by cycles of optimism and disappointment—"AI summers" when breakthrough seemed imminent, followed by "AI winters" when funding dried up and researchers scattered to other fields.</p>

<p>And yet.</p>

<h2>The Long View</h2>

<p>The Dartmouth founders were wrong about the timeline but perhaps right about the destination. The conjecture at the heart of their proposal—that intelligence can be precisely described and simulated—has never been disproven. If anything, recent advances in machine learning have made it seem more plausible, not less.</p>

<p>The four organizers of the workshop went on to remarkable careers. McCarthy spent most of his career at Stanford, where he founded the Stanford AI Laboratory and continued developing the theoretical foundations of the field. Minsky built MIT's AI laboratory into one of the world's leading research centers. Shannon, already famous, continued his work on information and communication. Rochester remained at IBM, contributing to the development of increasingly powerful computers.</p>

<p>All four are considered founding fathers of artificial intelligence, though the field they founded looks very different today than what they imagined. The symbolic methods that dominated early AI research have largely given way to statistical learning approaches. Neural networks, mentioned briefly in the original proposal, have become the dominant paradigm after decades in the wilderness. The deductive systems that excited early researchers have been overshadowed by systems that learn inductively from vast quantities of data.</p>

<p>But the fundamental question posed at Dartmouth—whether machines can truly think—remains open. The summer workshop didn't answer it. Perhaps nothing ever will. Perhaps the question itself is somehow malformed, like asking whether submarines can really swim.</p>

<p>What the workshop did accomplish was to give that question institutional form. It created a community of researchers committed to pursuing it, a shared vocabulary for discussing it, and a name that has endured: artificial intelligence.</p>

<p>Eight weeks in a New Hampshire summer. A dozen or so people at a time. No laboratory equipment, no breakthrough demonstrations, no definitive answers. Just conversation.</p>

<p>Sometimes that's how fields begin.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Dartmouth_workshop" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/069ff1bf-ab3a-4e2f-b1e9-3a473985854e/index.html">
          <strong>The Shape of Artificial Intelligence</strong>
        </a>
        <span class="article-meta">
          by Alberto Romero in The Algorithmic Bridge
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>