<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TESCREAL - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>TESCREAL</h1>
        <div class="article-meta">
          <span class="read-time">13 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/TESCREAL">Wikipedia: TESCREAL</a></p>

<p>Imagine a secret handshake, except instead of interlocking fingers, it's interlocking ideologies—and instead of fraternity brothers, it's some of the wealthiest and most influential people in Silicon Valley. That's the accusation behind TESCREAL, a newly coined acronym that its creators argue reveals something troubling about the intellectual currents shaping artificial intelligence, space exploration, and the future of humanity itself.</p>

<h2>The Alphabet Soup of Tomorrow</h2>

<p>TESCREAL stands for seven distinct but allegedly interconnected movements: Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalism, Effective Altruism, and Longtermism. Each word represents a philosophy about humanity's future. Together, critics argue, they form something more concerning than any single ideology could be on its own.</p>

<p>The term was proposed in 2023 by computer scientist Timnit Gebru and philosopher Émile Torres. Gebru is a prominent artificial intelligence researcher who made headlines when she departed Google amid controversy over her research into the biases embedded in large language models. Torres is a philosopher who has written extensively about existential risk and the ethics of emerging technologies.</p>

<p>Their argument is deceptively simple: these seven ideologies aren't separate intellectual traditions that happen to attract similar followers. They're a "bundle," interconnected and overlapping, with shared origins and—most controversially—shared roots in the eugenics movements of the twentieth century.</p>

<h2>Unpacking Each Letter</h2>

<p>Let's take these one at a time, because each represents a distinct vision of what humanity could become.</p>

<p><strong>Transhumanism</strong> is perhaps the most familiar. It's the idea that we should use technology to transcend the biological limitations of our bodies and minds. Advocates envision a future where humans merge with machines, extend their lifespans indefinitely, and enhance their cognitive abilities beyond anything evolution produced. Think brain-computer interfaces, genetic engineering, or uploading consciousness to digital substrates.</p>

<p><strong>Extropianism</strong> is transhumanism's more optimistic cousin, emphasizing the idea that technology will lead to ever-increasing order, intelligence, and capability. The term comes from "extropy," coined as the opposite of entropy—that inexorable tendency toward disorder that physics tells us governs the universe. Extropians believe we can fight back against cosmic decay.</p>

<p><strong>Singularitarianism</strong> centers on the concept of the technological singularity, a hypothetical future point when artificial intelligence becomes capable of recursive self-improvement. The idea is that once we create an AI smart enough to make itself smarter, and that smarter AI makes itself smarter still, intelligence will explode exponentially. What happens after that is, by definition, impossible for our pre-singularity minds to predict—hence "singularity," borrowed from physics, where a singularity is a point beyond which normal rules break down.</p>

<p><strong>Cosmism</strong> is perhaps the most obscure term in the bundle. It refers to a philosophical tradition with roots in early twentieth-century Russia, particularly the work of Nikolai Fedorov, who believed humanity's cosmic duty was to resurrect the dead and colonize space. Modern cosmism embraces similar grand ambitions about humanity's destiny among the stars.</p>

<p><strong>Rationalism</strong>, in this context, doesn't mean the broad philosophical tradition dating back to Descartes. Instead, it refers to a specific internet community centered around websites like LessWrong, where participants apply Bayesian reasoning and decision theory to questions about artificial intelligence, human cognition, and existential risk. This community has been enormously influential in shaping how Silicon Valley thinks about AI safety.</p>

<p><strong>Effective Altruism</strong> is a social movement that tries to use evidence and reasoning to determine the most effective ways to benefit others. At its most basic, it asks: if you want to do good in the world, how can you do the most good? This has led some effective altruists to focus on neglected tropical diseases, where charitable dollars stretch furthest, while others have concluded that preventing future catastrophes—including those posed by artificial intelligence—should be the priority.</p>

<p><strong>Longtermism</strong> is the philosophical view that positively influencing the long-term future is a key moral priority of our time. If humanity could exist for millions or billions of years, longtermists argue, then the potential number of future people vastly exceeds the current population. Actions that affect whether those future people exist, or what kind of lives they lead, become overwhelmingly important—even if they seem abstract or distant today.</p>

<h2>The Bundle Theory</h2>

<p>Here's where Gebru and Torres's critique becomes pointed. They argue these seven philosophies aren't merely overlapping Venn diagrams of interested nerds. They're a coherent worldview with a troubling genealogy.</p>

<p>The connecting thread, they claim, is a particular way of thinking about human value. Who matters? Who should be prioritized? Who gets to define what a good future looks like?</p>

<p>Consider longtermism's emphasis on future people. If we take seriously the claim that there could be trillions of future humans living among the stars, then present-day concerns—poverty, inequality, algorithmic bias—might seem comparatively trivial. After all, what's a few million people suffering today compared to trillions who might never exist if we fail to develop artificial general intelligence or colonize space?</p>

<p>This is where critics see danger. Media scholar Ethan Zuckerman argues that by only considering goals valuable to the TESCREAL worldview, proponents can justify projects with immediate drawbacks—racial inequity, environmental degradation, algorithmic bias—as acceptable costs for far-future benefits.</p>

<p>Science fiction author Charles Stross puts it more bluntly. He argues these ideologies allow billionaires to pursue massive personal projects—like space colonization—by framing them as existential necessities. If not pursuing your pet project poses an existential risk to humanity, who could object? The stakes, conveniently, always justify the investment.</p>

<h2>The Eugenics Question</h2>

<p>The most explosive element of Gebru and Torres's argument is their claim that TESCREAL ideologies "directly originate from twentieth-century eugenics."</p>

<p>This requires some historical context. Eugenics—the idea that humanity could be improved through selective breeding—was once mainstream science. It enjoyed support across the political spectrum and was implemented through forced sterilization programs in many countries, including the United States. The Nazi regime took eugenics to its horrific logical conclusion, and after World War II, the term became toxic.</p>

<p>But the underlying impulse—the desire to improve humanity, to guide evolution, to create better humans—didn't disappear. It changed vocabulary. Gebru and Torres argue that transhumanism and its related movements represent eugenics in new clothes, now focused on technological rather than biological enhancement, but retaining the same fundamental assumptions about human hierarchy and improvement.</p>

<p>Not everyone buys this genealogy. Critics of the TESCREAL concept argue that grouping these movements together ignores their genuine differences and conflates people with wildly different motivations. Writing in Asterisk, a magazine associated with effective altruism, Ozy Brennan criticized the framework as treating different philosophies as a "monolithic" movement when they're actually distinct.</p>

<p>Oliver Habryka, who runs the rationalist website LessWrong, expressed bemusement at being accused of participating in a movement he'd never heard of: "I've never in my life met a cosmist; apparently I'm great friends with them."</p>

<p>Politics writer Danyl McLauchlan at Radio New Zealand noted the awkwardness of lumping effective altruists—many of whom focus on helping the global poor through proven interventions like malaria prevention—into a conspiracy with would-be creators of superhuman AI.</p>

<h2>The Secular Religion</h2>

<p>One of the more interesting accusations leveled at TESCREAL movements is that they function as secular religions.</p>

<p>Consider the parallels. There's an eschatology—a narrative about the end of the world—whether that's the singularity, the existential catastrophe we must prevent, or the cosmic destiny we must fulfill. There's a chosen people: those enlightened enough to understand the stakes. There are prophets: figures like Ray Kurzweil preaching the coming singularity, or philosopher Nick Bostrom warning about superintelligent AI. There are sacred texts: Bostrom's <em>Superintelligence</em>, Eliezer Yudkowsky's writings on AI alignment, William MacAskill's <em>What We Owe the Future</em>.</p>

<p>There are even schisms. The TESCREAL world divides roughly into "accelerationists," who believe we must race toward superintelligent AI to achieve utopia, and "doomers," who believe that same AI is likely to destroy humanity unless we proceed with extreme caution. These camps fight bitterly, yet both agree that artificial general intelligence is the central question of our time.</p>

<p>Gebru has described this conflict as "a secular religion selling AGI-enabled utopia and apocalypse." The AI is either our salvation or our extinction, but either way, it's the only thing that really matters.</p>

<p>Writers in Current Affairs compared this to "any other monomaniacal faith... in which doubters are seen as enemies and beliefs are accepted without evidence."</p>

<h2>The Manifesto Moment</h2>

<p>In late 2023, venture capitalist Marc Andreessen published what he called the "Techno-Optimist Manifesto." Andreessen co-founded the legendary web browser company Netscape and now runs Andreessen Horowitz, one of Silicon Valley's most influential investment firms. His manifesto was a 5,000-word defense of technological progress against its critics.</p>

<p>The document reads like a religious text. It lists "Patron Saints of Techno-Optimism" including Nietzsche, Ayn Rand, and various economists and futurists. It condemns "enemies" of progress: sustainability advocates, social responsibility initiatives, the precautionary principle. Most notably, Andreessen argued that artificial intelligence could save countless future potential lives—and that those working to slow its development should be condemned as murderers.</p>

<p>Critics Jag Bhalla and Nathan Robinson called the manifesto a "perfect example" of TESCREAL ideologies in action. Here was a billionaire investor declaring that opposition to his portfolio companies' work was not merely mistaken but morally equivalent to mass murder.</p>

<h2>The Billionaire Gallery</h2>

<p>Part of what makes TESCREAL a compelling concept, even to skeptics, is how neatly it maps onto the actual stated beliefs of extremely powerful people.</p>

<p>Elon Musk tweeted in 2022 that William MacAskill's longtermist book <em>What We Owe the Future</em> was "a close match for my philosophy." Musk has founded Neuralink, a company developing brain-computer interfaces—a quintessentially transhumanist project. He founded SpaceX with the explicit goal of making humanity a multi-planetary species—a cosmist dream. His XAI company focuses on artificial general intelligence and existential risk—singularitarian and rationalist territory.</p>

<p>Peter Thiel, the PayPal co-founder and venture capitalist, has invested in life extension research and written about the importance of technological progress escaping the constraints of conventional governance. Benjamin Svetkey wrote in The Hollywood Reporter that Thiel and other Silicon Valley executives who supported Donald Trump's 2024 presidential campaign were pushing policies that would eliminate "regulators whose outdated restrictions on things like human experimentation are slowing down progress toward a technotopian paradise."</p>

<p>Sam Altman, the CEO of OpenAI, has been described as deeply influenced by TESCREAL movements. His company's explicit mission is to develop artificial general intelligence that benefits humanity—but critics argue the company's approach exemplifies precisely the dangers Gebru and Torres warn about: building enormously powerful systems first, and hoping to figure out the safety implications later.</p>

<h2>The FTX Connection</h2>

<p>No discussion of TESCREAL's influence would be complete without mentioning Sam Bankman-Fried, the cryptocurrency exchange founder who became the most prominent face of effective altruism before his empire collapsed in fraud.</p>

<p>Bankman-Fried was explicit about his motivations. He wanted to make as much money as possible so he could give it away as effectively as possible. He funded AI safety research, pandemic preparedness, and other causes aligned with longtermist priorities. His spectacular fall—he was convicted of fraud and sentenced to prison—raised uncomfortable questions about whether TESCREAL ideologies might rationalize bad behavior in pursuit of ostensibly noble ends.</p>

<p>According to The Guardian, bankruptcy administrators have been trying to recover approximately five million dollars allegedly transferred to help purchase a historic hotel used for conferences associated with longtermism, rationalism, and effective altruism. At one such conference, attendees reportedly included a self-described "liberal eugenicist."</p>

<h2>The Critique of the Critique</h2>

<p>Not everyone finds the TESCREAL framework convincing.</p>

<p>James Pethokoukis of the American Enterprise Institute, a conservative think tank, argues that the tech billionaires criticized for allegedly espousing TESCREAL have significantly advanced society. Whatever their philosophical motivations, they've built products that billions of people use daily.</p>

<p>Eli Sennesh and James Hughes, writing for the technoprogressive Institute for Ethics and Emerging Technologies, argue that TESCREAL is a left-wing conspiracy theory that groups together philosophies with mutually exclusive tenets. You cannot simultaneously believe that AI will inevitably destroy humanity and that AI will inevitably save humanity—yet the TESCREAL framework treats both views as part of the same movement.</p>

<p>There's also a simpler objection: maybe rich tech people just tend to be interested in science fiction, futurism, and grand visions of humanity's potential. That doesn't make them participants in a coordinated ideology with eugenic roots. It might just make them nerds with money.</p>

<h2>Why This Matters for AI</h2>

<p>At the heart of the TESCREAL debate is a question about who gets to shape the future of artificial intelligence.</p>

<p>Much of the discourse about existential risk from AI occurs among people Gebru and Torres would identify as TESCREALists. They frame the choices as binary: either we develop superintelligent AI and achieve utopia, or we fail and humanity goes extinct. Either we're accelerationists racing toward the singularity, or we're doomers desperately trying to avert it.</p>

<p>But what if this framing itself is the problem?</p>

<p>Gebru and Torres argue that both accelerationists and doomers use hypothetical AI-driven apocalypses to justify unlimited research, development, and deregulation. By focusing on speculative far-future scenarios, they distract from present-day harms: the workers displaced by automation, the communities surveilled by facial recognition, the people denied jobs or loans by opaque algorithmic systems, the carbon emissions of training ever-larger models.</p>

<p>Philosopher Yogi Hale Hendlin argues that TESCREALists simultaneously ignore the human causes of societal problems and over-engineer solutions, missing the context in which problems actually arise. You don't need to colonize Mars to address climate change. You don't need superintelligent AI to reduce global poverty. But those interventions are less exciting, require less venture capital, and don't come with the frisson of saving humanity itself.</p>

<h2>The Language of Philosophy</h2>

<p>George Orwell warned that when certain topics are raised, "the concrete melts into the abstract and no one seems able to think of turns of speech that are not hackneyed." He was writing about political language, but the observation applies equally to discussions of humanity's far future.</p>

<p>TESCREAL discourse is saturated with abstractions: existential risk, expected value, astronomical stakes, utility maximization, x-risk, s-risk, p(doom). These terms have precise technical meanings within their communities of use. They also have the effect of making speculative scenarios feel concrete while making present suffering feel abstract.</p>

<p>When you calculate that preventing human extinction could save trillions of potential future lives, the math seems to justify almost anything. When you speak of "astronomical waste"—the lost value of all those futures that won't exist if we don't colonize the galaxy—present human concerns can seem parochial, even selfish.</p>

<p>This is perhaps the deepest critique of the TESCREAL worldview. It's not just that these ideologies might share uncomfortable historical roots. It's that they might make it harder to think clearly about the world as it actually is, the people who actually exist, and the problems that actually need solving.</p>

<h2>The Future of the Argument</h2>

<p>Whether TESCREAL proves to be a useful analytical category or an overly broad brush remains to be seen. The term has clearly struck a nerve, entering discussions of technology ethics, AI governance, and Silicon Valley culture with remarkable speed for academic coinage.</p>

<p>What seems certain is that the underlying tensions won't disappear. As artificial intelligence becomes more powerful, as wealth concentrates further among those building and deploying these systems, as the gap between their visions of the future and everyone else's experience of the present widens—these questions will only become more urgent.</p>

<p>Who decides what humanity's future should look like? Whose concerns count? And when someone tells you they're working to save the world, it might be worth asking: whose world, exactly, and saved for whom?</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/TESCREAL" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/1f5ed959-7b0c-4e49-bc54-ebb547e48f70/index.html">
          <strong>Philosophy and The English Language</strong>
        </a>
        <span class="article-meta">
          by Bentham&#039;s Bulldog in 
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>