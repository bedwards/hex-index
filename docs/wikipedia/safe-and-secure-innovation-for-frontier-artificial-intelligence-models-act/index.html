<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Safe and Secure Innovation for Frontier Artificial Intelligence Models Act - Hex Index</title>
  <link rel="stylesheet" href="../../styles.css">
</head>
<body class="reading-mode">
  <header class="reading-header">
    <a href="../../index.html" class="back-link">&larr; Back to Library</a>
  </header>
  <main class="reading-content">
    
    <article class="wikipedia-page">
      <header class="wikipedia-header">
        <div class="type-badge">Wikipedia Deep Dive</div>
        <h1>Safe and Secure Innovation for Frontier Artificial Intelligence Models Act</h1>
        <div class="article-meta">
          <span class="read-time">12 min read</span>
        </div>
      </header>

      <div class="wikipedia-content">
        <p class="source-note">Based on <a href="https://en.wikipedia.org/wiki/Safe_and_Secure_Innovation_for_Frontier_Artificial_Intelligence_Models_Act">Wikipedia: Safe and Secure Innovation for Frontier Artificial Intelligence Models Act</a></p>

<h2>The Bill That Almost Changed Everything</h2>

<p>In September 2024, California Governor Gavin Newsom killed what might have been the most ambitious artificial intelligence safety law ever proposed in the United States. With a single veto, he ended months of fierce debate over a question that has divided technologists, ethicists, Hollywood celebrities, and venture capitalists alike: How do you regulate something so powerful it might not even exist yet?</p>

<p>Senate Bill 1047, officially titled the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, was designed to prevent catastrophic harm from AI systems that exceed current capabilities. Not the chatbots and image generators we have today. The ones coming next.</p>

<h2>What Made This Bill Different</h2>

<p>Most technology regulation responds to problems after they've occurred. We got seatbelt laws after decades of car crashes. Social media regulations emerged only after widespread documentation of their harms. SB 1047 attempted something unusual: regulating technology based on its potential future capabilities rather than demonstrated past harms.</p>

<p>The bill targeted what it called "frontier models"—AI systems trained using more than ten to the twenty-sixth power of computing operations. That's a one followed by twenty-six zeros. To put this in perspective, training such a model would cost over one hundred million dollars in computing resources alone.</p>

<p>These thresholds weren't arbitrary. They represented a deliberate attempt to focus only on the most powerful systems—the kind that could, theoretically, help someone design a biological weapon or launch devastating cyberattacks on power grids and financial systems.</p>

<h2>The Fear Behind the Legislation</h2>

<p>The bill emerged from a genuine anxiety spreading through parts of the AI research community. In November 2022, OpenAI released ChatGPT. Suddenly, millions of people experienced firsthand how capable these systems had become. The technology's rapid advancement startled even some of its creators.</p>

<p>By May 2023, hundreds of technology executives and AI researchers signed an extraordinary statement. They called for treating the "risk of extinction from AI" as a global priority, placing it alongside pandemics and nuclear war. Among the signatories were Geoffrey Hinton and Yoshua Bengio, two of the three researchers commonly called the "Godfathers of AI"—pioneers whose foundational work on neural networks helped create the current generation of AI systems.</p>

<p>When the people who invented a technology warn it might end civilization, regulators tend to pay attention.</p>

<h3>But Is Extinction Really on the Table?</h3>

<p>Not everyone agrees these fears are warranted. Many AI researchers view existential risk concerns as science fiction dressed up in academic language. They argue that focusing on hypothetical future catastrophes distracts from documented present harms: AI systems that discriminate against job applicants based on race, that spread misinformation, that enable new forms of fraud and harassment.</p>

<p>This tension—between those worried about AI ending humanity and those worried about AI making existing problems worse—shaped much of the debate around SB 1047.</p>

<h2>What the Bill Would Have Required</h2>

<p>Companies developing frontier AI models would have needed to create a "safety and security protocol" before training began. Before releasing their models to the public, they would have submitted compliance statements confirming they had taken reasonable care to prevent catastrophic misuse.</p>

<p>The bill defined "critical harms" in specific terms:</p>

<ul>
<li>Helping create chemical, biological, radiological, or nuclear weapons</li>
<li>Enabling cyberattacks on critical infrastructure causing mass casualties or at least five hundred million dollars in damage</li>
<li>Autonomous crimes resulting in mass casualties or equivalent financial harm</li>
</ul>

<p>Starting in 2026, companies would have faced annual third-party audits. The bill also required what the press variously called a "kill switch" or "circuit breaker"—the technical ability to shut down a model if something went wrong.</p>

<p>Perhaps most controversially, SB 1047 included whistleblower protections. Employees who reported safety problems would have been shielded from retaliation. Given the secrecy surrounding AI development at major companies, this provision particularly alarmed some industry leaders.</p>

<h3>CalCompute: The Lesser-Known Provision</h3>

<p>Buried beneath the safety requirements was something genuinely novel: the creation of CalCompute, a public cloud computing cluster affiliated with the University of California. This would have given startups, academic researchers, and community organizations access to computing resources typically available only to well-funded corporations.</p>

<p>Training frontier AI models requires enormous computing power. By making that power publicly available, CalCompute aimed to democratize AI development—ensuring that safety research and innovative applications wouldn't be the exclusive province of a few wealthy companies.</p>

<h2>The Political Journey</h2>

<p>State Senator Scott Wiener introduced SB 1047 in February 2024, building on earlier legislative groundwork. California has a history of stepping into regulatory vacuums left by federal inaction. The state pioneered consumer privacy protections with the California Consumer Privacy Act. It enacted net neutrality rules after federal protections were rolled back.</p>

<p>Wiener explicitly modeled the bill on President Biden's October 2023 executive order on artificial intelligence, adapting federal principles into state law. Without unified federal legislation, California—home to most major AI companies—would set the de facto national standard.</p>

<p>The bill's trajectory through the legislature seemed promising. It passed the State Senate 32 to 1 in May. After significant amendments responding to industry feedback, it cleared the State Assembly 48 to 16 in late August. A final Senate vote approved the amended version 30 to 9.</p>

<p>Then it landed on Governor Newsom's desk.</p>

<h2>Why Newsom Said No</h2>

<p>On September 29, 2024, Newsom vetoed the bill. His reasoning centered on what he saw as a fundamental flaw in its design: the focus on large models based purely on computational size.</p>

<p>A model trained with one hundred million dollars might pose certain risks. But a smaller, cheaper model deployed in a hospital's diagnostic system or a self-driving car might pose greater immediate danger. By targeting only the largest models, Newsom argued, the bill could create a "false sense of security" while ignoring genuinely dangerous applications of smaller systems.</p>

<p>He also expressed concern about adaptability. AI technology evolves rapidly. A regulatory framework locked to specific computational thresholds might become obsolete before its ink dried.</p>

<p>Newsom committed to working with technology experts and research institutions, including Stanford's Human-Centered AI Institute, to develop more flexible approaches. Whether this represents a genuine commitment to alternative regulation or a polite way of killing the concept entirely remains to be seen.</p>

<h2>The Battle Lines</h2>

<p>The debate over SB 1047 produced unusual alliances and unexpected divisions.</p>

<h3>The Supporters</h3>

<p>Geoffrey Hinton and Yoshua Bengio—those Godfathers of AI—supported the bill. So did Elon Musk, whose company xAI is developing its own large language models. Stuart Russell, a leading AI researcher at Berkeley and author of a standard textbook on artificial intelligence, endorsed it.</p>

<p>Former New York Mayor Bill de Blasio signed on, as did over one hundred twenty Hollywood celebrities including Mark Hamill, Jane Fonda, and director J.J. Abrams. The Screen Actors Guild, still processing AI's implications for their industry after their 2023 strike, sent a letter of support to the governor.</p>

<p>Several whistleblowers from OpenAI publicly backed the bill, including Daniel Kokotajlo and William Saunders—insiders who had grown alarmed at what they witnessed during their employment.</p>

<p>Max Tegmark, an MIT physicist who has written extensively about AI risk, compared the bill's approach to the Food and Drug Administration requiring clinical trials before drug companies can release new medications. The analogy was apt: both frameworks require demonstrating safety before deployment, shifting the burden of proof from regulators to developers.</p>

<h3>The Opposition</h3>

<p>The opposition included some equally prominent names. Andrew Ng, a Stanford professor who helped lead AI efforts at Google and Baidu, argued for more targeted regulations—addressing specific harms like deepfake pornography rather than attempting to regulate an entire technology category.</p>

<p>Fei-Fei Li, another Stanford luminary who helped create ImageNet (the dataset that sparked the deep learning revolution), opposed the bill. So did Yann LeCun, the third Godfather of AI and Chief AI Scientist at Meta.</p>

<p>Perhaps most striking was the opposition from California's own congressional delegation. Nancy Pelosi, Ro Khanna, Anna Eshoo, and Zoe Lofgren—all Democrats representing districts with substantial tech industry employment—came out against the bill.</p>

<p>Major companies were divided. Meta and OpenAI opposed or raised concerns. Google, Microsoft, and Anthropic proposed substantial amendments rather than outright opposition. After the August amendments, Anthropic CEO Dario Amodei wrote that the revised bill's "benefits likely outweigh its costs"—hardly a ringing endorsement, but a notable shift from earlier skepticism.</p>

<h2>The Open Source Controversy</h2>

<p>One of the most heated debates concerned open source AI. Companies like Meta have released powerful AI models freely, allowing anyone to download, modify, and deploy them. This democratizes access but creates regulatory complications: if Meta releases a model and someone else fine-tunes it for malicious purposes, who bears responsibility?</p>

<p>Yann LeCun argued the bill would "kill open source AI models." The AI Alliance, a coalition of open source advocates, formally opposed the legislation. Their concern was straightforward: faced with potential liability, companies might simply stop releasing models publicly.</p>

<p>Lawrence Lessig, the Harvard professor who co-founded Creative Commons and has spent decades advocating for open knowledge sharing, disagreed. He argued that clear liability rules would actually make open source AI safer and more popular, since developers would face reduced risk when using properly tested models.</p>

<h3>The Regulatory Capture Question</h3>

<p>A subtler criticism came from those who worried about regulatory capture—the phenomenon where regulations ostensibly designed to protect the public end up serving the interests of dominant industry players.</p>

<p>Critics argued that only large companies could afford the compliance costs associated with training frontier models. Safety testing, annual audits, compliance documentation—these requirements would create barriers to entry that established players could absorb but startups could not.</p>

<p>Supporters countered that the bill's thresholds were deliberately set high. Models costing over one hundred million dollars to train are not startup projects. The requirements would apply to perhaps a handful of companies globally.</p>

<p>Interestingly, OpenAI—the company that might seem most protected by regulation limiting new entrants—opposed the bill. This complicated the regulatory capture narrative, though critics noted that OpenAI's opposition focused on specific provisions rather than the concept of regulation itself.</p>

<h2>The Polling Wars</h2>

<p>Both sides commissioned polls to demonstrate public support for their positions. The results revealed as much about polling methodology as public opinion.</p>

<p>The Artificial Intelligence Policy Institute, which supported regulation, found support ranging from 54 to 74 percent across three surveys. Their question described the bill as requiring safety tests and creating liability for developers who fail to take "appropriate precautions."</p>

<p>The California Chamber of Commerce, which opposed the bill, found only 28 percent support. But their question described a "new state regulatory agency" with the power to force small startups to "pay tens of millions of dollars in fines" based on orders from "state bureaucrats." Observers described this framing as "badly biased."</p>

<p>A YouGov poll commissioned by the Economic Security Project, a bill sponsor, found 78 percent national support and 80 percent agreement that Newsom should sign. Their question emphasized safety testing and prevention of catastrophic harms like "disrupting the financial system, shutting down the power grid, or creating biological weapons."</p>

<p>The divergent results demonstrate a fundamental truth about polling on complex policy issues: how you ask the question largely determines the answer you receive.</p>

<h2>What Happens Now</h2>

<p>The deadline for California legislators to override Newsom's veto passed on November 30, 2024. SB 1047 is dead.</p>

<p>But the questions it raised remain very much alive. As AI systems continue advancing, the pressure for some form of regulation will intensify. The European Union has already enacted the AI Act, a comprehensive regulatory framework. China has implemented its own AI governance rules. The United States remains an outlier among major powers in lacking federal AI legislation.</p>

<p>California will likely see new AI safety bills in future legislative sessions, perhaps designed to address Newsom's concerns about focusing on model size rather than deployment context. Federal legislation may eventually preempt state efforts, for better or worse.</p>

<h3>The Deeper Question</h3>

<p>Beyond the specific provisions of SB 1047, the debate illuminated a fundamental challenge in technology governance. How do you regulate something based on what it might become rather than what it currently is? How do you balance innovation against precaution when experts genuinely disagree about the nature and magnitude of potential risks?</p>

<p>The pharmaceutical industry offers one model: extensive testing before release, with liability for harms that occur despite precautions. The software industry has largely operated under a different paradigm: release early, patch problems as they emerge, accept that some harm is inevitable.</p>

<p>Which model should apply to AI systems that might—or might not—pose existential risks? The answer matters enormously. And we haven't settled it yet.</p>

<h2>A Note on Uncertainty</h2>

<p>Perhaps the most honest assessment of SB 1047 came from Anthropic CEO Dario Amodei, who wrote that the amended bill's benefits "likely" outweighed its costs—but added, "we are not certain of this."</p>

<p>That uncertainty pervades the entire field of AI governance. We don't know how capable future AI systems will become. We don't know whether the risks that motivate safety concerns will materialize. We don't know whether regulation will prevent harm or merely drive development to less regulated jurisdictions.</p>

<p>What we do know is that decisions made now—including the decision not to act—will shape how this technology develops. Governor Newsom's veto was itself a choice, one that preserved the status quo of minimal AI regulation in California.</p>

<p>Whether that choice was wise or foolish, we may not learn for years. By then, of course, it may be too late to choose differently.</p>
      </div>

      <footer class="wikipedia-footer">
        <p class="source-link">
          <a href="https://en.wikipedia.org/wiki/Safe_and_Secure_Innovation_for_Frontier_Artificial_Intelligence_Models_Act" target="_blank" rel="noopener">
            View original Wikipedia article &rarr;
          </a>
        </p>
        <p class="rewrite-note">
          This article has been rewritten from Wikipedia source material for enjoyable reading.
          Content may have been condensed, restructured, or simplified.
        </p>
      </footer>

      
      <section class="related-articles">
        <h2>Related Articles</h2>
        <p class="related-intro">This deep dive was written in connection with these articles:</p>
        <ul class="related-list">
          
      <li class="related-article-item">
        <a href="../../article/a5620eae-0b62-43e8-b605-8d31612a0505/index.html">
          <strong>Helen Toner Takes the Reins at CSET</strong>
        </a>
        <span class="article-meta">
          by Jordan Schneider in ChinaTalk
        </span>
      </li>

      <li class="related-article-item">
        <a href="../../article/a790100d-8a15-455c-a4b4-0d59c9e132b3/index.html">
          <strong>&quot;This feels like 1996&quot;: Why a16z&#039;s Martin Casado believes the AI boom still has years to run (General Partner)</strong>
        </a>
        <span class="article-meta">
          by Mario Gabriele in The Generalist
        </span>
      </li>
        </ul>
      </section>
    
    </article>
  
  </main>
</body>
</html>